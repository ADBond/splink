{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d40bf19-f5b4-4530-8894-f070bdaad548",
   "metadata": {},
   "outputs": [],
   "source": [
    "from try_settings import settings_dict\n",
    "import pandas as pd\n",
    "import boto3\n",
    "from dataengineeringutils3.s3 import delete_s3_folder_contents\n",
    "import awswrangler as wr\n",
    "# creates a session at least on the platform...\n",
    "my_session = boto3.Session(region_name=\"eu-west-1\")\n",
    "\n",
    "small_link_job = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a4ee55-80f0-45f2-ae95-f3accf6934a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/home/jovyan/splink/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52489773-d768-4d56-922b-69b3e58a5fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset our db for another test run...\n",
    "if \"splink_awswrangler_test\" in wr.catalog.databases(limit=10000).values:\n",
    "    wr.catalog.delete_database(\n",
    "        name='splink_awswrangler_test',\n",
    "        boto3_session=my_session\n",
    "    )\n",
    "    print(\"Cleaning up folder contents...\")\n",
    "    # clean up folder contents from s3...\n",
    "    # can potentially add this as a module to our awslinker\n",
    "    delete_s3_folder_contents(\"s3://alpha-splink-db-testing/splink_warehouse/\")\n",
    "    delete_s3_folder_contents(\"s3://alpha-splink-db-testing/data/\")\n",
    "\n",
    "if \"splink_awswrangler_test\" not in wr.catalog.databases(limit=10000).values:\n",
    "    import time\n",
    "    time.sleep(3)\n",
    "    wr.catalog.create_database(\"splink_awswrangler_test\", exist_ok=True)\n",
    "print(\"Deleted existing db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d850bced-004e-44db-b7e4-4e1f45fcb16a",
   "metadata": {},
   "source": [
    "## Write all synthetic data\n",
    "\n",
    "Only runs if we want a larger link job    table_name = \"synthetic_data_all\"\n",
    "    data_path = f\"s3://alpha-splink-db-testing/perm_data/{table_name}.parquet\"\n",
    "    athena_s3_path = \"s3://alpha-splink-db-testing/data/\"\n",
    "    df = wr.s3.read_parquet(\n",
    "        path=data_path,\n",
    "        boto3_session=my_session\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29256ff-8ea6-4175-8272-5bf3c1739b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not small_link_job:\n",
    "    table_name = \"synthetic_data_all\"\n",
    "    data_path = f\"s3://alpha-splink-db-testing/perm_data/{table_name}.parquet\"\n",
    "    athena_s3_path = \"s3://alpha-splink-db-testing/data/\"\n",
    "    df = wr.s3.read_parquet(\n",
    "        path=data_path,\n",
    "        boto3_session=my_session\n",
    "    )\n",
    "    for i in range(4):\n",
    "        df = pd.concat([df, df])\n",
    "        \n",
    "    df['unique_id'] = range(len(df))\n",
    "    bucket = \"alpha-splink-db-testing\"\n",
    "    path = f\"s3://{bucket}/data/\"\n",
    "    df.drop([\"source_dataset\"], axis=1, inplace=True)\n",
    "    wr.s3.to_parquet(\n",
    "        df=df,\n",
    "        path=path,\n",
    "        dataset=True,\n",
    "        mode=\"overwrite\",\n",
    "        database=\"splink_awswrangler_test\",\n",
    "        table=table_name\n",
    "    );\n",
    "    print(len(df))\n",
    "    display(df.head(10))\n",
    "    del(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85acdb5c-70ab-414c-9473-eb7a81aa5a9e",
   "metadata": {},
   "source": [
    "## Write df_left and df_right\n",
    "\n",
    "Only runs if we are running a small link job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12fa7c2-ec49-46ae-b790-e554c1e47843",
   "metadata": {},
   "outputs": [],
   "source": [
    "if small_link_job:\n",
    "    df = pd.read_csv(\"./tests/datasets/fake_1000_from_splink_demos.csv\")\n",
    "    bucket = \"alpha-splink-db-testing\"\n",
    "    path = f\"s3://{bucket}/data/\"\n",
    "    wr.s3.to_parquet(\n",
    "        df=df,\n",
    "        path=path,\n",
    "        dataset=True,\n",
    "        mode=\"overwrite\",\n",
    "        database=\"splink_awswrangler_test\",\n",
    "        table=\"df_left\"\n",
    "    );\n",
    "    wr.s3.to_parquet(\n",
    "        df=df,\n",
    "        path=path,\n",
    "        dataset=True,\n",
    "        mode=\"overwrite\",\n",
    "        database=\"splink_awswrangler_test\",\n",
    "        table=\"df_right\"\n",
    "    );"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2ae54d-15ea-4cc7-ab78-5aac9a65d3cb",
   "metadata": {},
   "source": [
    "## Run linking job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b489dd7-4cf8-44a0-8875-a9b1beffc874",
   "metadata": {},
   "outputs": [],
   "source": [
    "from splink.aws.aws_linker import AWSLinker\n",
    "from try_settings import settings_dict, settings_dict_large\n",
    "import time\n",
    "\n",
    "if small_link_job:\n",
    "    inputs = {\"df_left\": \"df_left\", \"df_right\": \"df_right\"}\n",
    "    settings = settings_dict\n",
    "    txt = \"smaller link job\"\n",
    "else:\n",
    "    inputs = {\"synthetic_data_all\": \"synthetic_data_all\"}\n",
    "    settings = settings_dict_large\n",
    "    txt = \"large link job\"\n",
    "\n",
    "print(f\"Running {txt}\")\n",
    "print(f\"====================\")\n",
    "    \n",
    "t = time.time()\n",
    "linker = AWSLinker(\n",
    "    settings,\n",
    "    input_tables=inputs,\n",
    "    boto3_session=my_session,\n",
    "    output_bucket=\"alpha-splink-db-testing\",\n",
    "    database_name=\"splink_awswrangler_test\"\n",
    ")\n",
    "linker.train_u_using_random_sampling(target_rows=1e6)\n",
    "\n",
    "if small_link_job:\n",
    "    blocking_rule = \"l.first_name = r.first_name and l.surname = r.surname\"\n",
    "    linker.train_m_using_expectation_maximisation(blocking_rule)\n",
    "\n",
    "    blocking_rule = \"l.dob = r.dob\"\n",
    "    linker.train_m_using_expectation_maximisation(blocking_rule)\n",
    "else:\n",
    "    linker.train_m_using_expectation_maximisation(\"l.full_name = r.full_name\")\n",
    "\n",
    "    linker.train_m_using_expectation_maximisation(\n",
    "        \"l.dob = r.dob and substr(l.postcode,1,2) = substr(r.postcode,1,2)\"\n",
    "    )\n",
    "\n",
    "p = time.time()\n",
    "df = linker.predict()\n",
    "print(f\"Predict step took {time.time()-p} seconds\")\n",
    "\n",
    "print(f\"Total time taken {time.time()-t} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "splink",
   "language": "python",
   "name": "splink"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
