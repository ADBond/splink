{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "back_end = 'duckdb'\n",
    "import duckdb\n",
    "import logging\n",
    "LOGGER = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pytest\n",
    "\n",
    "if back_end == 'duckdb':\n",
    "    import splink.duckdb.duckdb_comparison_level_library as cll\n",
    "    import splink.duckdb.duckdb_comparison_library as cl\n",
    "    from splink.duckdb.duckdb_linker import DuckDBLinker\n",
    "    Linker = DuckDBLinker\n",
    "elif back_end == 'spark':\n",
    "    import splink.spark.spark_comparison_level_library as cll\n",
    "    import splink.spark.spark_comparison_library as cl\n",
    "    from splink.spark.spark_linker import SparkLinker\n",
    "    Linker = SparkLinker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if back_end == 'spark':\n",
    "    from pyspark.context import SparkContext, SparkConf\n",
    "    from pyspark.sql import SparkSession, types\n",
    "    from pyspark.sql.functions import col, udf\n",
    "    from splink.spark.jar_location import similarity_jar_location\n",
    "    conf = SparkConf()\n",
    "    path = similarity_jar_location()\n",
    "    conf.set(\"spark.jars\", path)\n",
    "    #conf.set(\"spark.sql.ansi.enabled\",True)\n",
    "    # conf.set(\"spark.sql.legacy.timeParserPolicy\",\"LEGACY\")\n",
    "\n",
    "    sc = SparkContext.getOrCreate(conf=conf)\n",
    "    spark = SparkSession(sc)\n",
    "    spark.sparkContext.setCheckpointDir('/Users/alice.oleary/Documents/spark_checkpoint_dir')\n",
    "\n",
    "    # Register the jaro winkler custom udf\n",
    "    spark.udf.registerJavaFunction(\n",
    "        \"jaro_winkler\", \"uk.gov.moj.dash.linkage.JaroWinklerSimilarity\", types.DoubleType()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "        [\n",
    "            {\n",
    "                \"unique_id\": 1,\n",
    "                \"first_name\": \"Tom\",\n",
    "                \"dob\": \"02-03-1993\",\n",
    "            },\n",
    "            {\n",
    "                \"unique_id\": 2,\n",
    "                \"first_name\": \"Robin\",\n",
    "                \"dob\": \"30-01-1992\",\n",
    "            },\n",
    "        ]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(\n",
    "#         [\n",
    "#             {\n",
    "#                 \"unique_id\": 1,\n",
    "#                 \"first_name\": \"Tom\",\n",
    "#                 \"dob\": \"2000-13-01\",\n",
    "#             },\n",
    "#             {\n",
    "#                 \"unique_id\": 2,\n",
    "#                 \"first_name\": \"Robin\",\n",
    "#                 \"dob\": \"2000-01-24\",\n",
    "#             },\n",
    "#             {\n",
    "#                 \"unique_id\": 3,\n",
    "#                 \"first_name\": \"Zoe\",\n",
    "#                 \"dob\": \"1995-14-30\",\n",
    "#             },\n",
    "#             {\n",
    "#                 \"unique_id\": 4,\n",
    "#                 \"first_name\": \"Sam\",\n",
    "#                 \"dob\": \"1966-07-05\",\n",
    "#             },\n",
    "#             {\n",
    "#                 \"unique_id\": 5,\n",
    "#                 \"first_name\": \"Andy\",\n",
    "#                 \"dob\": \"1996-32-15\",\n",
    "#             },\n",
    "#             {\n",
    "#                 \"unique_id\": 6,\n",
    "#                 \"first_name\": \"Alice\",\n",
    "#                 \"dob\": \"2000-03-25\",\n",
    "#             },\n",
    "#             {\n",
    "#                 \"unique_id\": 7,\n",
    "#                 \"first_name\": \"Afua\",\n",
    "#                 \"dob\": \"1960-01-01\",\n",
    "#             },\n",
    "#         ]\n",
    "#     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>first_name</th>\n",
       "      <th>dob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Tom</td>\n",
       "      <td>02-03-1993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Robin</td>\n",
       "      <td>30-01-1992</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   unique_id first_name         dob\n",
       "0          1        Tom  02-03-1993\n",
       "1          2      Robin  30-01-1992"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "exact_match_fn = cl.exact_match(\"first_name\")\n",
    "\n",
    "# For testing the cll version\n",
    "dob_diff = {\n",
    "    \"output_column_name\": \"dob\",\n",
    "    \"comparison_levels\": [\n",
    "        cll.null_level(\"dob\"),\n",
    "        cll.exact_match_level(\"dob\"),\n",
    "        cll.datediff_level(\n",
    "            date_col=\"dob\",\n",
    "            date_threshold=30,\n",
    "            date_metric=\"day\",\n",
    "            cast_strings_to_date=True,\n",
    "        ),\n",
    "        cll.datediff_level(\n",
    "            date_col=\"dob\",\n",
    "            date_threshold=12,\n",
    "            date_metric=\"month\",\n",
    "            cast_strings_to_date=True,\n",
    "        ),\n",
    "        cll.datediff_level(\n",
    "            date_col=\"dob\",\n",
    "            date_threshold=5,\n",
    "            date_metric=\"year\",\n",
    "            cast_strings_to_date=True,\n",
    "        ),\n",
    "        cll.datediff_level(\n",
    "            date_col=\"dob\",\n",
    "            date_threshold=100,\n",
    "            date_metric=\"year\",\n",
    "            cast_strings_to_date=True,\n",
    "        ),\n",
    "        cll.else_level(),\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = {\n",
    "    \"link_type\": \"dedupe_only\",\n",
    "    \"comparisons\": [exact_match_fn, dob_diff],\n",
    "}\n",
    "\n",
    "settings_cl = {\n",
    "    \"link_type\": \"dedupe_only\",\n",
    "    \"comparisons\": [\n",
    "        exact_match_fn,\n",
    "        cl.datediff_at_thresholds(\n",
    "            \"dob\", [30, 12, 5, 100], [\"day\", \"month\", \"year\", \"year\"],\n",
    "            cast_strings_to_date=True\n",
    "        ),\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"dob\"] = pd.to_datetime(df[\"dob\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alice.oleary/Documents/splink_dev_env/lib/python3.10/site-packages/pyspark/sql/pandas/utils.py:34: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n",
      "/Users/alice.oleary/Documents/splink_dev_env/lib/python3.10/site-packages/pyspark/sql/pandas/utils.py:34: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n",
      "/Users/alice.oleary/Documents/splink_dev_env/lib/python3.10/site-packages/pyspark/sql/pandas/utils.py:34: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n",
      "/Users/alice.oleary/Documents/splink_dev_env/lib/python3.10/site-packages/pyspark/sql/pandas/utils.py:34: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n",
      "/Users/alice.oleary/Documents/splink_dev_env/lib/python3.10/site-packages/pyspark/sql/pandas/utils.py:34: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n",
      "/Users/alice.oleary/Documents/splink_dev_env/lib/python3.10/site-packages/pyspark/sql/pandas/utils.py:34: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n",
      "/Users/alice.oleary/Documents/splink_dev_env/lib/python3.10/site-packages/pyspark/sql/pandas/utils.py:34: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n",
      "/Users/alice.oleary/Documents/splink_dev_env/lib/python3.10/site-packages/pyspark/sql/pandas/utils.py:34: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n"
     ]
    }
   ],
   "source": [
    "if back_end == 'spark':\n",
    "    df_new = spark.createDataFrame(df)\n",
    "    df_new.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'caplog' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mwith\u001b[39;00m caplog\u001b[39m.\u001b[39mat_level(logging\u001b[39m.\u001b[39mWARNING):\n\u001b[1;32m      2\u001b[0m     linker \u001b[39m=\u001b[39m Linker(df_new, settings)\n\u001b[1;32m      3\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mSomething bad happened!\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m caplog\u001b[39m.\u001b[39mtext\n",
      "\u001b[0;31mNameError\u001b[0m: name 'caplog' is not defined"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/05 06:03:15 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 36052917 ms exceeds timeout 120000 ms\n",
      "23/04/05 06:03:16 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    }
   ],
   "source": [
    "with caplog.at_level(logging.WARNING):\n",
    "    linker = Linker(df_new, settings)\n",
    "    assert 'Something bad happened!' in caplog.text\n",
    "# df_e = linker.predict().as_pandas_dataframe()\n",
    "# linker = Linker(df, settings_cl)\n",
    "# cl_df_e = linker.predict().as_pandas_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings_cl = {\n",
    "#     \"link_type\": \"dedupe_only\",\n",
    "#     \"comparisons\": [\n",
    "#         exact_match_fn,\n",
    "#         cl.datediff_at_thresholds(\n",
    "#             \"dob\", [30, 12, 5, 100], [\"day\", \"month\", \"year\", \"year\"],\n",
    "#             cast_strings_to_date=True, date_format=date_format_param\n",
    "#         ),\n",
    "#     ],\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_dob_linker(df, dobs=[], date_format_param='', Linker=None):\n",
    "    settings = {\n",
    "    \"link_type\": \"dedupe_only\",\n",
    "    \"comparisons\": [\n",
    "        exact_match_fn,\n",
    "        cl.datediff_at_thresholds(\n",
    "            \"dob\", [30, 12, 5, 100], [\"day\", \"month\", \"year\", \"year\"],\n",
    "            cast_strings_to_date=True, date_format=date_format_param\n",
    "        ),\n",
    "    ],\n",
    "    }\n",
    "    if len(dobs) == df.shape[0]:\n",
    "        df['dob'] = dobs\n",
    "    if back_end == 'spark':\n",
    "        df = spark.createDataFrame(df)\n",
    "        df.persist()\n",
    "    linker = Linker(df, settings)   \n",
    "    df_e = linker.predict().as_pandas_dataframe()\n",
    "    return df_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alice.oleary/Documents/splink_dev_env/lib/python3.10/site-packages/pyspark/sql/pandas/utils.py:34: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n",
      "/Users/alice.oleary/Documents/splink_dev_env/lib/python3.10/site-packages/pyspark/sql/pandas/utils.py:34: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n",
      "/Users/alice.oleary/Documents/splink_dev_env/lib/python3.10/site-packages/pyspark/sql/pandas/utils.py:34: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n",
      "/Users/alice.oleary/Documents/splink_dev_env/lib/python3.10/site-packages/pyspark/sql/pandas/utils.py:34: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n",
      "/Users/alice.oleary/Documents/splink_dev_env/lib/python3.10/site-packages/pyspark/sql/pandas/utils.py:34: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n",
      "/Users/alice.oleary/Documents/splink_dev_env/lib/python3.10/site-packages/pyspark/sql/pandas/utils.py:34: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n",
      "/Users/alice.oleary/Documents/splink_dev_env/lib/python3.10/site-packages/pyspark/sql/pandas/utils.py:34: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n",
      "/Users/alice.oleary/Documents/splink_dev_env/lib/python3.10/site-packages/pyspark/sql/pandas/utils.py:34: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m simple_dob_linker(df, dobs\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39m03/04/1994\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m19/02/1993\u001b[39;49m\u001b[39m'\u001b[39;49m], date_format_param\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39md/m/Y\u001b[39;49m\u001b[39m'\u001b[39;49m, Linker\u001b[39m=\u001b[39;49mLinker)\n",
      "Cell \u001b[0;32mIn[12], line 17\u001b[0m, in \u001b[0;36msimple_dob_linker\u001b[0;34m(df, dobs, date_format_param, Linker)\u001b[0m\n\u001b[1;32m     15\u001b[0m     df \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39mcreateDataFrame(df)\n\u001b[1;32m     16\u001b[0m     df\u001b[39m.\u001b[39mpersist()\n\u001b[0;32m---> 17\u001b[0m linker \u001b[39m=\u001b[39m Linker(df, settings)   \n\u001b[1;32m     18\u001b[0m df_e \u001b[39m=\u001b[39m linker\u001b[39m.\u001b[39mpredict()\u001b[39m.\u001b[39mas_pandas_dataframe()\n\u001b[1;32m     19\u001b[0m \u001b[39mreturn\u001b[39;00m df_e\n",
      "File \u001b[0;32m~/Documents/splink/splink/spark/spark_linker.py:154\u001b[0m, in \u001b[0;36mSparkLinker.__init__\u001b[0;34m(self, input_table_or_tables, settings_dict, break_lineage_method, set_up_basic_logging, input_table_aliases, spark, catalog, database, repartition_after_blocking, num_partitions_on_repartition)\u001b[0m\n\u001b[1;32m    151\u001b[0m     homogenised_tables\u001b[39m.\u001b[39mappend(table)\n\u001b[1;32m    152\u001b[0m     homogenised_aliases\u001b[39m.\u001b[39mappend(alias)\n\u001b[0;32m--> 154\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m    155\u001b[0m     homogenised_tables,\n\u001b[1;32m    156\u001b[0m     settings_dict,\n\u001b[1;32m    157\u001b[0m     set_up_basic_logging,\n\u001b[1;32m    158\u001b[0m     input_table_aliases\u001b[39m=\u001b[39;49mhomogenised_aliases,\n\u001b[1;32m    159\u001b[0m )\n\u001b[1;32m    160\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_ansi_enabled_if_converting_dates()\n\u001b[1;32m    162\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_databricks \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mDATABRICKS_RUNTIME_VERSION\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39menviron\n",
      "File \u001b[0;32m~/Documents/splink/splink/linker.py:177\u001b[0m, in \u001b[0;36mLinker.__init__\u001b[0;34m(self, input_table_or_tables, settings_dict, set_up_basic_logging, input_table_aliases)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pipeline \u001b[39m=\u001b[39m SQLPipeline()\n\u001b[1;32m    173\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_input_tables_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_input_tables_dict(\n\u001b[1;32m    174\u001b[0m     input_table_or_tables, input_table_aliases\n\u001b[1;32m    175\u001b[0m )\n\u001b[0;32m--> 177\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39;49m(settings_dict, (\u001b[39mdict\u001b[39m, \u001b[39mtype\u001b[39m(\u001b[39mNone\u001b[39;00m))):\n\u001b[1;32m    178\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_setup_settings_objs(\u001b[39mNone\u001b[39;00m)  \u001b[39m# feed it a blank settings dictionary\u001b[39;00m\n\u001b[1;32m    179\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mload_settings(settings_dict)\n",
      "File \u001b[0;32m~/Documents/splink/splink/linker.py:177\u001b[0m, in \u001b[0;36mLinker.__init__\u001b[0;34m(self, input_table_or_tables, settings_dict, set_up_basic_logging, input_table_aliases)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pipeline \u001b[39m=\u001b[39m SQLPipeline()\n\u001b[1;32m    173\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_input_tables_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_input_tables_dict(\n\u001b[1;32m    174\u001b[0m     input_table_or_tables, input_table_aliases\n\u001b[1;32m    175\u001b[0m )\n\u001b[0;32m--> 177\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39;49m(settings_dict, (\u001b[39mdict\u001b[39m, \u001b[39mtype\u001b[39m(\u001b[39mNone\u001b[39;00m))):\n\u001b[1;32m    178\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_setup_settings_objs(\u001b[39mNone\u001b[39;00m)  \u001b[39m# feed it a blank settings dictionary\u001b[39;00m\n\u001b[1;32m    179\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mload_settings(settings_dict)\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1457\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:701\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1152\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1135\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:312\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Documents/splink_dev_env/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   2067\u001b[0m             from_this_thread\u001b[39m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   2069\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_threads_suspended_single_notification\u001b[39m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[0;32m-> 2070\u001b[0m         keep_suspended \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_wait_suspend(thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\n\u001b[1;32m   2072\u001b[0m frames_list \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   2074\u001b[0m \u001b[39mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   2075\u001b[0m     \u001b[39m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/splink_dev_env/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2103\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_input_hook()\n\u001b[1;32m   2105\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocess_internal_commands()\n\u001b[0;32m-> 2106\u001b[0m     time\u001b[39m.\u001b[39;49msleep(\u001b[39m0.01\u001b[39;49m)\n\u001b[1;32m   2108\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[39mstr\u001b[39m(\u001b[39mid\u001b[39m(frame)))\n\u001b[1;32m   2110\u001b[0m \u001b[39m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "simple_dob_linker(df, dobs=['03/04/1994', '19/02/1993'], date_format_param='d/m/Y', Linker=Linker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with pytest.raises(duckdb.InvalidInputException):\n",
    "#     simple_linker(df, settings, Linker)\n",
    "simple_dob_linker(df, dobs=['03/04/1994', '19/02/1993'], date_format_param='%d/%m/%Y', Linker=Linker)\n",
    "simple_dob_linker(df, dobs=['03-04-1994', '19-02-1993'], date_format_param='%d-%m-%Y', Linker=Linker)\n",
    "simple_dob_linker(df, dobs=['04/05/1994', '10/02/1993'], date_format_param='%m/%d/%Y', Linker=Linker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Linker == SparkLinker:\n",
    "    valid_date_formats = ['dd/mm/Y', 'dd-mm-Y', 'mm/dd/Y', 'Y/mm/dd']\n",
    "elif Linker == DuckDBLinker:\n",
    "    valid_date_formats = ['%d/%m/%Y', '%d-%m-%Y', '%m/%d/%Y', '%Y/%m/%d']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pytest.raises(py4j.protocol.Py4JJavaError):\n",
    "    simple_dob_linker(df, dobs=['1994-14-15', '1994-12-03'], \n",
    "                        date_format_param='y-M-d', Linker=Linker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import py4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_dob_linker(df, dobs=['1994/14/15', '1994/12/03'], \n",
    "                    date_format_param='y/M/d', Linker=Linker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with pytest.raises(duckdb.InvalidInputException):\n",
    "#     simple_linker(df, settings, Linker)\n",
    "simple_dob_linker(df, dobs=['03/04/1994', '19/02/1993'], date_format_param='d/m/Y', Linker=Linker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "simple_dob_linker(df, dobs=['03/04/1994', '19/02/1993'], date_format_param='%d/%m/%Y', Linker=Linker)\n",
    "simple_dob_linker(df, dobs=['03-04-1994', '19-02-1993'], date_format_param='%d-%m-%Y', Linker=Linker)\n",
    "simple_dob_linker(df, dobs=['04/05/1994', '10/02/1993'], date_format_param='%m/%d/%Y', Linker=Linker)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_date_formats = ['%d/%m/%Y', '%d-%m-%Y', '%m/%d/%Y']\n",
    "#valid_date_formats = ['d/m/%Y', 'd-m-%Y', 'm/d/%Y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_dob_linker(df, dobs=['03/04/1994', '19/02/1993'], date_format_param=valid_date_formats[0], Linker=Linker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "simple_dob_linker(df, dobs=['03/04/1994', '19/02/1993'], date_format_param='d/m/Y', Linker=Linker)\n",
    "simple_dob_linker(df, dobs=['03-04-1994', '19-02-1993'], date_format_param='d-m-Y', Linker=Linker)\n",
    "simple_dob_linker(df, dobs=['04/05/1994', '10/02/1993'], date_format_param='m/d/Y', Linker=Linker)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_dob_linker(df, dobs=['04/05/1994', '10/02/1993'], date_format_param='m/d/Y', Linker=Linker)\n",
    "simple_dob_linker(df, dobs=['1994/05/04', '1993/14/02'], date_format_param='Y-m-d', Linker=Linker)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_dob_linker(df, dobs=['1994/55/54', '1993/14/02'], date_format_param='%Y-%m-%d', Linker=Linker)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# incompatible date formats\n",
    "with pytest.raises(duckdb.InvalidInputException):\n",
    "    simple_dob_linker(df, dobs=['03-04-1994', '19-02-1993'], date_format_param='%d/%m/%Y', Linker=Linker)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dobs = ['03-04-1994', '31-02-1993']\n",
    "df['dob'] = dobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings_cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_linker(df, settings_cl, Linker)\n",
    "df_e.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dict key: {size: gamma_level value}\n",
    "size_gamma_lookup = {1: 11, 2: 6, 3: 3, 4: 1}\n",
    "\n",
    "linker_outputs = {\n",
    "    \"cll\": df_e,\n",
    "    \"cl\": cl_df_e,\n",
    "}\n",
    "\n",
    "# Check gamma sizes are as expected\n",
    "for gamma, gamma_lookup in size_gamma_lookup.items():\n",
    "    for linker_pred in linker_outputs.values():\n",
    "        assert sum(linker_pred[\"gamma_dob\"] == gamma) == gamma_lookup\n",
    "\n",
    "# Check individual IDs are assigned to the correct gamma values\n",
    "# Dict key: {gamma_value: tuple of ID pairs}\n",
    "size_gamma_lookup = {\n",
    "    4: [(1, 2)],\n",
    "    3: [(3, 5), (1, 6), (2, 6)],\n",
    "    2: [(1, 3), (2, 3), (1, 5), (2, 5), (3, 6), (5, 6)],\n",
    "}\n",
    "\n",
    "for gamma, id_pairs in size_gamma_lookup.items():\n",
    "    for left, right in id_pairs:\n",
    "        for linker_name, linker_pred in linker_outputs.items():\n",
    "\n",
    "            print(f\"Checking IDs: {left}, {right} for {linker_name}\")\n",
    "\n",
    "            assert (\n",
    "                linker_pred.loc[\n",
    "                    (linker_pred.unique_id_l == left)\n",
    "                    & (linker_pred.unique_id_r == right)\n",
    "                ][\"gamma_dob\"].values[0]\n",
    "                == gamma\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_dob_linker(df, dobs=['1994/05/04', '1993/14/02'], date_format_param='%Y/%m/%d', Linker=Linker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pytest.raises(Exception) as e:\n",
    "    simple_dob_linker(df, dobs=['1994/05/04', '1993/14/02'], date_format_param='%Y/%m/%d', Linker=Linker)\n",
    "e.type   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pytest.raises(Exception) as e:\n",
    "    simple_dob_linker(df, dobs=['03-14-1994', '19-22-1993'], date_format_param='%d-%m-%Y', Linker=Linker)\n",
    "e.type   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pytest.raises(Exception) as e:\n",
    "    simple_dob_linker(df, dobs=['20-04-1993', '19-02-1993'], date_format_param='%d/%m/%Y', Linker=Linker)\n",
    "e.type   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from splink.duckdb.duckdb_linker import DuckDBLinker\n",
    "from splink.spark.spark_linker import SparkLinker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import py4j\n",
    "if Linker == SparkLinker:\n",
    "    expected_bad_dates_error = py4j.protocol.Py4JJavaError\n",
    "elif Linker == DuckDBLinker:\n",
    "    expected_bad_dates_error = duckdb.InvalidInputException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pytest.raises(expected_bad_dates_error):\n",
    "    simple_dob_linker(df, dobs=['1994/05/04', '1993/14/02'], date_format_param='%Y/%m/%d', Linker=Linker)\n",
    "with pytest.raises(expected_bad_dates_error):\n",
    "    simple_dob_linker(df, dobs=['03-14-1994', '19-22-1993'], date_format_param='%d-%m-%Y', Linker=Linker)\n",
    "with pytest.raises(expected_bad_dates_error):\n",
    "    simple_dob_linker(df, dobs=['20-04-1993', '19-02-1993'], date_format_param='%d/%m/%Y', Linker=Linker)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # # Differing lengths between thresholds and units\n",
    "\n",
    "cl.datediff_at_thresholds(\"dob\", [1], [\"day\", \"month\", \"year\", \"year\"])\n",
    "    # # Negative threshold\n",
    "    # with pytest.raises(ValueError):\n",
    "    #     cl.datediff_at_thresholds(\"dob\", [-1], [\"day\"])\n",
    "    # # Invalid metric\n",
    "    # with pytest.raises(ValueError):\n",
    "    #     cl.datediff_at_thresholds(\"dob\", [1], [\"dy\"])\n",
    "    # # Threshold len == 0\n",
    "    # with pytest.raises(ValueError):\n",
    "    #     cl.datediff_at_thresholds(\"dob\", [], [\"dy\"])\n",
    "    # # Metric len == 0\n",
    "    # with pytest.raises(ValueError):\n",
    "    #     cl.datediff_at_thresholds(\"dob\", [1], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl.datediff_at_thresholds(\"dob\", [1], [\"day\", \"month\", \"year\", \"year\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "splink_dev_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40ed3ce993a5a5d83f829fe220d0ce5dc391ba3c1504651e486245c4727b11f2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
