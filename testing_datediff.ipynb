{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "back_end = 'spark'\n",
    "import duckdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pytest\n",
    "\n",
    "if back_end == 'duckdb':\n",
    "    import splink.duckdb.duckdb_comparison_level_library as cll\n",
    "    import splink.duckdb.duckdb_comparison_library as cl\n",
    "    from splink.duckdb.duckdb_linker import DuckDBLinker\n",
    "    Linker = DuckDBLinker\n",
    "elif back_end == 'spark':\n",
    "    import splink.spark.spark_comparison_level_library as cll\n",
    "    import splink.spark.spark_comparison_library as cl\n",
    "    from splink.spark.spark_linker import SparkLinker\n",
    "    Linker = SparkLinker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/03/21 14:42:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/03/21 14:42:43 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "if back_end == 'spark':\n",
    "    from pyspark.context import SparkContext, SparkConf\n",
    "    from pyspark.sql import SparkSession, types\n",
    "    from pyspark.sql.functions import col, udf\n",
    "    from splink.spark.jar_location import similarity_jar_location\n",
    "    conf = SparkConf()\n",
    "    path = similarity_jar_location()\n",
    "    conf.set(\"spark.jars\", path)\n",
    "    conf.set(\"spark.sql.ansi.enabled\",True)\n",
    "    # conf.set(\"spark.sql.legacy.timeParserPolicy\",\"LEGACY\")\n",
    "\n",
    "    sc = SparkContext.getOrCreate(conf=conf)\n",
    "    spark = SparkSession(sc)\n",
    "    spark.sparkContext.setCheckpointDir('/Users/alice.oleary/Documents/spark_checkpoint_dir')\n",
    "\n",
    "    # Register the jaro winkler custom udf\n",
    "    spark.udf.registerJavaFunction(\n",
    "        \"jaro_winkler\", \"uk.gov.moj.dash.linkage.JaroWinklerSimilarity\", types.DoubleType()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "        [\n",
    "            {\n",
    "                \"unique_id\": 1,\n",
    "                \"first_name\": \"Tom\",\n",
    "                \"dob\": \"02-03-1993\",\n",
    "            },\n",
    "            {\n",
    "                \"unique_id\": 2,\n",
    "                \"first_name\": \"Robin\",\n",
    "                \"dob\": \"30-01-1992\",\n",
    "            },\n",
    "        ]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(\n",
    "#         [\n",
    "#             {\n",
    "#                 \"unique_id\": 1,\n",
    "#                 \"first_name\": \"Tom\",\n",
    "#                 \"dob\": \"2000-13-01\",\n",
    "#             },\n",
    "#             {\n",
    "#                 \"unique_id\": 2,\n",
    "#                 \"first_name\": \"Robin\",\n",
    "#                 \"dob\": \"2000-01-24\",\n",
    "#             },\n",
    "#             {\n",
    "#                 \"unique_id\": 3,\n",
    "#                 \"first_name\": \"Zoe\",\n",
    "#                 \"dob\": \"1995-14-30\",\n",
    "#             },\n",
    "#             {\n",
    "#                 \"unique_id\": 4,\n",
    "#                 \"first_name\": \"Sam\",\n",
    "#                 \"dob\": \"1966-07-05\",\n",
    "#             },\n",
    "#             {\n",
    "#                 \"unique_id\": 5,\n",
    "#                 \"first_name\": \"Andy\",\n",
    "#                 \"dob\": \"1996-32-15\",\n",
    "#             },\n",
    "#             {\n",
    "#                 \"unique_id\": 6,\n",
    "#                 \"first_name\": \"Alice\",\n",
    "#                 \"dob\": \"2000-03-25\",\n",
    "#             },\n",
    "#             {\n",
    "#                 \"unique_id\": 7,\n",
    "#                 \"first_name\": \"Afua\",\n",
    "#                 \"dob\": \"1960-01-01\",\n",
    "#             },\n",
    "#         ]\n",
    "#     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>first_name</th>\n",
       "      <th>dob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Tom</td>\n",
       "      <td>02-03-1993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Robin</td>\n",
       "      <td>30-01-1992</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   unique_id first_name         dob\n",
       "0          1        Tom  02-03-1993\n",
       "1          2      Robin  30-01-1992"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "exact_match_fn = cl.exact_match(\"first_name\")\n",
    "\n",
    "# For testing the cll version\n",
    "dob_diff = {\n",
    "    \"output_column_name\": \"dob\",\n",
    "    \"comparison_levels\": [\n",
    "        cll.null_level(\"dob\"),\n",
    "        cll.exact_match_level(\"dob\"),\n",
    "        cll.datediff_level(\n",
    "            date_col=\"dob\",\n",
    "            date_threshold=30,\n",
    "            date_metric=\"day\",\n",
    "            cast_strings_to_date=True,\n",
    "        ),\n",
    "        cll.datediff_level(\n",
    "            date_col=\"dob\",\n",
    "            date_threshold=12,\n",
    "            date_metric=\"month\",\n",
    "            cast_strings_to_date=True,\n",
    "        ),\n",
    "        cll.datediff_level(\n",
    "            date_col=\"dob\",\n",
    "            date_threshold=5,\n",
    "            date_metric=\"year\",\n",
    "            cast_strings_to_date=True,\n",
    "        ),\n",
    "        cll.datediff_level(\n",
    "            date_col=\"dob\",\n",
    "            date_threshold=100,\n",
    "            date_metric=\"year\",\n",
    "            cast_strings_to_date=True,\n",
    "        ),\n",
    "        cll.else_level(),\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = {\n",
    "    \"link_type\": \"dedupe_only\",\n",
    "    \"comparisons\": [exact_match_fn, dob_diff],\n",
    "}\n",
    "\n",
    "settings_cl = {\n",
    "    \"link_type\": \"dedupe_only\",\n",
    "    \"comparisons\": [\n",
    "        exact_match_fn,\n",
    "        cl.datediff_at_thresholds(\n",
    "            \"dob\", [30, 12, 5, 100], [\"day\", \"month\", \"year\", \"year\"],\n",
    "            cast_strings_to_date=True\n",
    "        ),\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"dob\"] = pd.to_datetime(df[\"dob\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if back_end == 'spark':\n",
    "#     df_new = spark.createDataFrame(df)\n",
    "#     df_new.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linker = Linker(df, settings)\n",
    "# df_e = linker.predict().as_pandas_dataframe()\n",
    "# linker = Linker(df, settings_cl)\n",
    "# cl_df_e = linker.predict().as_pandas_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings_cl = {\n",
    "#     \"link_type\": \"dedupe_only\",\n",
    "#     \"comparisons\": [\n",
    "#         exact_match_fn,\n",
    "#         cl.datediff_at_thresholds(\n",
    "#             \"dob\", [30, 12, 5, 100], [\"day\", \"month\", \"year\", \"year\"],\n",
    "#             cast_strings_to_date=True, date_format=date_format_param\n",
    "#         ),\n",
    "#     ],\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_dob_linker(df, dobs=[], date_format_param='', Linker=None):\n",
    "    settings = {\n",
    "    \"link_type\": \"dedupe_only\",\n",
    "    \"comparisons\": [\n",
    "        exact_match_fn,\n",
    "        cl.datediff_at_thresholds(\n",
    "            \"dob\", [30, 12, 5, 100], [\"day\", \"month\", \"year\", \"year\"],\n",
    "            cast_strings_to_date=True, date_format=date_format_param\n",
    "        ),\n",
    "    ],\n",
    "    }\n",
    "    if len(dobs) == df.shape[0]:\n",
    "        df['dob'] = dobs\n",
    "    if back_end == 'spark':\n",
    "        df = spark.createDataFrame(df)\n",
    "        df.persist()\n",
    "    linker = Linker(df, settings)   \n",
    "    df_e = linker.predict().as_pandas_dataframe()\n",
    "    return df_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alice.oleary/Documents/splink_dev_env/lib/python3.10/site-packages/pyspark/sql/pandas/utils.py:34: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n",
      "/Users/alice.oleary/Documents/splink_dev_env/lib/python3.10/site-packages/pyspark/sql/pandas/utils.py:34: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n",
      "/Users/alice.oleary/Documents/splink_dev_env/lib/python3.10/site-packages/pyspark/sql/pandas/utils.py:34: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n",
      "/Users/alice.oleary/Documents/splink_dev_env/lib/python3.10/site-packages/pyspark/sql/pandas/utils.py:34: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n",
      "/Users/alice.oleary/Documents/splink_dev_env/lib/python3.10/site-packages/pyspark/sql/pandas/utils.py:34: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n",
      "/Users/alice.oleary/Documents/splink_dev_env/lib/python3.10/site-packages/pyspark/sql/pandas/utils.py:34: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n",
      "/Users/alice.oleary/Documents/splink_dev_env/lib/python3.10/site-packages/pyspark/sql/pandas/utils.py:34: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n",
      "/Users/alice.oleary/Documents/splink_dev_env/lib/python3.10/site-packages/pyspark/sql/pandas/utils.py:34: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n",
      "23/03/21 14:42:47 WARN SimpleFunctionRegistry: The function jaro_winkler replaced a previously registered function.\n",
      "23/03/21 14:42:50 WARN DataSource: All paths were ignored:                      \n",
      "  file:/Users/alice.oleary/Documents/spark_checkpoint_dir/ffb6d686-aa41-478e-a33a-b015897210c2/__splink__df_concat_with_tf_35659bfda\n",
      "23/03/21 14:42:51 ERROR FileFormatWriter: Aborting job c04c4c42-3e4c-49b5-a270-227d54fd1941.\n",
      "org.apache.spark.SparkUpgradeException: You may get a different result due to the upgrading of Spark 3.0: Fail to recognize 'd/m/Y' pattern in the DateTimeFormatter. 1) You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the behavior before Spark 3.0. 2) You can form a valid datetime pattern with the guide from https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkLegacyFormatter$1.applyOrElse(DateTimeFormatterHelper.scala:196)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkLegacyFormatter$1.applyOrElse(DateTimeFormatterHelper.scala:185)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.validatePatternString(TimestampFormatter.scala:109)\n",
      "\tat org.apache.spark.sql.catalyst.util.TimestampFormatter$.getFormatter(TimestampFormatter.scala:300)\n",
      "\tat org.apache.spark.sql.catalyst.util.TimestampFormatter$.apply(TimestampFormatter.scala:333)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.TimestampFormatterHelper.getFormatter(datetimeExpressions.scala:72)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.TimestampFormatterHelper.getFormatter$(datetimeExpressions.scala:67)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.ToTimestamp.getFormatter(datetimeExpressions.scala:918)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.TimestampFormatterHelper.$anonfun$formatterOption$1(datetimeExpressions.scala:64)\n",
      "\tat scala.Option.map(Option.scala:230)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.TimestampFormatterHelper.formatterOption(datetimeExpressions.scala:64)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.TimestampFormatterHelper.formatterOption$(datetimeExpressions.scala:62)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.ToTimestamp.formatterOption$lzycompute(datetimeExpressions.scala:918)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.ToTimestamp.formatterOption(datetimeExpressions.scala:918)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.ToTimestamp.doGenCode(datetimeExpressions.scala:978)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$genCode$3(Expression.scala:146)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:141)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.CastBase.doGenCode(Cast.scala:853)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$genCode$3(Expression.scala:146)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:141)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.CastBase.genCode(Cast.scala:848)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.BinaryExpression.nullSafeCodeGen(Expression.scala:608)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.BinaryExpression.defineCodeGen(Expression.scala:591)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.DateDiff.doGenCode(datetimeExpressions.scala:1869)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$genCode$3(Expression.scala:146)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:141)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.UnaryExpression.nullSafeCodeGen(Expression.scala:519)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.UnaryExpression.defineCodeGen(Expression.scala:503)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.Abs.doGenCode(arithmetic.scala:150)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$genCode$3(Expression.scala:146)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:141)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.BinaryExpression.nullSafeCodeGen(Expression.scala:608)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.BinaryExpression.defineCodeGen(Expression.scala:591)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.BinaryComparison.doGenCode(predicates.scala:768)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$genCode$3(Expression.scala:146)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:141)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.CaseWhen.$anonfun$multiBranchesCodegen$1(conditionalExpressions.scala:211)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n",
      "\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.CaseWhen.multiBranchesCodegen(conditionalExpressions.scala:210)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.CaseWhen.doGenCode(conditionalExpressions.scala:293)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$genCode$3(Expression.scala:146)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:141)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.$anonfun$subexpressionEliminationForWholeStageCodegen$3(CodeGenerator.scala:1051)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:392)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n",
      "\tat scala.collection.immutable.List.map(List.scala:298)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.commonExprVals$lzycompute$1(CodeGenerator.scala:1051)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.commonExprVals$1(CodeGenerator.scala:1051)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.subexpressionEliminationForWholeStageCodegen(CodeGenerator.scala:1072)\n",
      "\tat org.apache.spark.sql.execution.ProjectExec.doConsume(basicPhysicalOperators.scala:71)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:194)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:149)\n",
      "\tat org.apache.spark.sql.execution.InputAdapter.consume(WholeStageCodegenExec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.InputRDDCodegen.doProduce(WholeStageCodegenExec.scala:483)\n",
      "\tat org.apache.spark.sql.execution.InputRDDCodegen.doProduce$(WholeStageCodegenExec.scala:456)\n",
      "\tat org.apache.spark.sql.execution.InputAdapter.doProduce(WholeStageCodegenExec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n",
      "\tat org.apache.spark.sql.execution.InputAdapter.produce(WholeStageCodegenExec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:54)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n",
      "\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n",
      "\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:41)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:655)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:718)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:177)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:188)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:874)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.lang.IllegalArgumentException: All week-based patterns are unsupported since Spark 3.0, detected: Y, Please use the SQL function EXTRACT instead\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$.$anonfun$convertIncompatiblePattern$4(DateTimeFormatterHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$.$anonfun$convertIncompatiblePattern$4$adapted(DateTimeFormatterHelper.scala:321)\n",
      "\tat scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:877)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.immutable.StringOps.foreach(StringOps.scala:33)\n",
      "\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:876)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$.$anonfun$convertIncompatiblePattern$2(DateTimeFormatterHelper.scala:321)\n",
      "\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n",
      "\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$.convertIncompatiblePattern(DateTimeFormatterHelper.scala:318)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper.getOrCreateFormatter(DateTimeFormatterHelper.scala:121)\n",
      "\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper.getOrCreateFormatter$(DateTimeFormatterHelper.scala:117)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.getOrCreateFormatter(TimestampFormatter.scala:59)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.formatter$lzycompute(TimestampFormatter.scala:68)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.formatter(TimestampFormatter.scala:67)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.validatePatternString(TimestampFormatter.scala:108)\n",
      "\t... 123 more\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o133.parquet.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:231)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:188)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:874)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkUpgradeException: You may get a different result due to the upgrading of Spark 3.0: Fail to recognize 'd/m/Y' pattern in the DateTimeFormatter. 1) You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the behavior before Spark 3.0. 2) You can form a valid datetime pattern with the guide from https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkLegacyFormatter$1.applyOrElse(DateTimeFormatterHelper.scala:196)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkLegacyFormatter$1.applyOrElse(DateTimeFormatterHelper.scala:185)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.validatePatternString(TimestampFormatter.scala:109)\n\tat org.apache.spark.sql.catalyst.util.TimestampFormatter$.getFormatter(TimestampFormatter.scala:300)\n\tat org.apache.spark.sql.catalyst.util.TimestampFormatter$.apply(TimestampFormatter.scala:333)\n\tat org.apache.spark.sql.catalyst.expressions.TimestampFormatterHelper.getFormatter(datetimeExpressions.scala:72)\n\tat org.apache.spark.sql.catalyst.expressions.TimestampFormatterHelper.getFormatter$(datetimeExpressions.scala:67)\n\tat org.apache.spark.sql.catalyst.expressions.ToTimestamp.getFormatter(datetimeExpressions.scala:918)\n\tat org.apache.spark.sql.catalyst.expressions.TimestampFormatterHelper.$anonfun$formatterOption$1(datetimeExpressions.scala:64)\n\tat scala.Option.map(Option.scala:230)\n\tat org.apache.spark.sql.catalyst.expressions.TimestampFormatterHelper.formatterOption(datetimeExpressions.scala:64)\n\tat org.apache.spark.sql.catalyst.expressions.TimestampFormatterHelper.formatterOption$(datetimeExpressions.scala:62)\n\tat org.apache.spark.sql.catalyst.expressions.ToTimestamp.formatterOption$lzycompute(datetimeExpressions.scala:918)\n\tat org.apache.spark.sql.catalyst.expressions.ToTimestamp.formatterOption(datetimeExpressions.scala:918)\n\tat org.apache.spark.sql.catalyst.expressions.ToTimestamp.doGenCode(datetimeExpressions.scala:978)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$genCode$3(Expression.scala:146)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:141)\n\tat org.apache.spark.sql.catalyst.expressions.CastBase.doGenCode(Cast.scala:853)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$genCode$3(Expression.scala:146)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:141)\n\tat org.apache.spark.sql.catalyst.expressions.CastBase.genCode(Cast.scala:848)\n\tat org.apache.spark.sql.catalyst.expressions.BinaryExpression.nullSafeCodeGen(Expression.scala:608)\n\tat org.apache.spark.sql.catalyst.expressions.BinaryExpression.defineCodeGen(Expression.scala:591)\n\tat org.apache.spark.sql.catalyst.expressions.DateDiff.doGenCode(datetimeExpressions.scala:1869)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$genCode$3(Expression.scala:146)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:141)\n\tat org.apache.spark.sql.catalyst.expressions.UnaryExpression.nullSafeCodeGen(Expression.scala:519)\n\tat org.apache.spark.sql.catalyst.expressions.UnaryExpression.defineCodeGen(Expression.scala:503)\n\tat org.apache.spark.sql.catalyst.expressions.Abs.doGenCode(arithmetic.scala:150)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$genCode$3(Expression.scala:146)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:141)\n\tat org.apache.spark.sql.catalyst.expressions.BinaryExpression.nullSafeCodeGen(Expression.scala:608)\n\tat org.apache.spark.sql.catalyst.expressions.BinaryExpression.defineCodeGen(Expression.scala:591)\n\tat org.apache.spark.sql.catalyst.expressions.BinaryComparison.doGenCode(predicates.scala:768)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$genCode$3(Expression.scala:146)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:141)\n\tat org.apache.spark.sql.catalyst.expressions.CaseWhen.$anonfun$multiBranchesCodegen$1(conditionalExpressions.scala:211)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.catalyst.expressions.CaseWhen.multiBranchesCodegen(conditionalExpressions.scala:210)\n\tat org.apache.spark.sql.catalyst.expressions.CaseWhen.doGenCode(conditionalExpressions.scala:293)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$genCode$3(Expression.scala:146)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:141)\n\tat org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.$anonfun$subexpressionEliminationForWholeStageCodegen$3(CodeGenerator.scala:1051)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.immutable.List.map(List.scala:298)\n\tat org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.commonExprVals$lzycompute$1(CodeGenerator.scala:1051)\n\tat org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.commonExprVals$1(CodeGenerator.scala:1051)\n\tat org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.subexpressionEliminationForWholeStageCodegen(CodeGenerator.scala:1072)\n\tat org.apache.spark.sql.execution.ProjectExec.doConsume(basicPhysicalOperators.scala:71)\n\tat org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:194)\n\tat org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:149)\n\tat org.apache.spark.sql.execution.InputAdapter.consume(WholeStageCodegenExec.scala:496)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.doProduce(WholeStageCodegenExec.scala:483)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.doProduce$(WholeStageCodegenExec.scala:456)\n\tat org.apache.spark.sql.execution.InputAdapter.doProduce(WholeStageCodegenExec.scala:496)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.InputAdapter.produce(WholeStageCodegenExec.scala:496)\n\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:54)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:41)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:655)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:718)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:177)\n\t... 33 more\nCaused by: java.lang.IllegalArgumentException: All week-based patterns are unsupported since Spark 3.0, detected: Y, Please use the SQL function EXTRACT instead\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$.$anonfun$convertIncompatiblePattern$4(DateTimeFormatterHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$.$anonfun$convertIncompatiblePattern$4$adapted(DateTimeFormatterHelper.scala:321)\n\tat scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:877)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.immutable.StringOps.foreach(StringOps.scala:33)\n\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:876)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$.$anonfun$convertIncompatiblePattern$2(DateTimeFormatterHelper.scala:321)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$.convertIncompatiblePattern(DateTimeFormatterHelper.scala:318)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper.getOrCreateFormatter(DateTimeFormatterHelper.scala:121)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper.getOrCreateFormatter$(DateTimeFormatterHelper.scala:117)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.getOrCreateFormatter(TimestampFormatter.scala:59)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.formatter$lzycompute(TimestampFormatter.scala:68)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.formatter(TimestampFormatter.scala:67)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.validatePatternString(TimestampFormatter.scala:108)\n\t... 123 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m simple_dob_linker(df, dobs\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39m03/04/1994\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m19/02/1993\u001b[39;49m\u001b[39m'\u001b[39;49m], date_format_param\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39md/m/Y\u001b[39;49m\u001b[39m'\u001b[39;49m, Linker\u001b[39m=\u001b[39;49mLinker)\n",
      "Cell \u001b[0;32mIn[13], line 18\u001b[0m, in \u001b[0;36msimple_dob_linker\u001b[0;34m(df, dobs, date_format_param, Linker)\u001b[0m\n\u001b[1;32m     16\u001b[0m     df\u001b[39m.\u001b[39mpersist()\n\u001b[1;32m     17\u001b[0m linker \u001b[39m=\u001b[39m Linker(df, settings)   \n\u001b[0;32m---> 18\u001b[0m df_e \u001b[39m=\u001b[39m linker\u001b[39m.\u001b[39;49mpredict()\u001b[39m.\u001b[39mas_pandas_dataframe()\n\u001b[1;32m     19\u001b[0m \u001b[39mreturn\u001b[39;00m df_e\n",
      "File \u001b[0;32m~/Documents/splink/splink/linker.py:1296\u001b[0m, in \u001b[0;36mLinker.predict\u001b[0;34m(self, threshold_match_probability, threshold_match_weight, materialise_after_computing_term_frequencies)\u001b[0m\n\u001b[1;32m   1293\u001b[0m \u001b[39mfor\u001b[39;00m sql \u001b[39min\u001b[39;00m sqls:\n\u001b[1;32m   1294\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_enqueue_sql(sql[\u001b[39m\"\u001b[39m\u001b[39msql\u001b[39m\u001b[39m\"\u001b[39m], sql[\u001b[39m\"\u001b[39m\u001b[39moutput_table_name\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m-> 1296\u001b[0m predictions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_execute_sql_pipeline(input_dataframes)\n\u001b[1;32m   1297\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_predict_warning()\n\u001b[1;32m   1298\u001b[0m \u001b[39mreturn\u001b[39;00m predictions\n",
      "File \u001b[0;32m~/Documents/splink/splink/linker.py:438\u001b[0m, in \u001b[0;36mLinker._execute_sql_pipeline\u001b[0;34m(self, input_dataframes, materialise_as_hash, use_cache)\u001b[0m\n\u001b[1;32m    435\u001b[0m output_tablename_templated \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pipeline\u001b[39m.\u001b[39mqueue[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39moutput_table_name\n\u001b[1;32m    437\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 438\u001b[0m     dataframe \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sql_to_splink_dataframe_checking_cache(\n\u001b[1;32m    439\u001b[0m         sql_gen,\n\u001b[1;32m    440\u001b[0m         output_tablename_templated,\n\u001b[1;32m    441\u001b[0m         materialise_as_hash,\n\u001b[1;32m    442\u001b[0m         use_cache,\n\u001b[1;32m    443\u001b[0m     )\n\u001b[1;32m    444\u001b[0m \u001b[39mexcept\u001b[39;00m Error \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    445\u001b[0m     \u001b[39mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/Documents/splink/splink/linker.py:580\u001b[0m, in \u001b[0;36mLinker._sql_to_splink_dataframe_checking_cache\u001b[0;34m(self, sql, output_tablename_templated, materialise_as_hash, use_cache)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[39mprint\u001b[39m(sql)\n\u001b[1;32m    579\u001b[0m \u001b[39mif\u001b[39;00m materialise_as_hash:\n\u001b[0;32m--> 580\u001b[0m     splink_dataframe \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_execute_sql_against_backend(\n\u001b[1;32m    581\u001b[0m         sql, output_tablename_templated, table_name_hash\n\u001b[1;32m    582\u001b[0m     )\n\u001b[1;32m    583\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    584\u001b[0m     splink_dataframe \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_execute_sql_against_backend(\n\u001b[1;32m    585\u001b[0m         sql,\n\u001b[1;32m    586\u001b[0m         output_tablename_templated,\n\u001b[1;32m    587\u001b[0m         output_tablename_templated,\n\u001b[1;32m    588\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/splink/splink/spark/spark_linker.py:390\u001b[0m, in \u001b[0;36mSparkLinker._execute_sql_against_backend\u001b[0;34m(self, sql, templated_name, physical_name)\u001b[0m\n\u001b[1;32m    387\u001b[0m logger\u001b[39m.\u001b[39mlog(\u001b[39m5\u001b[39m, log_sql(sql))\n\u001b[1;32m    388\u001b[0m spark_df \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mspark\u001b[39m.\u001b[39msql(sql)\n\u001b[0;32m--> 390\u001b[0m spark_df \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_break_lineage_and_repartition(\n\u001b[1;32m    391\u001b[0m     spark_df, templated_name, physical_name\n\u001b[1;32m    392\u001b[0m )\n\u001b[1;32m    394\u001b[0m \u001b[39m# After blocking, want to repartition\u001b[39;00m\n\u001b[1;32m    395\u001b[0m \u001b[39m# if templated\u001b[39;00m\n\u001b[1;32m    396\u001b[0m spark_df\u001b[39m.\u001b[39mcreateOrReplaceTempView(physical_name)\n",
      "File \u001b[0;32m~/Documents/splink/splink/spark/spark_linker.py:359\u001b[0m, in \u001b[0;36mSparkLinker._break_lineage_and_repartition\u001b[0;34m(self, spark_df, templated_name, physical_name)\u001b[0m\n\u001b[1;32m    357\u001b[0m checkpoint_dir \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_checkpoint_dir_path(spark_df)\n\u001b[1;32m    358\u001b[0m write_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(checkpoint_dir, physical_name)\n\u001b[0;32m--> 359\u001b[0m spark_df\u001b[39m.\u001b[39;49mwrite\u001b[39m.\u001b[39;49mmode(\u001b[39m\"\u001b[39;49m\u001b[39moverwrite\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49mparquet(write_path)\n\u001b[1;32m    360\u001b[0m spark_df \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mspark\u001b[39m.\u001b[39mread\u001b[39m.\u001b[39mparquet(write_path)\n\u001b[1;32m    361\u001b[0m logger\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mWrote \u001b[39m\u001b[39m{\u001b[39;00mtemplated_name\u001b[39m}\u001b[39;00m\u001b[39m to parquet\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/splink_dev_env/lib/python3.10/site-packages/pyspark/sql/readwriter.py:1249\u001b[0m, in \u001b[0;36mDataFrameWriter.parquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m   1247\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpartitionBy(partitionBy)\n\u001b[1;32m   1248\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_opts(compression\u001b[39m=\u001b[39mcompression)\n\u001b[0;32m-> 1249\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jwrite\u001b[39m.\u001b[39;49mparquet(path)\n",
      "File \u001b[0;32m~/Documents/splink_dev_env/lib/python3.10/site-packages/py4j/java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1298\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1303\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1304\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1305\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1307\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1308\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/Documents/splink_dev_env/lib/python3.10/site-packages/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw):\n\u001b[1;32m    110\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    112\u001b[0m     \u001b[39mexcept\u001b[39;00m py4j\u001b[39m.\u001b[39mprotocol\u001b[39m.\u001b[39mPy4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    113\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/Documents/splink_dev_env/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o133.parquet.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:231)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:188)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:874)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkUpgradeException: You may get a different result due to the upgrading of Spark 3.0: Fail to recognize 'd/m/Y' pattern in the DateTimeFormatter. 1) You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the behavior before Spark 3.0. 2) You can form a valid datetime pattern with the guide from https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkLegacyFormatter$1.applyOrElse(DateTimeFormatterHelper.scala:196)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkLegacyFormatter$1.applyOrElse(DateTimeFormatterHelper.scala:185)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.validatePatternString(TimestampFormatter.scala:109)\n\tat org.apache.spark.sql.catalyst.util.TimestampFormatter$.getFormatter(TimestampFormatter.scala:300)\n\tat org.apache.spark.sql.catalyst.util.TimestampFormatter$.apply(TimestampFormatter.scala:333)\n\tat org.apache.spark.sql.catalyst.expressions.TimestampFormatterHelper.getFormatter(datetimeExpressions.scala:72)\n\tat org.apache.spark.sql.catalyst.expressions.TimestampFormatterHelper.getFormatter$(datetimeExpressions.scala:67)\n\tat org.apache.spark.sql.catalyst.expressions.ToTimestamp.getFormatter(datetimeExpressions.scala:918)\n\tat org.apache.spark.sql.catalyst.expressions.TimestampFormatterHelper.$anonfun$formatterOption$1(datetimeExpressions.scala:64)\n\tat scala.Option.map(Option.scala:230)\n\tat org.apache.spark.sql.catalyst.expressions.TimestampFormatterHelper.formatterOption(datetimeExpressions.scala:64)\n\tat org.apache.spark.sql.catalyst.expressions.TimestampFormatterHelper.formatterOption$(datetimeExpressions.scala:62)\n\tat org.apache.spark.sql.catalyst.expressions.ToTimestamp.formatterOption$lzycompute(datetimeExpressions.scala:918)\n\tat org.apache.spark.sql.catalyst.expressions.ToTimestamp.formatterOption(datetimeExpressions.scala:918)\n\tat org.apache.spark.sql.catalyst.expressions.ToTimestamp.doGenCode(datetimeExpressions.scala:978)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$genCode$3(Expression.scala:146)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:141)\n\tat org.apache.spark.sql.catalyst.expressions.CastBase.doGenCode(Cast.scala:853)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$genCode$3(Expression.scala:146)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:141)\n\tat org.apache.spark.sql.catalyst.expressions.CastBase.genCode(Cast.scala:848)\n\tat org.apache.spark.sql.catalyst.expressions.BinaryExpression.nullSafeCodeGen(Expression.scala:608)\n\tat org.apache.spark.sql.catalyst.expressions.BinaryExpression.defineCodeGen(Expression.scala:591)\n\tat org.apache.spark.sql.catalyst.expressions.DateDiff.doGenCode(datetimeExpressions.scala:1869)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$genCode$3(Expression.scala:146)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:141)\n\tat org.apache.spark.sql.catalyst.expressions.UnaryExpression.nullSafeCodeGen(Expression.scala:519)\n\tat org.apache.spark.sql.catalyst.expressions.UnaryExpression.defineCodeGen(Expression.scala:503)\n\tat org.apache.spark.sql.catalyst.expressions.Abs.doGenCode(arithmetic.scala:150)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$genCode$3(Expression.scala:146)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:141)\n\tat org.apache.spark.sql.catalyst.expressions.BinaryExpression.nullSafeCodeGen(Expression.scala:608)\n\tat org.apache.spark.sql.catalyst.expressions.BinaryExpression.defineCodeGen(Expression.scala:591)\n\tat org.apache.spark.sql.catalyst.expressions.BinaryComparison.doGenCode(predicates.scala:768)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$genCode$3(Expression.scala:146)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:141)\n\tat org.apache.spark.sql.catalyst.expressions.CaseWhen.$anonfun$multiBranchesCodegen$1(conditionalExpressions.scala:211)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:108)\n\tat org.apache.spark.sql.catalyst.expressions.CaseWhen.multiBranchesCodegen(conditionalExpressions.scala:210)\n\tat org.apache.spark.sql.catalyst.expressions.CaseWhen.doGenCode(conditionalExpressions.scala:293)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$genCode$3(Expression.scala:146)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:141)\n\tat org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.$anonfun$subexpressionEliminationForWholeStageCodegen$3(CodeGenerator.scala:1051)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.immutable.List.map(List.scala:298)\n\tat org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.commonExprVals$lzycompute$1(CodeGenerator.scala:1051)\n\tat org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.commonExprVals$1(CodeGenerator.scala:1051)\n\tat org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.subexpressionEliminationForWholeStageCodegen(CodeGenerator.scala:1072)\n\tat org.apache.spark.sql.execution.ProjectExec.doConsume(basicPhysicalOperators.scala:71)\n\tat org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:194)\n\tat org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:149)\n\tat org.apache.spark.sql.execution.InputAdapter.consume(WholeStageCodegenExec.scala:496)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.doProduce(WholeStageCodegenExec.scala:483)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.doProduce$(WholeStageCodegenExec.scala:456)\n\tat org.apache.spark.sql.execution.InputAdapter.doProduce(WholeStageCodegenExec.scala:496)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.InputAdapter.produce(WholeStageCodegenExec.scala:496)\n\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:54)\n\tat org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:41)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:655)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:718)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:177)\n\t... 33 more\nCaused by: java.lang.IllegalArgumentException: All week-based patterns are unsupported since Spark 3.0, detected: Y, Please use the SQL function EXTRACT instead\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$.$anonfun$convertIncompatiblePattern$4(DateTimeFormatterHelper.scala:323)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$.$anonfun$convertIncompatiblePattern$4$adapted(DateTimeFormatterHelper.scala:321)\n\tat scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:877)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.immutable.StringOps.foreach(StringOps.scala:33)\n\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:876)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$.$anonfun$convertIncompatiblePattern$2(DateTimeFormatterHelper.scala:321)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:238)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:231)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$.convertIncompatiblePattern(DateTimeFormatterHelper.scala:318)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper.getOrCreateFormatter(DateTimeFormatterHelper.scala:121)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper.getOrCreateFormatter$(DateTimeFormatterHelper.scala:117)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.getOrCreateFormatter(TimestampFormatter.scala:59)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.formatter$lzycompute(TimestampFormatter.scala:68)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.formatter(TimestampFormatter.scala:67)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.validatePatternString(TimestampFormatter.scala:108)\n\t... 123 more\n"
     ]
    }
   ],
   "source": [
    "simple_dob_linker(df, dobs=['03/04/1994', '19/02/1993'], date_format_param='d/m/Y', Linker=Linker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with pytest.raises(duckdb.InvalidInputException):\n",
    "#     simple_linker(df, settings, Linker)\n",
    "simple_dob_linker(df, dobs=['03/04/1994', '19/02/1993'], date_format_param='%d/%m/%Y', Linker=Linker)\n",
    "simple_dob_linker(df, dobs=['03-04-1994', '19-02-1993'], date_format_param='%d-%m-%Y', Linker=Linker)\n",
    "simple_dob_linker(df, dobs=['04/05/1994', '10/02/1993'], date_format_param='%m/%d/%Y', Linker=Linker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Linker == SparkLinker:\n",
    "    valid_date_formats = ['dd/mm/Y', 'dd-mm-Y', 'mm/dd/Y', 'Y/mm/dd']\n",
    "elif Linker == DuckDBLinker:\n",
    "    valid_date_formats = ['%d/%m/%Y', '%d-%m-%Y', '%m/%d/%Y', '%Y/%m/%d']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alice.oleary/Documents/splink_dev_env/lib/python3.10/site-packages/pyspark/sql/pandas/utils.py:34: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n",
      "/Users/alice.oleary/Documents/splink_dev_env/lib/python3.10/site-packages/pyspark/sql/pandas/utils.py:34: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n",
      "/Users/alice.oleary/Documents/splink_dev_env/lib/python3.10/site-packages/pyspark/sql/pandas/utils.py:34: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n",
      "/Users/alice.oleary/Documents/splink_dev_env/lib/python3.10/site-packages/pyspark/sql/pandas/utils.py:34: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n",
      "/Users/alice.oleary/Documents/splink_dev_env/lib/python3.10/site-packages/pyspark/sql/pandas/utils.py:34: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n",
      "/Users/alice.oleary/Documents/splink_dev_env/lib/python3.10/site-packages/pyspark/sql/pandas/utils.py:34: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n",
      "/Users/alice.oleary/Documents/splink_dev_env/lib/python3.10/site-packages/pyspark/sql/pandas/utils.py:34: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n",
      "/Users/alice.oleary/Documents/splink_dev_env/lib/python3.10/site-packages/pyspark/sql/pandas/utils.py:34: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n",
      "23/03/21 14:52:02 WARN SimpleFunctionRegistry: The function jaro_winkler replaced a previously registered function.\n",
      "23/03/21 14:52:02 WARN SimpleFunctionRegistry: The function jaccard replaced a previously registered function.\n",
      "23/03/21 14:52:02 WARN SimpleFunctionRegistry: The function cosine_distance replaced a previously registered function.\n",
      "23/03/21 14:52:02 WARN SimpleFunctionRegistry: The function dmetaphone replaced a previously registered function.\n",
      "23/03/21 14:52:02 WARN SimpleFunctionRegistry: The function dmetaphonealt replaced a previously registered function.\n",
      "23/03/21 14:52:02 WARN SimpleFunctionRegistry: The function qgramtokeniser replaced a previously registered function.\n",
      "23/03/21 14:52:02 WARN DataSource: All paths were ignored:\n",
      "  file:/Users/alice.oleary/Documents/spark_checkpoint_dir/ffb6d686-aa41-478e-a33a-b015897210c2/__splink__df_concat_with_tf_c85d9f0f1\n",
      "23/03/21 14:52:03 ERROR Executor: Exception in task 1.0 in stage 48.0 (TID 469)\n",
      "java.time.format.DateTimeParseException: Text '1994-14-15' could not be parsed: Invalid value for MonthOfYear (valid values 1 - 12): 14\n",
      "\tat java.time.format.DateTimeFormatter.createError(DateTimeFormatter.java:1920)\n",
      "\tat java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1781)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.$anonfun$parse$1(TimestampFormatter.scala:78)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:77)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.project_subExpr_3$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:265)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.time.DateTimeException: Invalid value for MonthOfYear (valid values 1 - 12): 14\n",
      "\tat java.time.temporal.ValueRange.checkValidIntValue(ValueRange.java:330)\n",
      "\tat java.time.temporal.ChronoField.checkValidIntValue(ChronoField.java:722)\n",
      "\tat java.time.chrono.IsoChronology.resolveYMD(IsoChronology.java:550)\n",
      "\tat java.time.chrono.IsoChronology.resolveYMD(IsoChronology.java:123)\n",
      "\tat java.time.chrono.AbstractChronology.resolveDate(AbstractChronology.java:472)\n",
      "\tat java.time.chrono.IsoChronology.resolveDate(IsoChronology.java:492)\n",
      "\tat java.time.chrono.IsoChronology.resolveDate(IsoChronology.java:123)\n",
      "\tat java.time.format.Parsed.resolveDateFields(Parsed.java:352)\n",
      "\tat java.time.format.Parsed.resolveFields(Parsed.java:257)\n",
      "\tat java.time.format.Parsed.resolve(Parsed.java:244)\n",
      "\tat java.time.format.DateTimeParseContext.toResolved(DateTimeParseContext.java:331)\n",
      "\tat java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:1955)\n",
      "\tat java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1777)\n",
      "\t... 18 more\n",
      "23/03/21 14:52:03 WARN TaskSetManager: Lost task 1.0 in stage 48.0 (TID 469) (192.168.0.3 executor driver): java.time.format.DateTimeParseException: Text '1994-14-15' could not be parsed: Invalid value for MonthOfYear (valid values 1 - 12): 14\n",
      "\tat java.time.format.DateTimeFormatter.createError(DateTimeFormatter.java:1920)\n",
      "\tat java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1781)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.$anonfun$parse$1(TimestampFormatter.scala:78)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:77)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.project_subExpr_3$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:265)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.time.DateTimeException: Invalid value for MonthOfYear (valid values 1 - 12): 14\n",
      "\tat java.time.temporal.ValueRange.checkValidIntValue(ValueRange.java:330)\n",
      "\tat java.time.temporal.ChronoField.checkValidIntValue(ChronoField.java:722)\n",
      "\tat java.time.chrono.IsoChronology.resolveYMD(IsoChronology.java:550)\n",
      "\tat java.time.chrono.IsoChronology.resolveYMD(IsoChronology.java:123)\n",
      "\tat java.time.chrono.AbstractChronology.resolveDate(AbstractChronology.java:472)\n",
      "\tat java.time.chrono.IsoChronology.resolveDate(IsoChronology.java:492)\n",
      "\tat java.time.chrono.IsoChronology.resolveDate(IsoChronology.java:123)\n",
      "\tat java.time.format.Parsed.resolveDateFields(Parsed.java:352)\n",
      "\tat java.time.format.Parsed.resolveFields(Parsed.java:257)\n",
      "\tat java.time.format.Parsed.resolve(Parsed.java:244)\n",
      "\tat java.time.format.DateTimeParseContext.toResolved(DateTimeParseContext.java:331)\n",
      "\tat java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:1955)\n",
      "\tat java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1777)\n",
      "\t... 18 more\n",
      "\n",
      "23/03/21 14:52:03 ERROR TaskSetManager: Task 1 in stage 48.0 failed 1 times; aborting job\n",
      "23/03/21 14:52:03 ERROR FileFormatWriter: Aborting job 3b86d403-b2bb-40f6-b878-72fc6a660cea.\n",
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 48.0 failed 1 times, most recent failure: Lost task 1.0 in stage 48.0 (TID 469) (192.168.0.3 executor driver): java.time.format.DateTimeParseException: Text '1994-14-15' could not be parsed: Invalid value for MonthOfYear (valid values 1 - 12): 14\n",
      "\tat java.time.format.DateTimeFormatter.createError(DateTimeFormatter.java:1920)\n",
      "\tat java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1781)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.$anonfun$parse$1(TimestampFormatter.scala:78)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:77)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.project_subExpr_3$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:265)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.time.DateTimeException: Invalid value for MonthOfYear (valid values 1 - 12): 14\n",
      "\tat java.time.temporal.ValueRange.checkValidIntValue(ValueRange.java:330)\n",
      "\tat java.time.temporal.ChronoField.checkValidIntValue(ChronoField.java:722)\n",
      "\tat java.time.chrono.IsoChronology.resolveYMD(IsoChronology.java:550)\n",
      "\tat java.time.chrono.IsoChronology.resolveYMD(IsoChronology.java:123)\n",
      "\tat java.time.chrono.AbstractChronology.resolveDate(AbstractChronology.java:472)\n",
      "\tat java.time.chrono.IsoChronology.resolveDate(IsoChronology.java:492)\n",
      "\tat java.time.chrono.IsoChronology.resolveDate(IsoChronology.java:123)\n",
      "\tat java.time.format.Parsed.resolveDateFields(Parsed.java:352)\n",
      "\tat java.time.format.Parsed.resolveFields(Parsed.java:257)\n",
      "\tat java.time.format.Parsed.resolve(Parsed.java:244)\n",
      "\tat java.time.format.DateTimeParseContext.toResolved(DateTimeParseContext.java:331)\n",
      "\tat java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:1955)\n",
      "\tat java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1777)\n",
      "\t... 18 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:200)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:188)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:874)\n",
      "\tat sun.reflect.GeneratedMethodAccessor198.invoke(Unknown Source)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.time.format.DateTimeParseException: Text '1994-14-15' could not be parsed: Invalid value for MonthOfYear (valid values 1 - 12): 14\n",
      "\tat java.time.format.DateTimeFormatter.createError(DateTimeFormatter.java:1920)\n",
      "\tat java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1781)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.$anonfun$parse$1(TimestampFormatter.scala:78)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:77)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.project_subExpr_3$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:265)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "Caused by: java.time.DateTimeException: Invalid value for MonthOfYear (valid values 1 - 12): 14\n",
      "\tat java.time.temporal.ValueRange.checkValidIntValue(ValueRange.java:330)\n",
      "\tat java.time.temporal.ChronoField.checkValidIntValue(ChronoField.java:722)\n",
      "\tat java.time.chrono.IsoChronology.resolveYMD(IsoChronology.java:550)\n",
      "\tat java.time.chrono.IsoChronology.resolveYMD(IsoChronology.java:123)\n",
      "\tat java.time.chrono.AbstractChronology.resolveDate(AbstractChronology.java:472)\n",
      "\tat java.time.chrono.IsoChronology.resolveDate(IsoChronology.java:492)\n",
      "\tat java.time.chrono.IsoChronology.resolveDate(IsoChronology.java:123)\n",
      "\tat java.time.format.Parsed.resolveDateFields(Parsed.java:352)\n",
      "\tat java.time.format.Parsed.resolveFields(Parsed.java:257)\n",
      "\tat java.time.format.Parsed.resolve(Parsed.java:244)\n",
      "\tat java.time.format.DateTimeParseContext.toResolved(DateTimeParseContext.java:331)\n",
      "\tat java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:1955)\n",
      "\tat java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1777)\n",
      "\t... 18 more\n",
      "23/03/21 14:52:03 WARN TaskSetManager: Lost task 0.0 in stage 48.0 (TID 468) (192.168.0.3 executor driver): TaskKilled (Stage cancelled)\n"
     ]
    }
   ],
   "source": [
    "with pytest.raises(py4j.protocol.Py4JJavaError):\n",
    "    simple_dob_linker(df, dobs=['1994-14-15', '1994-12-03'], \n",
    "                        date_format_param='y-M-d', Linker=Linker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import py4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alice.oleary/Documents/splink_dev_env/lib/python3.10/site-packages/pyspark/sql/pandas/utils.py:34: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n",
      "/Users/alice.oleary/Documents/splink_dev_env/lib/python3.10/site-packages/pyspark/sql/pandas/utils.py:34: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n",
      "/Users/alice.oleary/Documents/splink_dev_env/lib/python3.10/site-packages/pyspark/sql/pandas/utils.py:34: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n",
      "/Users/alice.oleary/Documents/splink_dev_env/lib/python3.10/site-packages/pyspark/sql/pandas/utils.py:34: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n",
      "/Users/alice.oleary/Documents/splink_dev_env/lib/python3.10/site-packages/pyspark/sql/pandas/utils.py:34: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n",
      "/Users/alice.oleary/Documents/splink_dev_env/lib/python3.10/site-packages/pyspark/sql/pandas/utils.py:34: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n",
      "/Users/alice.oleary/Documents/splink_dev_env/lib/python3.10/site-packages/pyspark/sql/pandas/utils.py:34: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n",
      "/Users/alice.oleary/Documents/splink_dev_env/lib/python3.10/site-packages/pyspark/sql/pandas/utils.py:34: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n",
      "23/03/21 14:48:47 WARN SimpleFunctionRegistry: The function jaro_winkler replaced a previously registered function.\n",
      "23/03/21 14:48:47 WARN SimpleFunctionRegistry: The function jaccard replaced a previously registered function.\n",
      "23/03/21 14:48:47 WARN SimpleFunctionRegistry: The function cosine_distance replaced a previously registered function.\n",
      "23/03/21 14:48:47 WARN SimpleFunctionRegistry: The function dmetaphone replaced a previously registered function.\n",
      "23/03/21 14:48:47 WARN SimpleFunctionRegistry: The function dmetaphonealt replaced a previously registered function.\n",
      "23/03/21 14:48:47 WARN SimpleFunctionRegistry: The function qgramtokeniser replaced a previously registered function.\n",
      "23/03/21 14:48:47 WARN DataSource: All paths were ignored:\n",
      "  file:/Users/alice.oleary/Documents/spark_checkpoint_dir/ffb6d686-aa41-478e-a33a-b015897210c2/__splink__df_concat_with_tf_73c05f65b\n",
      "23/03/21 14:48:48 ERROR Executor: Exception in task 1.0 in stage 43.0 (TID 421)\n",
      "java.time.format.DateTimeParseException: Text '1994/14/15' could not be parsed: Invalid value for MonthOfYear (valid values 1 - 12): 14\n",
      "\tat java.time.format.DateTimeFormatter.createError(DateTimeFormatter.java:1920)\n",
      "\tat java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1781)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.$anonfun$parse$1(TimestampFormatter.scala:78)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:77)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.project_subExpr_3$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:265)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.time.DateTimeException: Invalid value for MonthOfYear (valid values 1 - 12): 14\n",
      "\tat java.time.temporal.ValueRange.checkValidIntValue(ValueRange.java:330)\n",
      "\tat java.time.temporal.ChronoField.checkValidIntValue(ChronoField.java:722)\n",
      "\tat java.time.chrono.IsoChronology.resolveYMD(IsoChronology.java:550)\n",
      "\tat java.time.chrono.IsoChronology.resolveYMD(IsoChronology.java:123)\n",
      "\tat java.time.chrono.AbstractChronology.resolveDate(AbstractChronology.java:472)\n",
      "\tat java.time.chrono.IsoChronology.resolveDate(IsoChronology.java:492)\n",
      "\tat java.time.chrono.IsoChronology.resolveDate(IsoChronology.java:123)\n",
      "\tat java.time.format.Parsed.resolveDateFields(Parsed.java:352)\n",
      "\tat java.time.format.Parsed.resolveFields(Parsed.java:257)\n",
      "\tat java.time.format.Parsed.resolve(Parsed.java:244)\n",
      "\tat java.time.format.DateTimeParseContext.toResolved(DateTimeParseContext.java:331)\n",
      "\tat java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:1955)\n",
      "\tat java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1777)\n",
      "\t... 18 more\n",
      "23/03/21 14:48:48 WARN TaskSetManager: Lost task 1.0 in stage 43.0 (TID 421) (192.168.0.3 executor driver): java.time.format.DateTimeParseException: Text '1994/14/15' could not be parsed: Invalid value for MonthOfYear (valid values 1 - 12): 14\n",
      "\tat java.time.format.DateTimeFormatter.createError(DateTimeFormatter.java:1920)\n",
      "\tat java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1781)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.$anonfun$parse$1(TimestampFormatter.scala:78)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:77)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.project_subExpr_3$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:265)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.time.DateTimeException: Invalid value for MonthOfYear (valid values 1 - 12): 14\n",
      "\tat java.time.temporal.ValueRange.checkValidIntValue(ValueRange.java:330)\n",
      "\tat java.time.temporal.ChronoField.checkValidIntValue(ChronoField.java:722)\n",
      "\tat java.time.chrono.IsoChronology.resolveYMD(IsoChronology.java:550)\n",
      "\tat java.time.chrono.IsoChronology.resolveYMD(IsoChronology.java:123)\n",
      "\tat java.time.chrono.AbstractChronology.resolveDate(AbstractChronology.java:472)\n",
      "\tat java.time.chrono.IsoChronology.resolveDate(IsoChronology.java:492)\n",
      "\tat java.time.chrono.IsoChronology.resolveDate(IsoChronology.java:123)\n",
      "\tat java.time.format.Parsed.resolveDateFields(Parsed.java:352)\n",
      "\tat java.time.format.Parsed.resolveFields(Parsed.java:257)\n",
      "\tat java.time.format.Parsed.resolve(Parsed.java:244)\n",
      "\tat java.time.format.DateTimeParseContext.toResolved(DateTimeParseContext.java:331)\n",
      "\tat java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:1955)\n",
      "\tat java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1777)\n",
      "\t... 18 more\n",
      "\n",
      "23/03/21 14:48:48 ERROR TaskSetManager: Task 1 in stage 43.0 failed 1 times; aborting job\n",
      "23/03/21 14:48:48 ERROR FileFormatWriter: Aborting job 25e8a506-9329-46c4-8350-f5388b5a75eb.\n",
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 43.0 failed 1 times, most recent failure: Lost task 1.0 in stage 43.0 (TID 421) (192.168.0.3 executor driver): java.time.format.DateTimeParseException: Text '1994/14/15' could not be parsed: Invalid value for MonthOfYear (valid values 1 - 12): 14\n",
      "\tat java.time.format.DateTimeFormatter.createError(DateTimeFormatter.java:1920)\n",
      "\tat java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1781)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.$anonfun$parse$1(TimestampFormatter.scala:78)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:77)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.project_subExpr_3$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:265)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.time.DateTimeException: Invalid value for MonthOfYear (valid values 1 - 12): 14\n",
      "\tat java.time.temporal.ValueRange.checkValidIntValue(ValueRange.java:330)\n",
      "\tat java.time.temporal.ChronoField.checkValidIntValue(ChronoField.java:722)\n",
      "\tat java.time.chrono.IsoChronology.resolveYMD(IsoChronology.java:550)\n",
      "\tat java.time.chrono.IsoChronology.resolveYMD(IsoChronology.java:123)\n",
      "\tat java.time.chrono.AbstractChronology.resolveDate(AbstractChronology.java:472)\n",
      "\tat java.time.chrono.IsoChronology.resolveDate(IsoChronology.java:492)\n",
      "\tat java.time.chrono.IsoChronology.resolveDate(IsoChronology.java:123)\n",
      "\tat java.time.format.Parsed.resolveDateFields(Parsed.java:352)\n",
      "\tat java.time.format.Parsed.resolveFields(Parsed.java:257)\n",
      "\tat java.time.format.Parsed.resolve(Parsed.java:244)\n",
      "\tat java.time.format.DateTimeParseContext.toResolved(DateTimeParseContext.java:331)\n",
      "\tat java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:1955)\n",
      "\tat java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1777)\n",
      "\t... 18 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:200)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:188)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:874)\n",
      "\tat sun.reflect.GeneratedMethodAccessor198.invoke(Unknown Source)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.time.format.DateTimeParseException: Text '1994/14/15' could not be parsed: Invalid value for MonthOfYear (valid values 1 - 12): 14\n",
      "\tat java.time.format.DateTimeFormatter.createError(DateTimeFormatter.java:1920)\n",
      "\tat java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1781)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.$anonfun$parse$1(TimestampFormatter.scala:78)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:77)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.project_subExpr_3$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:265)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "Caused by: java.time.DateTimeException: Invalid value for MonthOfYear (valid values 1 - 12): 14\n",
      "\tat java.time.temporal.ValueRange.checkValidIntValue(ValueRange.java:330)\n",
      "\tat java.time.temporal.ChronoField.checkValidIntValue(ChronoField.java:722)\n",
      "\tat java.time.chrono.IsoChronology.resolveYMD(IsoChronology.java:550)\n",
      "\tat java.time.chrono.IsoChronology.resolveYMD(IsoChronology.java:123)\n",
      "\tat java.time.chrono.AbstractChronology.resolveDate(AbstractChronology.java:472)\n",
      "\tat java.time.chrono.IsoChronology.resolveDate(IsoChronology.java:492)\n",
      "\tat java.time.chrono.IsoChronology.resolveDate(IsoChronology.java:123)\n",
      "\tat java.time.format.Parsed.resolveDateFields(Parsed.java:352)\n",
      "\tat java.time.format.Parsed.resolveFields(Parsed.java:257)\n",
      "\tat java.time.format.Parsed.resolve(Parsed.java:244)\n",
      "\tat java.time.format.DateTimeParseContext.toResolved(DateTimeParseContext.java:331)\n",
      "\tat java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:1955)\n",
      "\tat java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1777)\n",
      "\t... 18 more\n",
      "23/03/21 14:48:48 WARN TaskSetManager: Lost task 0.0 in stage 43.0 (TID 420) (192.168.0.3 executor driver): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1008.parquet.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:231)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:188)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:874)\n\tat sun.reflect.GeneratedMethodAccessor198.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 43.0 failed 1 times, most recent failure: Lost task 1.0 in stage 43.0 (TID 421) (192.168.0.3 executor driver): java.time.format.DateTimeParseException: Text '1994/14/15' could not be parsed: Invalid value for MonthOfYear (valid values 1 - 12): 14\n\tat java.time.format.DateTimeFormatter.createError(DateTimeFormatter.java:1920)\n\tat java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1781)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.$anonfun$parse$1(TimestampFormatter.scala:78)\n\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:77)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.project_subExpr_3$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:265)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.time.DateTimeException: Invalid value for MonthOfYear (valid values 1 - 12): 14\n\tat java.time.temporal.ValueRange.checkValidIntValue(ValueRange.java:330)\n\tat java.time.temporal.ChronoField.checkValidIntValue(ChronoField.java:722)\n\tat java.time.chrono.IsoChronology.resolveYMD(IsoChronology.java:550)\n\tat java.time.chrono.IsoChronology.resolveYMD(IsoChronology.java:123)\n\tat java.time.chrono.AbstractChronology.resolveDate(AbstractChronology.java:472)\n\tat java.time.chrono.IsoChronology.resolveDate(IsoChronology.java:492)\n\tat java.time.chrono.IsoChronology.resolveDate(IsoChronology.java:123)\n\tat java.time.format.Parsed.resolveDateFields(Parsed.java:352)\n\tat java.time.format.Parsed.resolveFields(Parsed.java:257)\n\tat java.time.format.Parsed.resolve(Parsed.java:244)\n\tat java.time.format.DateTimeParseContext.toResolved(DateTimeParseContext.java:331)\n\tat java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:1955)\n\tat java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1777)\n\t... 18 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:200)\n\t... 32 more\nCaused by: java.time.format.DateTimeParseException: Text '1994/14/15' could not be parsed: Invalid value for MonthOfYear (valid values 1 - 12): 14\n\tat java.time.format.DateTimeFormatter.createError(DateTimeFormatter.java:1920)\n\tat java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1781)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.$anonfun$parse$1(TimestampFormatter.scala:78)\n\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:77)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.project_subExpr_3$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:265)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.time.DateTimeException: Invalid value for MonthOfYear (valid values 1 - 12): 14\n\tat java.time.temporal.ValueRange.checkValidIntValue(ValueRange.java:330)\n\tat java.time.temporal.ChronoField.checkValidIntValue(ChronoField.java:722)\n\tat java.time.chrono.IsoChronology.resolveYMD(IsoChronology.java:550)\n\tat java.time.chrono.IsoChronology.resolveYMD(IsoChronology.java:123)\n\tat java.time.chrono.AbstractChronology.resolveDate(AbstractChronology.java:472)\n\tat java.time.chrono.IsoChronology.resolveDate(IsoChronology.java:492)\n\tat java.time.chrono.IsoChronology.resolveDate(IsoChronology.java:123)\n\tat java.time.format.Parsed.resolveDateFields(Parsed.java:352)\n\tat java.time.format.Parsed.resolveFields(Parsed.java:257)\n\tat java.time.format.Parsed.resolve(Parsed.java:244)\n\tat java.time.format.DateTimeParseContext.toResolved(DateTimeParseContext.java:331)\n\tat java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:1955)\n\tat java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1777)\n\t... 18 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m simple_dob_linker(df, dobs\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39m1994/14/15\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m1994/12/03\u001b[39;49m\u001b[39m'\u001b[39;49m], \n\u001b[1;32m      2\u001b[0m                     date_format_param\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39my/M/d\u001b[39;49m\u001b[39m'\u001b[39;49m, Linker\u001b[39m=\u001b[39;49mLinker)\n",
      "Cell \u001b[0;32mIn[13], line 18\u001b[0m, in \u001b[0;36msimple_dob_linker\u001b[0;34m(df, dobs, date_format_param, Linker)\u001b[0m\n\u001b[1;32m     16\u001b[0m     df\u001b[39m.\u001b[39mpersist()\n\u001b[1;32m     17\u001b[0m linker \u001b[39m=\u001b[39m Linker(df, settings)   \n\u001b[0;32m---> 18\u001b[0m df_e \u001b[39m=\u001b[39m linker\u001b[39m.\u001b[39;49mpredict()\u001b[39m.\u001b[39mas_pandas_dataframe()\n\u001b[1;32m     19\u001b[0m \u001b[39mreturn\u001b[39;00m df_e\n",
      "File \u001b[0;32m~/Documents/splink/splink/linker.py:1296\u001b[0m, in \u001b[0;36mLinker.predict\u001b[0;34m(self, threshold_match_probability, threshold_match_weight, materialise_after_computing_term_frequencies)\u001b[0m\n\u001b[1;32m   1293\u001b[0m \u001b[39mfor\u001b[39;00m sql \u001b[39min\u001b[39;00m sqls:\n\u001b[1;32m   1294\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_enqueue_sql(sql[\u001b[39m\"\u001b[39m\u001b[39msql\u001b[39m\u001b[39m\"\u001b[39m], sql[\u001b[39m\"\u001b[39m\u001b[39moutput_table_name\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m-> 1296\u001b[0m predictions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_execute_sql_pipeline(input_dataframes)\n\u001b[1;32m   1297\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_predict_warning()\n\u001b[1;32m   1298\u001b[0m \u001b[39mreturn\u001b[39;00m predictions\n",
      "File \u001b[0;32m~/Documents/splink/splink/linker.py:438\u001b[0m, in \u001b[0;36mLinker._execute_sql_pipeline\u001b[0;34m(self, input_dataframes, materialise_as_hash, use_cache)\u001b[0m\n\u001b[1;32m    435\u001b[0m output_tablename_templated \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pipeline\u001b[39m.\u001b[39mqueue[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39moutput_table_name\n\u001b[1;32m    437\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 438\u001b[0m     dataframe \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sql_to_splink_dataframe_checking_cache(\n\u001b[1;32m    439\u001b[0m         sql_gen,\n\u001b[1;32m    440\u001b[0m         output_tablename_templated,\n\u001b[1;32m    441\u001b[0m         materialise_as_hash,\n\u001b[1;32m    442\u001b[0m         use_cache,\n\u001b[1;32m    443\u001b[0m     )\n\u001b[1;32m    444\u001b[0m \u001b[39mexcept\u001b[39;00m Error \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    445\u001b[0m     \u001b[39mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/Documents/splink/splink/linker.py:580\u001b[0m, in \u001b[0;36mLinker._sql_to_splink_dataframe_checking_cache\u001b[0;34m(self, sql, output_tablename_templated, materialise_as_hash, use_cache)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[39mprint\u001b[39m(sql)\n\u001b[1;32m    579\u001b[0m \u001b[39mif\u001b[39;00m materialise_as_hash:\n\u001b[0;32m--> 580\u001b[0m     splink_dataframe \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_execute_sql_against_backend(\n\u001b[1;32m    581\u001b[0m         sql, output_tablename_templated, table_name_hash\n\u001b[1;32m    582\u001b[0m     )\n\u001b[1;32m    583\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    584\u001b[0m     splink_dataframe \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_execute_sql_against_backend(\n\u001b[1;32m    585\u001b[0m         sql,\n\u001b[1;32m    586\u001b[0m         output_tablename_templated,\n\u001b[1;32m    587\u001b[0m         output_tablename_templated,\n\u001b[1;32m    588\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/splink/splink/spark/spark_linker.py:390\u001b[0m, in \u001b[0;36mSparkLinker._execute_sql_against_backend\u001b[0;34m(self, sql, templated_name, physical_name)\u001b[0m\n\u001b[1;32m    387\u001b[0m logger\u001b[39m.\u001b[39mlog(\u001b[39m5\u001b[39m, log_sql(sql))\n\u001b[1;32m    388\u001b[0m spark_df \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mspark\u001b[39m.\u001b[39msql(sql)\n\u001b[0;32m--> 390\u001b[0m spark_df \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_break_lineage_and_repartition(\n\u001b[1;32m    391\u001b[0m     spark_df, templated_name, physical_name\n\u001b[1;32m    392\u001b[0m )\n\u001b[1;32m    394\u001b[0m \u001b[39m# After blocking, want to repartition\u001b[39;00m\n\u001b[1;32m    395\u001b[0m \u001b[39m# if templated\u001b[39;00m\n\u001b[1;32m    396\u001b[0m spark_df\u001b[39m.\u001b[39mcreateOrReplaceTempView(physical_name)\n",
      "File \u001b[0;32m~/Documents/splink/splink/spark/spark_linker.py:359\u001b[0m, in \u001b[0;36mSparkLinker._break_lineage_and_repartition\u001b[0;34m(self, spark_df, templated_name, physical_name)\u001b[0m\n\u001b[1;32m    357\u001b[0m checkpoint_dir \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_checkpoint_dir_path(spark_df)\n\u001b[1;32m    358\u001b[0m write_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(checkpoint_dir, physical_name)\n\u001b[0;32m--> 359\u001b[0m spark_df\u001b[39m.\u001b[39;49mwrite\u001b[39m.\u001b[39;49mmode(\u001b[39m\"\u001b[39;49m\u001b[39moverwrite\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49mparquet(write_path)\n\u001b[1;32m    360\u001b[0m spark_df \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mspark\u001b[39m.\u001b[39mread\u001b[39m.\u001b[39mparquet(write_path)\n\u001b[1;32m    361\u001b[0m logger\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mWrote \u001b[39m\u001b[39m{\u001b[39;00mtemplated_name\u001b[39m}\u001b[39;00m\u001b[39m to parquet\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/splink_dev_env/lib/python3.10/site-packages/pyspark/sql/readwriter.py:1249\u001b[0m, in \u001b[0;36mDataFrameWriter.parquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m   1247\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpartitionBy(partitionBy)\n\u001b[1;32m   1248\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_opts(compression\u001b[39m=\u001b[39mcompression)\n\u001b[0;32m-> 1249\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jwrite\u001b[39m.\u001b[39;49mparquet(path)\n",
      "File \u001b[0;32m~/Documents/splink_dev_env/lib/python3.10/site-packages/py4j/java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1298\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1303\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1304\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1305\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1307\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1308\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/Documents/splink_dev_env/lib/python3.10/site-packages/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw):\n\u001b[1;32m    110\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    112\u001b[0m     \u001b[39mexcept\u001b[39;00m py4j\u001b[39m.\u001b[39mprotocol\u001b[39m.\u001b[39mPy4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    113\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/Documents/splink_dev_env/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1008.parquet.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:231)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:188)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:874)\n\tat sun.reflect.GeneratedMethodAccessor198.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 43.0 failed 1 times, most recent failure: Lost task 1.0 in stage 43.0 (TID 421) (192.168.0.3 executor driver): java.time.format.DateTimeParseException: Text '1994/14/15' could not be parsed: Invalid value for MonthOfYear (valid values 1 - 12): 14\n\tat java.time.format.DateTimeFormatter.createError(DateTimeFormatter.java:1920)\n\tat java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1781)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.$anonfun$parse$1(TimestampFormatter.scala:78)\n\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:77)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.project_subExpr_3$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:265)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.time.DateTimeException: Invalid value for MonthOfYear (valid values 1 - 12): 14\n\tat java.time.temporal.ValueRange.checkValidIntValue(ValueRange.java:330)\n\tat java.time.temporal.ChronoField.checkValidIntValue(ChronoField.java:722)\n\tat java.time.chrono.IsoChronology.resolveYMD(IsoChronology.java:550)\n\tat java.time.chrono.IsoChronology.resolveYMD(IsoChronology.java:123)\n\tat java.time.chrono.AbstractChronology.resolveDate(AbstractChronology.java:472)\n\tat java.time.chrono.IsoChronology.resolveDate(IsoChronology.java:492)\n\tat java.time.chrono.IsoChronology.resolveDate(IsoChronology.java:123)\n\tat java.time.format.Parsed.resolveDateFields(Parsed.java:352)\n\tat java.time.format.Parsed.resolveFields(Parsed.java:257)\n\tat java.time.format.Parsed.resolve(Parsed.java:244)\n\tat java.time.format.DateTimeParseContext.toResolved(DateTimeParseContext.java:331)\n\tat java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:1955)\n\tat java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1777)\n\t... 18 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:200)\n\t... 32 more\nCaused by: java.time.format.DateTimeParseException: Text '1994/14/15' could not be parsed: Invalid value for MonthOfYear (valid values 1 - 12): 14\n\tat java.time.format.DateTimeFormatter.createError(DateTimeFormatter.java:1920)\n\tat java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1781)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.$anonfun$parse$1(TimestampFormatter.scala:78)\n\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:77)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.project_subExpr_3$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:265)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:210)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.time.DateTimeException: Invalid value for MonthOfYear (valid values 1 - 12): 14\n\tat java.time.temporal.ValueRange.checkValidIntValue(ValueRange.java:330)\n\tat java.time.temporal.ChronoField.checkValidIntValue(ChronoField.java:722)\n\tat java.time.chrono.IsoChronology.resolveYMD(IsoChronology.java:550)\n\tat java.time.chrono.IsoChronology.resolveYMD(IsoChronology.java:123)\n\tat java.time.chrono.AbstractChronology.resolveDate(AbstractChronology.java:472)\n\tat java.time.chrono.IsoChronology.resolveDate(IsoChronology.java:492)\n\tat java.time.chrono.IsoChronology.resolveDate(IsoChronology.java:123)\n\tat java.time.format.Parsed.resolveDateFields(Parsed.java:352)\n\tat java.time.format.Parsed.resolveFields(Parsed.java:257)\n\tat java.time.format.Parsed.resolve(Parsed.java:244)\n\tat java.time.format.DateTimeParseContext.toResolved(DateTimeParseContext.java:331)\n\tat java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:1955)\n\tat java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1777)\n\t... 18 more\n"
     ]
    }
   ],
   "source": [
    "simple_dob_linker(df, dobs=['1994/14/15', '1994/12/03'], \n",
    "                    date_format_param='y/M/d', Linker=Linker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with pytest.raises(duckdb.InvalidInputException):\n",
    "#     simple_linker(df, settings, Linker)\n",
    "simple_dob_linker(df, dobs=['03/04/1994', '19/02/1993'], date_format_param='d/m/Y', Linker=Linker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "simple_dob_linker(df, dobs=['03/04/1994', '19/02/1993'], date_format_param='%d/%m/%Y', Linker=Linker)\n",
    "simple_dob_linker(df, dobs=['03-04-1994', '19-02-1993'], date_format_param='%d-%m-%Y', Linker=Linker)\n",
    "simple_dob_linker(df, dobs=['04/05/1994', '10/02/1993'], date_format_param='%m/%d/%Y', Linker=Linker)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_date_formats = ['%d/%m/%Y', '%d-%m-%Y', '%m/%d/%Y']\n",
    "#valid_date_formats = ['d/m/%Y', 'd-m-%Y', 'm/d/%Y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_dob_linker(df, dobs=['03/04/1994', '19/02/1993'], date_format_param=valid_date_formats[0], Linker=Linker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "simple_dob_linker(df, dobs=['03/04/1994', '19/02/1993'], date_format_param='d/m/Y', Linker=Linker)\n",
    "simple_dob_linker(df, dobs=['03-04-1994', '19-02-1993'], date_format_param='d-m-Y', Linker=Linker)\n",
    "simple_dob_linker(df, dobs=['04/05/1994', '10/02/1993'], date_format_param='m/d/Y', Linker=Linker)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_dob_linker(df, dobs=['04/05/1994', '10/02/1993'], date_format_param='m/d/Y', Linker=Linker)\n",
    "simple_dob_linker(df, dobs=['1994/05/04', '1993/14/02'], date_format_param='Y-m-d', Linker=Linker)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_dob_linker(df, dobs=['1994/55/54', '1993/14/02'], date_format_param='%Y-%m-%d', Linker=Linker)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# incompatible date formats\n",
    "with pytest.raises(duckdb.InvalidInputException):\n",
    "    simple_dob_linker(df, dobs=['03-04-1994', '19-02-1993'], date_format_param='%d/%m/%Y', Linker=Linker)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dobs = ['03-04-1994', '31-02-1993']\n",
    "df['dob'] = dobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings_cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_linker(df, settings_cl, Linker)\n",
    "df_e.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dict key: {size: gamma_level value}\n",
    "size_gamma_lookup = {1: 11, 2: 6, 3: 3, 4: 1}\n",
    "\n",
    "linker_outputs = {\n",
    "    \"cll\": df_e,\n",
    "    \"cl\": cl_df_e,\n",
    "}\n",
    "\n",
    "# Check gamma sizes are as expected\n",
    "for gamma, gamma_lookup in size_gamma_lookup.items():\n",
    "    for linker_pred in linker_outputs.values():\n",
    "        assert sum(linker_pred[\"gamma_dob\"] == gamma) == gamma_lookup\n",
    "\n",
    "# Check individual IDs are assigned to the correct gamma values\n",
    "# Dict key: {gamma_value: tuple of ID pairs}\n",
    "size_gamma_lookup = {\n",
    "    4: [(1, 2)],\n",
    "    3: [(3, 5), (1, 6), (2, 6)],\n",
    "    2: [(1, 3), (2, 3), (1, 5), (2, 5), (3, 6), (5, 6)],\n",
    "}\n",
    "\n",
    "for gamma, id_pairs in size_gamma_lookup.items():\n",
    "    for left, right in id_pairs:\n",
    "        for linker_name, linker_pred in linker_outputs.items():\n",
    "\n",
    "            print(f\"Checking IDs: {left}, {right} for {linker_name}\")\n",
    "\n",
    "            assert (\n",
    "                linker_pred.loc[\n",
    "                    (linker_pred.unique_id_l == left)\n",
    "                    & (linker_pred.unique_id_r == right)\n",
    "                ][\"gamma_dob\"].values[0]\n",
    "                == gamma\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_dob_linker(df, dobs=['1994/05/04', '1993/14/02'], date_format_param='%Y/%m/%d', Linker=Linker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pytest.raises(Exception) as e:\n",
    "    simple_dob_linker(df, dobs=['1994/05/04', '1993/14/02'], date_format_param='%Y/%m/%d', Linker=Linker)\n",
    "e.type   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pytest.raises(Exception) as e:\n",
    "    simple_dob_linker(df, dobs=['03-14-1994', '19-22-1993'], date_format_param='%d-%m-%Y', Linker=Linker)\n",
    "e.type   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pytest.raises(Exception) as e:\n",
    "    simple_dob_linker(df, dobs=['20-04-1993', '19-02-1993'], date_format_param='%d/%m/%Y', Linker=Linker)\n",
    "e.type   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from splink.duckdb.duckdb_linker import DuckDBLinker\n",
    "from splink.spark.spark_linker import SparkLinker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import py4j\n",
    "if Linker == SparkLinker:\n",
    "    expected_bad_dates_error = py4j.protocol.Py4JJavaError\n",
    "elif Linker == DuckDBLinker:\n",
    "    expected_bad_dates_error = duckdb.InvalidInputException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pytest.raises(expected_bad_dates_error):\n",
    "    simple_dob_linker(df, dobs=['1994/05/04', '1993/14/02'], date_format_param='%Y/%m/%d', Linker=Linker)\n",
    "with pytest.raises(expected_bad_dates_error):\n",
    "    simple_dob_linker(df, dobs=['03-14-1994', '19-22-1993'], date_format_param='%d-%m-%Y', Linker=Linker)\n",
    "with pytest.raises(expected_bad_dates_error):\n",
    "    simple_dob_linker(df, dobs=['20-04-1993', '19-02-1993'], date_format_param='%d/%m/%Y', Linker=Linker)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # # Differing lengths between thresholds and units\n",
    "\n",
    "cl.datediff_at_thresholds(\"dob\", [1], [\"day\", \"month\", \"year\", \"year\"])\n",
    "    # # Negative threshold\n",
    "    # with pytest.raises(ValueError):\n",
    "    #     cl.datediff_at_thresholds(\"dob\", [-1], [\"day\"])\n",
    "    # # Invalid metric\n",
    "    # with pytest.raises(ValueError):\n",
    "    #     cl.datediff_at_thresholds(\"dob\", [1], [\"dy\"])\n",
    "    # # Threshold len == 0\n",
    "    # with pytest.raises(ValueError):\n",
    "    #     cl.datediff_at_thresholds(\"dob\", [], [\"dy\"])\n",
    "    # # Metric len == 0\n",
    "    # with pytest.raises(ValueError):\n",
    "    #     cl.datediff_at_thresholds(\"dob\", [1], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl.datediff_at_thresholds(\"dob\", [1], [\"day\", \"month\", \"year\", \"year\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "splink_dev_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40ed3ce993a5a5d83f829fe220d0ce5dc391ba3c1504651e486245c4727b11f2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
