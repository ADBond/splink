import re


def get_queryplan_text(df, blocking_rule):
    spark = df.sql_ctx.sparkSession
    # set spark broadcast join limit to 1GB

    spark.conf.set("spark.sql.autoBroadcastJoinThreshold", "-1")
    sql = f"""
    select * from
    df as l
    left join df as r
    on {blocking_rule}
    """
    df = spark.sql(sql)
    qe = df._jdf.queryExecution()
    sp = qe.sparkPlan()
    return sp.treeString()


def get_sortmergejoin_query_plan_text(df, blocking_rule):
    sp_tree_string = get_queryplan_text(df, blocking_rule)
    lines = sp_tree_string.splitlines()
    lines = [l for l in lines if l.startswith("SortMergeJoin")]
    try:
        return lines[0]
    except IndexError:
        return None


def split_by_commas_ignoring_within_brackets(input_str):
    counter = 0
    captured_strings = []
    captured_string = ""
    for i in input_str:
        captured_string += i

        if i in ("(", "[", "{"):
            counter += 1

        if i in (")", "]", "}"):
            counter -= 1
        if counter == 0 and i == ",":
            captured_string = captured_string[:-1]
            captured_strings.append(captured_string)
            captured_string = ""
    captured_strings.append(captured_string)
    captured_strings = [s.strip() for s in captured_strings]
    return captured_strings


def extract_text_from_within_brackets_balanced(input_str, bracket_type=["[", "]"]):
    bracket_counter = 0

    start_bracket = bracket_type[0]
    end_bracket = bracket_type[1]

    captured_string = ""

    if start_bracket not in input_str:
        return None

    for i in input_str:

        if i == start_bracket:
            bracket_counter += 1

        if i == end_bracket:
            bracket_counter -= 1
            if bracket_counter == 0:
                break
        if bracket_counter > 0:
            captured_string += i

    return captured_string[1:]


def remove_col_ids(input_str):
    return re.sub(r"#\d{1,6}", "", input_str)


def _sorted_array(df: DataFrame, field_list: list):
    """Create a new field called _sorted_array
    containing a sorted array populated with the values from field_list
    Args:
        df (DataFrame): Input dataframe
        field_list (list): List of fields e.g. ["first_name", "surname"]
    Returns:
        df, with a new field called _sorted_array
    """

    df = df.withColumn("_sorted_array", F.array(*field_list))
    df = df.withColumn("_sorted_array", F.sort_array("_sorted_array"))
    return df


def _sort_fields(df: DataFrame, field_list_to_sort: list):
    """
    Take the fields in field_list and derive new fields
    with the same values but sorted alphabetically.
    The derieved fields are named __sorted_{field}
    Args:
        df (DataFrame): Input dataframe
        field_list_to_sort (list): list of fields e.g. ["first_name", "surname"]
    Returns:
        DataFrame: dataframe with new fields __sorted_{field}
    """

    df = _sorted_array(df, field_list_to_sort)

    for i, field in enumerate(field_list_to_sort):
        df = df.withColumn(f"__sorted_{field}", F.col("_sorted_array")[i])
    df = df.drop((f"_sorted_array"))
    return df


def get_total_comparisons(df, blocking_columns: list):
    """Compute the total number of records that will be generated
    by blocking all columns in the blocking columns list
    For instnce, if blocking_columns = ["first_name", "dmetaphone(surname)"]
    will compute the total number of comparisons generated by the blocking rule:
    ["l.first_name = r.first_name and dmetaphone(l.surname) = dmetaphone(r.surname)"]
    Args:
        df (DataFrame): Input dataframe
        blocking_columns (list): List of blocking columns e.g. ["first_name", "surname"]
    Returns:
        integer: Number of comparisons generated by the blocking rule
    """

    sel_expr = ", ".join(blocking_columns)
    concat_expr = f"concat({sel_expr})"
    spark = df.sql_ctx.sparkSession

    df.createOrReplaceTempView("df")
    sql = f"""
    with
    block_groups as (
        SELECT {concat_expr}, {sel_expr},
        count(*) * (count(*)-1)/2 as num_comparisons
    FROM df
    where {concat_expr} is not null
    GROUP BY {concat_expr}, {sel_expr}
    )
    select sum(num_comparisons) as total_comparisons
    from block_groups
    """

    return spark.sql(sql).collect()[0]["total_comparisons"]



def get_num_comparisons_from_blocking_rule(df, blocking_rule):
    # Use n strategy if the blocking rule is symmetric
    # Use n^2 strategy when inversions are present
    # Reults as dict with num comparisons, hash columns, filter columns, join strategy reported

