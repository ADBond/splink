{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from splink.spark.spark_linker import SparkLinker\n",
    "import splink.spark.spark_comparison_library as cl\n",
    "import splink.spark.spark_comparison_level_library as cll\n",
    "\n",
    "df = pd.read_csv(\"./tests/datasets/fake_1000_from_splink_demos.csv\")\n",
    "print(df.dtypes)\n",
    "print(f\"The number of rows is: {df.shape[0]:d}\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tricky_dates = ['2021-13-21', '2000-14-22', '1999-10-42', '2002-11-52', '2019-15-55']\n",
    "tricky_dates_df = tricky_dates * int(df.shape[0]/len(tricky_dates))\n",
    "df_test = pd.DataFrame(tricky_dates_df, columns=['dob'])\n",
    "print(df_test.dtypes)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a second dataframe and add the bad date strings\n",
    "df_2 = df.copy(deep=True)\n",
    "df_2['dob'] = df_test['dob'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession, types\n",
    "from pyspark.sql.functions import col, udf\n",
    "from splink.spark.jar_location import similarity_jar_location\n",
    "# sc = SparkContext.getOrCreate()\n",
    "\n",
    "# spark=(\n",
    "#     SparkSession.builder.master('local[*]')\n",
    "#     .appName('test')\n",
    "#     .config('spark.sql.ansi.enabled','true')\n",
    "#     .getOrCreate())\n",
    "\n",
    "# hadoop_conf = spark.sparkContext._jsc.hadoopConfiguration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf()\n",
    "path = similarity_jar_location()\n",
    "conf.set(\"spark.jars\", path)\n",
    "conf.set(\"spark.sql.ansi.enabled\",True)\n",
    "conf.set(\"spark.sql.legacy.timeParserPolicy\",\"LEGACY\")\n",
    "\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "spark = SparkSession(sc)\n",
    "spark.sparkContext.setCheckpointDir('/Users/alice.oleary/Documents/spark_checkpoint_dir')\n",
    "\n",
    "# Register the jaro winkler custom udf\n",
    "spark.udf.registerJavaFunction(\n",
    "    \"jaro_winkler\", \"uk.gov.moj.dash.linkage.JaroWinklerSimilarity\", types.DoubleType()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType\n",
    "\n",
    "my_schema = StructType.fromJson({'fields': [{'metadata': {},'name': 'unique_id','nullable': True,'type': 'integer'},\n",
    "  {'metadata': {}, 'name': 'first_name', 'nullable': True, 'type': 'string'},\n",
    "  {'metadata': {}, 'name': 'surname', 'nullable': True, 'type': 'string'},\n",
    "  {'metadata': {}, 'name': 'dob','nullable': True,'type': 'string'},\n",
    "  {'metadata': {}, 'name': 'city','nullable': True,'type': 'string'},\n",
    "  {'metadata': {}, 'name': 'email','nullable': True,'type': 'string'},\n",
    "  {'metadata': {}, 'name': 'group','nullable': True,'type': 'string'}],\n",
    " 'type': 'struct'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark.createDataFrame(df,schema=my_schema)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to register dataframe / convert to spark dataframe in order to run the linker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linker = SparkLinker(spark_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_name_comparison = cl.exact_match(\"first_name\")\n",
    "print(first_name_comparison.human_readable_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_name_comparison.as_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dob_comparison = cl.datediff_at_thresholds(\"dob\",date_thresholds=[1,2,3], date_metrics =[\"day\",\"year\",\"month\"],\\\n",
    "                                            cast_strings_to_date=True, date_format=\"yyyy-MM-dd\")\n",
    "dob_comparison.as_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocking_rules_predict = [\n",
    "   \n",
    "    # Tight(ish) blocking rule to start\n",
    "    \"l.first_name = r.first_name and l.surname = r.surname\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city = cl.exact_match(\"city\", term_frequency_adjustments=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = {\n",
    "    \"link_type\": \"dedupe_only\",\n",
    "    \"unique_id_column_name\": \"unique_id\",\n",
    "    \"retain_matching_columns\": True,\n",
    "    \"retain_intermediate_calculation_columns\": True,\n",
    "    \"max_iterations\": 10,\n",
    "    \"em_convergence\": 0.01,\n",
    "    \"comparisons\": [\n",
    "        city,\n",
    "        first_name_comparison,\n",
    "        dob_comparison,\n",
    "    ],\n",
    "    \"blocking_rules_to_generate_predictions\": blocking_rules_predict,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linker = SparkLinker(spark_df, settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deterministic_rules = [\n",
    "    \"l.first_name = r.first_name and levenshtein(r.dob, l.dob) <= 1\",\n",
    "    \"l.surname = r.surname and levenshtein(r.dob, l.dob) <= 1\",\n",
    "    \"l.first_name = r.first_name and levenshtein(r.surname, l.surname) <= 2\",\n",
    "    \"l.email = r.email\"\n",
    "]\n",
    "\n",
    "linker.estimate_probability_two_random_records_match(deterministic_rules, recall=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linker.estimate_u_using_random_sampling(target_rows=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_blocking_rule = \"l.first_name = r.first_name and l.surname = r.surname\"\n",
    "training_session_fname_sname = linker.estimate_parameters_using_expectation_maximisation(training_blocking_rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ansi_conf = spark.sparkContext.getConf().get(\"spark.sql.ansi.enabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ansi_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linker.spark.sparkContext.getConf().get(\"spark.sql.ansi.enabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = dir(linker._settings_obj)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linker._settings_obj._settings_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparisons_as_list = linker._settings_obj._settings_dict[\"comparisons\"]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Way to see if linker settings have used the datestr thingL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "any(['to_timestamp' in str(comparisons_as_list[x].values()) \n",
    "     for x in range(0, len(comparisons_as_list))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "splink_dev_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40ed3ce993a5a5d83f829fe220d0ce5dc391ba3c1504651e486245c4727b11f2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
