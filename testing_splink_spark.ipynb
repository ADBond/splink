{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique_id      int64\n",
      "first_name    object\n",
      "surname       object\n",
      "dob           object\n",
      "city          object\n",
      "email         object\n",
      "group          int64\n",
      "dtype: object\n",
      "The number of rows is: 1000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id</th>\n",
       "      <th>first_name</th>\n",
       "      <th>surname</th>\n",
       "      <th>dob</th>\n",
       "      <th>city</th>\n",
       "      <th>email</th>\n",
       "      <th>group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Julia</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-10-29</td>\n",
       "      <td>London</td>\n",
       "      <td>hannah88@powers.com</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Julia</td>\n",
       "      <td>Taylor</td>\n",
       "      <td>2015-07-31</td>\n",
       "      <td>London</td>\n",
       "      <td>hannah88@powers.com</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Julia</td>\n",
       "      <td>Taylor</td>\n",
       "      <td>2016-01-27</td>\n",
       "      <td>London</td>\n",
       "      <td>hannah88@powers.com</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Julia</td>\n",
       "      <td>Taylor</td>\n",
       "      <td>2015-10-29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hannah88opowersc@m</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>oNah</td>\n",
       "      <td>Watson</td>\n",
       "      <td>2008-03-23</td>\n",
       "      <td>Bolton</td>\n",
       "      <td>matthew78@ballard-mcdonald.net</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   unique_id first_name surname         dob    city  \\\n",
       "0          0     Julia      NaN  2015-10-29  London   \n",
       "1          1     Julia   Taylor  2015-07-31  London   \n",
       "2          2     Julia   Taylor  2016-01-27  London   \n",
       "3          3     Julia   Taylor  2015-10-29     NaN   \n",
       "4          4       oNah  Watson  2008-03-23  Bolton   \n",
       "\n",
       "                            email  group  \n",
       "0             hannah88@powers.com      0  \n",
       "1             hannah88@powers.com      0  \n",
       "2             hannah88@powers.com      0  \n",
       "3              hannah88opowersc@m      0  \n",
       "4  matthew78@ballard-mcdonald.net      1  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "from splink.spark.spark_linker import SparkLinker\n",
    "import splink.spark.spark_comparison_library as cl\n",
    "import splink.spark.spark_comparison_level_library as cll\n",
    "\n",
    "df = pd.read_csv(\"./tests/datasets/fake_1000_from_splink_demos.csv\")\n",
    "print(df.dtypes)\n",
    "print(f\"The number of rows is: {df.shape[0]:d}\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dob    object\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-13-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000-14-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1999-10-42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2002-11-52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-15-55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          dob\n",
       "0  2021-13-21\n",
       "1  2000-14-22\n",
       "2  1999-10-42\n",
       "3  2002-11-52\n",
       "4  2019-15-55"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tricky_dates = ['2021-13-21', '2000-14-22', '1999-10-42', '2002-11-52', '2019-15-55']\n",
    "tricky_dates_df = tricky_dates * int(df.shape[0]/len(tricky_dates))\n",
    "df_test = pd.DataFrame(tricky_dates_df, columns=['dob'])\n",
    "print(df_test.dtypes)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a second dataframe and add the bad date strings\n",
    "df_2 = df.copy(deep=True)\n",
    "df_2['dob'] = df_test['dob'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession, types\n",
    "from pyspark.sql.functions import col, udf\n",
    "from splink.spark.jar_location import similarity_jar_location\n",
    "# sc = SparkContext.getOrCreate()\n",
    "\n",
    "# spark=(\n",
    "#     SparkSession.builder.master('local[*]')\n",
    "#     .appName('test')\n",
    "#     .config('spark.sql.ansi.enabled','true')\n",
    "#     .getOrCreate())\n",
    "\n",
    "# hadoop_conf = spark.sparkContext._jsc.hadoopConfiguration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/03/14 11:13:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/03/14 11:13:22 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "conf = SparkConf()\n",
    "path = similarity_jar_location()\n",
    "conf.set(\"spark.jars\", path)\n",
    "conf.set(\"spark.sql.ansi.enabled\",False)\n",
    "conf.set(\"spark.sql.legacy.timeParserPolicy\",\"LEGACY\")\n",
    "\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "spark = SparkSession(sc)\n",
    "spark.sparkContext.setCheckpointDir('/Users/alice.oleary/Documents/spark_checkpoint_dir')\n",
    "\n",
    "# Register the jaro winkler custom udf\n",
    "spark.udf.registerJavaFunction(\n",
    "    \"jaro_winkler\", \"uk.gov.moj.dash.linkage.JaroWinklerSimilarity\", types.DoubleType()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType\n",
    "\n",
    "my_schema = StructType.fromJson({'fields': [{'metadata': {},'name': 'unique_id','nullable': True,'type': 'integer'},\n",
    "  {'metadata': {}, 'name': 'first_name', 'nullable': True, 'type': 'string'},\n",
    "  {'metadata': {}, 'name': 'surname', 'nullable': True, 'type': 'string'},\n",
    "  {'metadata': {}, 'name': 'dob','nullable': True,'type': 'string'},\n",
    "  {'metadata': {}, 'name': 'city','nullable': True,'type': 'string'},\n",
    "  {'metadata': {}, 'name': 'email','nullable': True,'type': 'string'},\n",
    "  {'metadata': {}, 'name': 'group','nullable': True,'type': 'string'}],\n",
    " 'type': 'struct'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alice.oleary/Documents/splink_dev_env/lib/python3.10/site-packages/pyspark/sql/pandas/utils.py:34: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n",
      "/Users/alice.oleary/Documents/splink_dev_env/lib/python3.10/site-packages/pyspark/sql/pandas/utils.py:34: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n"
     ]
    }
   ],
   "source": [
    "spark_df = spark.createDataFrame(df,schema=my_schema)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to register dataframe / convert to spark dataframe in order to run the linker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/03/14 11:13:26 WARN SimpleFunctionRegistry: The function jaro_winkler replaced a previously registered function.\n"
     ]
    }
   ],
   "source": [
    "linker = SparkLinker(spark_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison 'Exact match vs. anything else' of `first_name`.\n",
      "Similarity is assessed using the following ComparisonLevels:\n",
      "    - 'Null' with SQL rule: `first_name_l` IS NULL OR `first_name_r` IS NULL\n",
      "    - 'Exact match' with SQL rule: `first_name_l` = `first_name_r`\n",
      "    - 'All other comparisons' with SQL rule: ELSE\n",
      "\n"
     ]
    }
   ],
   "source": [
    "first_name_comparison = cl.exact_match(\"first_name\")\n",
    "print(first_name_comparison.human_readable_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_column_name': 'first_name',\n",
       " 'comparison_levels': [{'sql_condition': '`first_name_l` IS NULL OR `first_name_r` IS NULL',\n",
       "   'label_for_charts': 'Null',\n",
       "   'is_null_level': True},\n",
       "  {'sql_condition': '`first_name_l` = `first_name_r`',\n",
       "   'label_for_charts': 'Exact match'},\n",
       "  {'sql_condition': 'ELSE', 'label_for_charts': 'All other comparisons'}],\n",
       " 'comparison_description': 'Exact match vs. anything else'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_name_comparison.as_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_column_name': 'dob',\n",
       " 'comparison_levels': [{'sql_condition': '`dob_l` IS NULL OR `dob_r` IS NULL',\n",
       "   'label_for_charts': 'Null',\n",
       "   'is_null_level': True},\n",
       "  {'sql_condition': '`dob_l` = `dob_r`', 'label_for_charts': 'Exact match'},\n",
       "  {'sql_condition': \"\\n        abs(datediff(to_timestamp(`dob_l`,'yyyy-MM-dd'),to_timestamp(`dob_r`,'yyyy-MM-dd'))) <= 1\\n    \",\n",
       "   'label_for_charts': 'Within 1 day'},\n",
       "  {'sql_condition': \"\\n        floor(abs(months_between(to_timestamp(`dob_l`,'yyyy-MM-dd'),to_timestamp(`dob_r`, 'yyyy-MM-dd')) / 12)) <= 2\\n    \",\n",
       "   'label_for_charts': 'Within 2 years'},\n",
       "  {'sql_condition': \"\\n        floor(abs(months_between(to_timestamp(`dob_l`,'yyyy-MM-dd'),to_timestamp(`dob_r`, 'yyyy-MM-dd')))) <= 3\\n    \",\n",
       "   'label_for_charts': 'Within 3 months'},\n",
       "  {'sql_condition': 'ELSE', 'label_for_charts': 'All other comparisons'}],\n",
       " 'comparison_description': 'Exact match vs. Dates within the following thresholds Day(s): 1, Year(s): 2, Month(s): 3 vs. anything else'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dob_comparison = cl.datediff_at_thresholds(\"dob\",date_thresholds=[1,2,3], date_metrics =[\"day\",\"year\",\"month\"],\\\n",
    "                                            cast_strings_to_date=True, date_format=\"yyyy-MM-dd\")\n",
    "dob_comparison.as_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocking_rules_predict = [\n",
    "   \n",
    "    # Tight(ish) blocking rule to start\n",
    "    \"l.first_name = r.first_name and l.surname = r.surname\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "city = cl.exact_match(\"city\", term_frequency_adjustments=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = {\n",
    "    \"link_type\": \"dedupe_only\",\n",
    "    \"unique_id_column_name\": \"unique_id\",\n",
    "    \"retain_matching_columns\": True,\n",
    "    \"retain_intermediate_calculation_columns\": True,\n",
    "    \"max_iterations\": 10,\n",
    "    \"em_convergence\": 0.01,\n",
    "    \"comparisons\": [\n",
    "        city,\n",
    "        first_name_comparison,\n",
    "        dob_comparison,\n",
    "    ],\n",
    "    \"blocking_rules_to_generate_predictions\": blocking_rules_predict,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARN: You are using datediff comparison with str-casting and ANSI is not enabled. Bad dates e.g. 1999-13-54 will not trigger an exception and might mess up comparisons. Ensure date strings are cleaned to remove bad dates\n",
      "23/03/14 11:13:26 WARN SimpleFunctionRegistry: The function jaro_winkler replaced a previously registered function.\n",
      "23/03/14 11:13:26 WARN SimpleFunctionRegistry: The function jaccard replaced a previously registered function.\n",
      "23/03/14 11:13:26 WARN SimpleFunctionRegistry: The function cosine_distance replaced a previously registered function.\n",
      "23/03/14 11:13:26 WARN SimpleFunctionRegistry: The function dmetaphone replaced a previously registered function.\n",
      "23/03/14 11:13:26 WARN SimpleFunctionRegistry: The function dmetaphonealt replaced a previously registered function.\n",
      "23/03/14 11:13:26 WARN SimpleFunctionRegistry: The function qgramtokeniser replaced a previously registered function.\n"
     ]
    }
   ],
   "source": [
    "linker = SparkLinker(spark_df, settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alice.oleary/Documents/splink_dev_env/lib/python3.10/site-packages/pyspark/sql/pandas/utils.py:34: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n",
      "/Users/alice.oleary/Documents/splink_dev_env/lib/python3.10/site-packages/pyspark/sql/pandas/utils.py:34: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n",
      "/Users/alice.oleary/Documents/splink_dev_env/lib/python3.10/site-packages/pyspark/sql/pandas/utils.py:34: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n",
      "/Users/alice.oleary/Documents/splink_dev_env/lib/python3.10/site-packages/pyspark/sql/pandas/utils.py:34: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n",
      "Probability two random records match is estimated to be  0.0246.                \n",
      "This means that amongst all possible pairwise record comparisons, one in 40.72 are expected to match.  With 499,500 total possible comparisons, we expect a total of around 12,267.14 matching pairs\n"
     ]
    }
   ],
   "source": [
    "deterministic_rules = [\n",
    "    \"l.first_name = r.first_name and levenshtein(r.dob, l.dob) <= 1\",\n",
    "    \"l.surname = r.surname and levenshtein(r.dob, l.dob) <= 1\",\n",
    "    \"l.first_name = r.first_name and levenshtein(r.surname, l.surname) <= 2\",\n",
    "    \"l.email = r.email\"\n",
    "]\n",
    "\n",
    "linker.estimate_probability_two_random_records_match(deterministic_rules, recall=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----- Estimating u probabilities using random sampling -----\n",
      "23/03/14 11:13:37 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "23/03/14 11:13:37 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "23/03/14 11:13:37 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 75.08% for 9 writers\n",
      "23/03/14 11:13:37 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 67.58% for 10 writers\n",
      "23/03/14 11:13:37 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 61.43% for 11 writers\n",
      "23/03/14 11:13:37 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 56.31% for 12 writers\n",
      "23/03/14 11:13:37 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 51.98% for 13 writers\n",
      "23/03/14 11:13:37 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 48.27% for 14 writers\n",
      "23/03/14 11:13:37 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 45.05% for 15 writers\n",
      "23/03/14 11:13:37 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 42.24% for 16 writers\n",
      "23/03/14 11:13:37 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 45.05% for 15 writers\n",
      "23/03/14 11:13:37 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 48.27% for 14 writers\n",
      "23/03/14 11:13:37 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 51.98% for 13 writers\n",
      "23/03/14 11:13:37 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 56.31% for 12 writers\n",
      "23/03/14 11:13:37 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 61.43% for 11 writers\n",
      "23/03/14 11:13:37 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 67.58% for 10 writers\n",
      "23/03/14 11:13:37 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 75.08% for 9 writers\n",
      "23/03/14 11:13:37 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "23/03/14 11:13:37 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "23/03/14 11:13:37 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "23/03/14 11:13:37 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "23/03/14 11:13:37 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 75.08% for 9 writers\n",
      "23/03/14 11:13:37 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "23/03/14 11:13:37 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "23/03/14 11:13:37 WARN DataSource: All paths were ignored:                      \n",
      "  file:/Users/alice.oleary/Documents/spark_checkpoint_dir/8f25aab8-4f61-4e62-a7bc-1a569f933472/__splink__df_concat_with_tf_4e672d3ad\n",
      "/Users/alice.oleary/Documents/splink_dev_env/lib/python3.10/site-packages/pyspark/sql/pandas/utils.py:34: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n",
      "/Users/alice.oleary/Documents/splink_dev_env/lib/python3.10/site-packages/pyspark/sql/pandas/utils.py:34: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n",
      "/Users/alice.oleary/Documents/splink_dev_env/lib/python3.10/site-packages/pyspark/sql/pandas/utils.py:34: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n",
      "/Users/alice.oleary/Documents/splink_dev_env/lib/python3.10/site-packages/pyspark/sql/pandas/utils.py:34: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n",
      "u probability not trained for dob - Within 1 day (comparison vector value: 3). This usually means the comparison level was never observed in the training data.\n",
      "u probability not trained for dob - Within 3 months (comparison vector value: 1). This usually means the comparison level was never observed in the training data.\n",
      "\n",
      "Estimated u probabilities using random sampling\n",
      "\n",
      "Your model is not yet fully trained. Missing estimates for:\n",
      "    - city (no m values are trained).\n",
      "    - first_name (no m values are trained).\n",
      "    - dob (some u values are not trained, no m values are trained).\n"
     ]
    }
   ],
   "source": [
    "linker.estimate_u_using_random_sampling(target_rows=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "----- Starting EM training session -----\n",
      "\n",
      "Estimating the m probabilities of the model by blocking on:\n",
      "l.first_name = r.first_name and l.surname = r.surname\n",
      "\n",
      "Parameter estimates will be made for the following comparison(s):\n",
      "    - city\n",
      "    - dob\n",
      "\n",
      "Parameter estimates cannot be made for the following comparison(s) since they are used in the blocking rules: \n",
      "    - first_name\n",
      "23/03/14 11:13:41 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "23/03/14 11:13:41 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "23/03/14 11:13:41 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 75.08% for 9 writers\n",
      "23/03/14 11:13:41 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 67.58% for 10 writers\n",
      "23/03/14 11:13:41 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 61.43% for 11 writers\n",
      "23/03/14 11:13:41 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 56.31% for 12 writers\n",
      "23/03/14 11:13:41 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 61.43% for 11 writers\n",
      "23/03/14 11:13:41 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 67.58% for 10 writers\n",
      "23/03/14 11:13:41 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 75.08% for 9 writers\n",
      "23/03/14 11:13:41 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 67.58% for 10 writers\n",
      "23/03/14 11:13:41 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 61.43% for 11 writers\n",
      "23/03/14 11:13:41 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 67.58% for 10 writers\n",
      "23/03/14 11:13:41 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 75.08% for 9 writers\n",
      "23/03/14 11:13:41 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "23/03/14 11:13:41 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "23/03/14 11:13:41 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "23/03/14 11:13:41 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "23/03/14 11:13:41 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "23/03/14 11:13:41 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 75.08% for 9 writers\n",
      "23/03/14 11:13:41 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "23/03/14 11:13:41 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "23/03/14 11:13:41 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "23/03/14 11:13:41 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "23/03/14 11:13:41 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "23/03/14 11:13:41 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "23/03/14 11:13:41 WARN DataSource: All paths were ignored:                      \n",
      "  file:/Users/alice.oleary/Documents/spark_checkpoint_dir/8f25aab8-4f61-4e62-a7bc-1a569f933472/__splink__df_comparison_vectors_ac1d4e5ca\n",
      "/Users/alice.oleary/Documents/splink_dev_env/lib/python3.10/site-packages/pyspark/sql/pandas/utils.py:34: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n",
      "/Users/alice.oleary/Documents/splink_dev_env/lib/python3.10/site-packages/pyspark/sql/pandas/utils.py:34: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n",
      "\n",
      "/Users/alice.oleary/Documents/splink_dev_env/lib/python3.10/site-packages/pyspark/sql/pandas/utils.py:34: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n",
      "/Users/alice.oleary/Documents/splink_dev_env/lib/python3.10/site-packages/pyspark/sql/pandas/utils.py:34: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n",
      "\n",
      "WARNING:\n",
      "Level Within 1 day on comparison dob not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Within 3 months on comparison dob not observed in dataset, unable to train m value\n",
      "Iteration 1: Largest change in params was 0.353 in the m_probability of city, level `All other comparisons`\n",
      "/Users/alice.oleary/Documents/splink_dev_env/lib/python3.10/site-packages/pyspark/sql/pandas/utils.py:34: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n",
      "/Users/alice.oleary/Documents/splink_dev_env/lib/python3.10/site-packages/pyspark/sql/pandas/utils.py:34: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n",
      "\n",
      "WARNING:\n",
      "Level Within 1 day on comparison dob not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Within 3 months on comparison dob not observed in dataset, unable to train m value\n",
      "Iteration 2: Largest change in params was 0.14 in the m_probability of dob, level `Within 2 years`\n",
      "/Users/alice.oleary/Documents/splink_dev_env/lib/python3.10/site-packages/pyspark/sql/pandas/utils.py:34: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n",
      "/Users/alice.oleary/Documents/splink_dev_env/lib/python3.10/site-packages/pyspark/sql/pandas/utils.py:34: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n",
      "\n",
      "WARNING:\n",
      "Level Within 1 day on comparison dob not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Within 3 months on comparison dob not observed in dataset, unable to train m value\n",
      "Iteration 3: Largest change in params was 0.0548 in probability_two_random_records_match\n",
      "/Users/alice.oleary/Documents/splink_dev_env/lib/python3.10/site-packages/pyspark/sql/pandas/utils.py:34: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n",
      "/Users/alice.oleary/Documents/splink_dev_env/lib/python3.10/site-packages/pyspark/sql/pandas/utils.py:34: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n",
      "\n",
      "WARNING:\n",
      "Level Within 1 day on comparison dob not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Within 3 months on comparison dob not observed in dataset, unable to train m value\n",
      "Iteration 4: Largest change in params was 0.0123 in probability_two_random_records_match\n",
      "/Users/alice.oleary/Documents/splink_dev_env/lib/python3.10/site-packages/pyspark/sql/pandas/utils.py:34: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n",
      "/Users/alice.oleary/Documents/splink_dev_env/lib/python3.10/site-packages/pyspark/sql/pandas/utils.py:34: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n",
      "\n",
      "WARNING:\n",
      "Level Within 1 day on comparison dob not observed in dataset, unable to train m value\n",
      "\n",
      "WARNING:\n",
      "Level Within 3 months on comparison dob not observed in dataset, unable to train m value\n",
      "Iteration 5: Largest change in params was 0.00224 in probability_two_random_records_match\n",
      "\n",
      "EM converged after 5 iterations\n",
      "m probability not trained for dob - Within 1 day (comparison vector value: 3). This usually means the comparison level was never observed in the training data.\n",
      "m probability not trained for dob - Within 3 months (comparison vector value: 1). This usually means the comparison level was never observed in the training data.\n",
      "\n",
      "Your model is not yet fully trained. Missing estimates for:\n",
      "    - first_name (no m values are trained).\n",
      "    - dob (some u values are not trained, some m values are not trained).\n"
     ]
    }
   ],
   "source": [
    "training_blocking_rule = \"l.first_name = r.first_name and l.surname = r.surname\"\n",
    "training_session_fname_sname = linker.estimate_parameters_using_expectation_maximisation(training_blocking_rule)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Way to see if linker settings have used the datestr thingL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not isinstance(settings, (dict, type(None)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict, NoneType)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(dict, type(None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Want to test a bunch of dates/ formats which should throw errors. Catch exceptions that are specific for each backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "splink_dev_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40ed3ce993a5a5d83f829fe220d0ce5dc391ba3c1504651e486245c4727b11f2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
