{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this notebook is to implement the basic EM approach used by the R fastLink package in Apache Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging \n",
    "\n",
    "logging.basicConfig()\n",
    "\n",
    "log = logging.getLogger(\"sql_logs\").setLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.types import StructType\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "\n",
    "conf=SparkConf()\n",
    "conf.set('spark.driver.memory', '8g')\n",
    "conf.set(\"spark.sql.shuffle.partitions\", \"4\") \n",
    "\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"parquet/fake_1000.parquet\")\n",
    "df.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append(\"gluejoblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utility_functions import *\n",
    "from sql_steps import *\n",
    "from pipelines import get_features_df\n",
    "from accuracy import *\n",
    "from rules import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sql_add_unique_row_id_to_original_table(df, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = get_test_data_rules()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_select_expr = sql_gen_col_selection_compare_cols(df)\n",
    "df.registerTempTable(\"df\")\n",
    "\n",
    "sqls = []\n",
    "for rule in rules:\n",
    "    sql = f\"\"\"\n",
    "    select {sql_select_expr}  \n",
    "     from df as l\n",
    "        left join df as r\n",
    "        on\n",
    "        {rule}\n",
    "        where l.row_id < r.row_id\n",
    "\n",
    "    \"\"\"\n",
    "    sqls.append(sql)\n",
    "    \n",
    "df_comparison = spark.sql(\" union \".join(sqls))\n",
    "\n",
    "df_comparison = df_comparison.dropDuplicates([\"row_id_l\", \"row_id_r\"])\n",
    "\n",
    "df_comparison.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate gammas dataset \n",
    "def gammas_case_statement(col_name, i):\n",
    "    return f\"\"\"case \n",
    "    when {col_name}_l = {col_name}_r then 1\n",
    "    else 0 end as gamma_{i}\"\"\"\n",
    "    \n",
    "    \n",
    "\n",
    "def add_gammas(df, spark, binary_comparison_cols=None, approximate_string_cols=None):\n",
    "    gamma_select_expressions = []\n",
    "    for i, col_name in enumerate(binary_comparison_cols):\n",
    "        gamma_select_expressions.append(gammas_case_statement(col_name, i))\n",
    "    \n",
    "    gammas_select_expr = \",\\n\".join(gamma_select_expressions)\n",
    "    \n",
    "    df.registerTempTable(\"df\")\n",
    "    sql = f\"\"\"\n",
    "    select *, {gammas_select_expr}\n",
    "    from df\n",
    "    \"\"\"\n",
    "\n",
    "    df = spark.sql(sql)\n",
    "    return df\n",
    "\n",
    "    \n",
    "cols = [\"first_name\", \"surname\", \"dob\", \"city\", \"email\", \"group\"]\n",
    "df_with_gamma = add_gammas(df_comparison, spark, cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_gamma.persist()\n",
    "df_with_gamma.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_gamma.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_gamma.registerTempTable(\"df_with_gamma\")\n",
    "sql = \"\"\"\n",
    "select avg(gamma_5)\n",
    "from df_with_gamma\n",
    "\"\"\"\n",
    "spark.sql(sql).collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    col = cols[i]\n",
    "    print(col)\n",
    "    field = f\"gamma_{i}\"\n",
    "    sql = f\"\"\"\n",
    "    select gamma_5 as m, avg({field}) as prop_match_{field}, count(*) as num_records\n",
    "    from df_with_gamma\n",
    "    group by gamma_5\n",
    "    \"\"\"\n",
    "    spark.sql(sql).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_gamma.sample(0.001).show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_gamma.filter(df_with_gamma[\"city_l\"]== df_with_gamma[\"city_r\"]).filter(df_with_gamma[\"gamma_5\"] == 0).sample(0.001).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"first_name\", \"surname\", \"dob\", \"city\", \"email\"]\n",
    "def generate_params(binary_comparison_cols=cols):\n",
    "    params = {}\n",
    "    params[\"λ\"] = 0.8\n",
    "    params[\"π\"] = {}\n",
    "    \n",
    "    \n",
    "    for i, col in enumerate(binary_comparison_cols):\n",
    "        params[\"π\"][f\"gamma_{i}\"] = {}\n",
    "        this_gamma = params[\"π\"][f\"gamma_{i}\"]\n",
    "        this_gamma[\"desc\"] = f\"Exact match on {col}\"\n",
    "        this_gamma[\"type\"] = \"exact_match_only\"\n",
    "        \n",
    "        pdm = {\n",
    "                \"level_0\": {\n",
    "                    \"value\": 0,\n",
    "                    \"probability\": 0.1\n",
    "                },\n",
    "                \"level_1\": {\n",
    "                    \"value\": 1,\n",
    "                    \"probability\": 0.9\n",
    "                }\n",
    "            }\n",
    "              \n",
    "               \n",
    "        pdnm =  {\n",
    "                \"level_0\": {\n",
    "                    \"value\": 0,\n",
    "                    \"probability\": 0.8\n",
    "                },\n",
    "                \"level_1\": {\n",
    "                    \"value\": 1,\n",
    "                    \"probability\": 0.2\n",
    "                }\n",
    "        }\n",
    "        this_gamma[\"prob_dist_match\"] = pdm\n",
    "        this_gamma[\"prob_dist_non_match\"] = pdnm\n",
    "                 \n",
    "    return params \n",
    "        \n",
    "\n",
    "params = generate_params(cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expectation step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from em_in_spark.fns import *\n",
    "\n",
    "\n",
    "print(params[\"λ\"])\n",
    "print(params[\"π\"][\"gamma_0\"])\n",
    "\n",
    "df_e = run_expectation_step(df_with_gamma, spark, params)\n",
    "\n",
    "new_params = update_params(df_e, spark, params)\n",
    "print(new_params[\"λ\"])\n",
    "print(new_params[\"π\"][\"gamma_0\"])\n",
    "\n",
    "df_e = run_expectation_step(df_with_gamma, spark, new_params)\n",
    "new_params = update_params(df_e, spark, new_params)\n",
    "print(new_params[\"λ\"])\n",
    "print(new_params[\"π\"][\"gamma_0\"])\n",
    "\n",
    "df_e = run_expectation_step(df_with_gamma, spark, new_params)\n",
    "new_params = update_params(df_e, spark, new_params)\n",
    "print(new_params[\"λ\"])\n",
    "print(new_params[\"π\"][\"gamma_0\"])\n",
    "\n",
    "df_e = run_expectation_step(df_with_gamma, spark, new_params)\n",
    "new_params = update_params(df_e, spark, new_params)\n",
    "print(new_params[\"λ\"])\n",
    "print(new_params[\"π\"][\"gamma_0\"])\n",
    "\n",
    "df_e = run_expectation_step(df_with_gamma, spark, new_params)\n",
    "new_params = update_params(df_e, spark, new_params)\n",
    "print(new_params[\"λ\"])\n",
    "print(new_params[\"π\"][\"gamma_0\"])\n",
    "\n",
    "df_e = run_expectation_step(df_with_gamma, spark, new_params)\n",
    "new_params = update_params(df_e, spark, new_params)\n",
    "print(new_params[\"λ\"])\n",
    "print(new_params[\"π\"][\"gamma_0\"])\n",
    "\n",
    "df_e = run_expectation_step(df_with_gamma, spark, new_params)\n",
    "new_params = update_params(df_e, spark, new_params)\n",
    "print(new_params[\"λ\"])\n",
    "print(new_params[\"π\"][\"gamma_0\"])\n",
    "\n",
    "df_e = run_expectation_step(df_with_gamma, spark, new_params)\n",
    "new_params = update_params(df_e, spark, new_params)\n",
    "print(new_params[\"λ\"])\n",
    "print(new_params[\"π\"][\"gamma_0\"])\n",
    "\n",
    "df_e = run_expectation_step(df_with_gamma, spark, new_params)\n",
    "new_params = update_params(df_e, spark, new_params)\n",
    "print(new_params[\"λ\"])\n",
    "print(new_params[\"π\"][\"gamma_0\"])\n",
    "\n",
    "\n",
    "df_e = run_expectation_step(df_with_gamma, spark, new_params)\n",
    "new_params = update_params(df_e, spark, new_params)\n",
    "print(new_params[\"λ\"])\n",
    "print(new_params[\"π\"][\"gamma_0\"])\n",
    "\n",
    "df_e.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
