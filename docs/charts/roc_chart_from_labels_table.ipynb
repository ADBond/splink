{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `roc_chart_from_labels_table_chart`\n",
    "\n",
    "!!! info \"At a glance\"\n",
    "    **Useful for:** Assessing the relationship between True and False Positive Rates.\n",
    "\n",
    "    **API Documentation:** [roc_chart_from_labels_table_chart()](../linker.md#splink.linker.Linker.roc_chart_from_labels_table_chart)\n",
    "\n",
    "    **What is needed to generate the chart?** A trained `linker` and a corresponding labelled dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Worked Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "from splink.duckdb.linker import DuckDBLinker\n",
    "import splink.duckdb.comparison_library as cl\n",
    "import splink.duckdb.comparison_template_library as ctl\n",
    "from splink.duckdb.blocking_rule_library import block_on\n",
    "from splink.datasets import splink_datasets, splink_dataset_labels\n",
    "import logging, sys\n",
    "logging.disable(sys.maxsize)\n",
    "\n",
    "df = splink_datasets.fake_1000\n",
    "\n",
    "settings = {\n",
    "    \"link_type\": \"dedupe_only\",\n",
    "    \"blocking_rules_to_generate_predictions\": [\n",
    "        block_on(\"first_name\"),\n",
    "        block_on(\"surname\"),\n",
    "    ],\n",
    "    \"comparisons\": [\n",
    "        ctl.name_comparison(\"first_name\"),\n",
    "        ctl.name_comparison(\"surname\"),\n",
    "        ctl.date_comparison(\"dob\", cast_strings_to_date=True),\n",
    "        cl.exact_match(\"city\", term_frequency_adjustments=True),\n",
    "        ctl.email_comparison(\"email\", include_username_fuzzy_level=False),\n",
    "    ],\n",
    "}\n",
    "\n",
    "linker = DuckDBLinker(df, settings)\n",
    "linker.estimate_u_using_random_sampling(max_pairs=1e6)\n",
    "\n",
    "blocking_rule_for_training = block_on([\"first_name\", \"surname\"])\n",
    "\n",
    "linker.estimate_parameters_using_expectation_maximisation(blocking_rule_for_training)\n",
    "\n",
    "blocking_rule_for_training = block_on(\"dob\")\n",
    "linker.estimate_parameters_using_expectation_maximisation(blocking_rule_for_training)\n",
    "\n",
    "\n",
    "df_labels = splink_dataset_labels.fake_1000_labels\n",
    "labels_table = linker.register_labels_table(df_labels)\n",
    "\n",
    "linker.roc_chart_from_labels_table(labels_table)"
   ],
   "outputs": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What the chart shows\n",
    "\n",
    "The chart plots the True Positive Rate against False Positive Rate for clerically reviewed records. Each point on the curve reflects the choice of a match weight threshold for a match and the subsequent True/False Positive Rates.\n",
    "\n",
    "??? note \"What the chart tooltip shows\"\n",
    "    ![](./img/roc_chart_from_labels_table_tooltip.png)\n",
    "\n",
    "    The tooltip shows information based on the point on the curve that the user is hoverng over, including:\n",
    "\n",
    "    - The match weight and match probability threshold\n",
    "    - The False and True Positive Rate\n",
    "    - The count of True Positives, True Negatives, False Positives and False Negatives\n",
    "    - Precision, Recall and F1 score\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to interpret the chart\n",
    "\n",
    "A ROC chart shows how the number of False Positives and False Negatives varies depending on the match threshold chosen. The match threshold is the match weight chosen as a cutoff for which pairwise comparisons to accept as matches.\n",
    "\n",
    "\n",
    "For a perfect classifier, we should be able to get 100% of True Positives without gaining any False Positives (see \"ideal class descriminator\" in the chart below).\n",
    "\n",
    "On the other hand, for a random classifier we would expect False Positives and False Negatives to be roughly equal (see \"no predictive value\" in the chart below).\n",
    "\n",
    "In reality, most models sit somethere between these two extremes.\n",
    "\n",
    "![](./img/roc_curve_explainer.png)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions to take as a result of the chart\n",
    "\n",
    "If the ROC curve resembles the \"No predictive value\" example above, your model is not performing very well. In this case, it is worth reassessing your modesl (comparisons, comparison levels, blocking rules etc.) to see if there is a better solution.\n",
    "\n",
    "It is also worth considering the impact of your labelled data on this chart. For labels, it is important to consider a variety of pairwise comparisons (which includes True/False Positives and True/False Negatives). For example, it you only label pairwise comparisons that are true matches, this chart will not give any insights (as there will be no False Positives). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
