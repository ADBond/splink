---
date: 2024-07-22
authors:
  - erica-k
categories:
  - Bias
---

# Bias in Data Linking:<br>_Progress and next steps_

In March 2024, the Splink team launched a six-month _'Bias in Data Linking'_ internship with the [Alan Turing Institute](https://www.turing.ac.uk). This installment of the Splink Blog is going to introduce the internship, its goals, and provide an update on what's happened so far.

<!-- more -->

The internship is being undertaken by myself, Erica Kane. I am a PhD student based at the University of Leeds. My doctoral research is in Data Analytics, conducted in partnership with the Parole Board. I have a background in quantitative research within Criminal Justice.

## üìù Background

The internship stemmed from the team's [previous engagement with ethics](https://moj-analytical-services.github.io/splink/blog/2024/01/23/ethics-in-data-linking.html), understanding that this complex yet inevitable aspect of data science has implications for data linking. 

Data science pipelines are intricate processes with lots of decision points. At each of these decision points bias can creep in. If it does, its impact on results can vary as it interacts with different parts of the pipeline. For example, two datasets might react differently to the same bias introduced by a model. Additionally, multiple biases can interact with each other, making it difficult to see their individual effects. Therefore, detecting, monitoring, quantifying, and mitigating bias in data science pipelines is extremely challenging.

## üéØ Goals

To set project goals, experts were consulted and relevant materials were reviewed. The overarching aim of the internship was to create something practical for users and developers to assess bias.

While ideas on how exactly to achieve this varied, the general consensus was for a standardised framework to facilitate this assessment. This framework should help users with two processes:

1. Identify potential **sources** of bias in a linkage pipeline.
2. Evaluate the **impact** of bias on the linked data. 

Before developing the framework, it was crucial to think about the best approach for these processes. 

## üîç Identifying the source

Identifying bias sources in data linking varies by use case, making standardisation difficult. However, flexible guidelines can aid this process. To explore this, it was useful to identify general ways bias can enter a data linking pipeline:

![Image 1](./img/bias_chart.png)

The **input data** can contain mistakes (misspellings of names), or legitimate qualities (changing names) which make some records harder to link than others. If these mistakes or qualities are not random, this means the input data will introduce bias. Addressing these mistakes or qualities through data preparation (character removal) can also introduce bias.

**Model design** involves specifying settings (which records to compare/how to compare them). If these design choices result in a better/worse performance for certain record types, bias will be introduced.

This process can support a bias assessment framework by guiding users to explore their input data and model design choices. It helps lay the groundwork for evaluation by identifying interest areas to develop hypotheses about where bias occurs.

## üìä Evaluating the impact 

Understanding the impact of bias requires a performance evaluation process for which there are several well-established strategies. For example using [charts](https://moj-analytical-services.github.io/splink/topic_guides/evaluation/model.html) to explore models, [metrics](https://moj-analytical-services.github.io/splink/topic_guides/evaluation/edge_metrics.html) to analyse links, and [clusters](https://moj-analytical-services.github.io/splink/topic_guides/evaluation/clusters/overview.html) to review the output data. 

To streamline the project, link-based evaluation was chosen. This allows for a standard approach of classifying record comparisons and analysing outcomes. Analysing overall performance is less valuable here. Instead, the hypothesis from the [identification](#üîç-identifying-the-source) process can be used pinpoint records of interest.

The process would follow three steps:

![Image 2](./img/process_flow.png)

_Each step was investigated to uncover issues which could arise when attempting to evaluate bias..._

### **Step 1: Hand label record comparisons**

Firstly, records are labelled by human experts to decide if they relate to the same person, creating a 'true' sample of comparisons. This sample provides the base from which performance is analysed.

When working with real data, it's not always clear whether records relate to the same individual... 

![Image 3](./img/record_eg.png)

It can be difficult for even human evaluators to make these decisions, resulting in a lack of ground truth in the labels. In making this decision, further bias can be introduced. 

### **Step 2: Locate records of interest**

The second step is crucial for using performance evaluation to address bias. To find records of interest there are two main options, which will be demonstrated with the example of locating records with non-anglicised names:

 <u>Using variables as proxies</u>  
Using ethnicity/nationality as a proxy for name origin relies on both accurate data recording and a presumed link between the variable and name origin.

<u>Directly identifying features</u>  
Creating a name origin detector. This depends on a robust theoretical and technical solution, which would be complex to develop and verify.

Both options are likely to introduce bias.

### **Step 3: Assess the performance outcomes**

The final step is to analyse performance outcome by comparing the labelled data with the pipeline‚Äôs predictions to spot errors (false positives and false negatives). 

A high-level typical data linking performance may look like this:

![Image 4](./img/confusion_matrix_1.png)

This represents the reality of the dominant class (non-links), leaving very few errors to evaluate. When analysing for bias, the focus would be on an even smaller subset of records of interest. 

A bias specific data linking performance may look like this:

![Image 5](./img/confusion_matrix_2.png)

This reduces the number of examples further, making it hard to spot patterns or draw solid conclusions on where things go wrong.

Getting enough useful examples is possible, but it‚Äôs impractical. It would require either dedicating a lot more resources to labelling or using a sampling method that could introduce bias.

## üí° Conclusions 

The initial investigation into creating a bias assessment framework looked at how to identify and evaluate bias. Bias identification can be standardised by highlighting pipeline areas and giving examples to guide users in developing hypotheses. However, finding a solution for the evaluation process is trickier. A common evaluation method showed three main issues:

1. There is no 'ground truth' in labelling.
2. Records of interest are difficult to identify.
3. Gathering large samples of FPs and FNs is impractical.

These challenges stem from working with real data and make this method unsuitable for the framework. We‚Äôre now looking into alternative evaluation approaches‚Äî**stay tuned for updates!**




