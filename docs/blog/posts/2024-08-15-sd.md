---
date: 2024-08-27
authors:
  - erica-k
categories:
  - Bias
---

# Bias in Data Linking #2

This Splink Blog is the second installment dedicated to the topic of bias. It wraps up the work completed during the the six-month [Alan Turing Institute](https://www.turing.ac.uk) internship on '_Bias in Data Linking_', summarising the final thoughts.

<!-- more -->

## ‚è™ Recap

This blog builds upon the [previous post in this series](https://moj-analytical-services.github.io/splink/blog/2024/08/19/bias-in-data-linking.html)...

In the first few months of the internship, the focus was on exploring potential sources of bias in data linking pipelines. This highlighted the complexity of these biases and the need for a standardised approach to evaluate them. As a result, a goal was set to develop such an approach. 

To achieve this, a common performance evaluation method using clerical review was examined to see how it could be adapted for bias assessment. However, challenges arose with this method, primarily due to difficulties in using real data to detect bias in the messy linkage output.

## ‚è© Recent developments 

Building upon these challenges, it was determined that the output of a linkage pipeline (linked data) shouldn‚Äôt be used to _detect_ bias within a pipeline. Instead, it can be used to understand the _impact_ of any detected bias. 

Detecting bias should be guided by a hypothesis to form a key part of understanding any data linkage pipeline. This blog proposes an approach to bias detection, broken down into 5 key steps:

![Image 1](./img/bias_investigation_steps.png)

These steps will be explained to show how you can apply this bias detection process to any data linkage pipeline. There's also a [supplementary notebook]() that demonstrates how to do this in practice using Splink.

## <u>1. Generate synthetic records</u>

This process builds a neutral foundation for bias detection by creating specific records to evaluate a pre-trained model's results. While the process of generating these records can vary, it's crucial to keep it controlled. Automatically generating large amounts of data without understanding the content can lead to issues similar to using real production data.

Instead of focussing on the number of synthetic records, the key is ensuring each has a clear purpose that is relevant to the hypothesis. Additionally, including records that capture potential downstream effects is useful, as they provide important context for the next steps. 

For example, if the hypothesis is that misspellings of names lead to worse linkage, and you suspect that misspellings are more common for names of certain origins, your synthetic records might look like this:

![Image 2](./img/sp_mistake_data.png)

This is a simplification, and a real generation would likely include more rows with various spelling error iterations. The last record also accounts for potential downstream effects by including similarly spelt names that relate to different individuals.

## <u>2. Investigate model parameters</u>

The pre-trained model can be analysed specifically in relation to the hypothesis, rather than just generally. In Splink, one way to do this is by using a [match weights chart](https://moj-analytical-services.github.io/splink/charts/match_weights_chart.html):

![Image 3](./img/match_weights_chart.png)

These model parameters will be relevant to the hypothesis, and it's useful to start thinking about how they relate to the scenario represented by the synthetic records. However, because linkage relies on the accumulation of these weights, it‚Äôs difficult to fully understand their impact without generating comparisons. 

## <u>3. Perform and evaluate linkage</u>

Instead of dumping the every record into the model, it‚Äôs better to manually decide which to compare. This lets you focus on the scenarios you care about, rather than sifting through every possible combination. You'll generate match probabilities for the synthetic record comparisons using the pre-trained model, comparing results to the pipeline's threshold to see which records link.

It‚Äôs also useful to examine each comparison to see which features impact the match probability the most. In Splink, you can use a [waterfall chart](https://moj-analytical-services.github.io/splink/charts/waterfall_chart.html) for this. This will help you identify if any weights are too predictive or not predictive enough based on your hypothesis. 

Some factor weightings might seem off for your hypothesis but be reasonable for the [overall model](#2-train-and-investigate-model). They might not be _wrong_ per se, but if they create issues in specific scenarios, they will **introduce bias into the pipeline**.

## <u>4. Identify bias mitigations</u>

If you detect bias in the pipeline, you'll need to decide whether or not to address it. Assessing the mitigation strategy in this controlled manner ensures it directly addresses the bias. Focus on aligning the mitigation with the hypothesis, rather finding a solution than randomly pushes comparisons over a threshold.

Here's some considerations which should help you make that decision:

![Image 4](./img/bias_mitigation_flowchart.png)

If you choose to attempt a bias mitigation, **repeat steps 1-3**, incorporating the technical solution before reassessing the results to determine if it was successful. Keep in mind, this attempt may only work partially - or not at all. 

This outcome will guide your next steps...

## <u>5. Make a statement about bias</u>

The mitigation stage could lead to three potential outcomes, each shaping a different statement about the bias in the pipeline:

1. **Bias was not detected**  
_You might disprove the hypothesis, though it's rare that there's no bias at all. Before concluding there's no bias in the pipeline, consider exploring other hypotheses._

2. **Bias was detected - it has been fully mitigated**  
_It's unlikely, but possible. If this happens, ensure the impact on overall performance and any further bias is thoroughly investigated and clearly explained._

3. **Bias was detected - it has been partially/could not be mitigated**  
_This is the most likely scenario. If mitigation is partial, clearly explain where it falls short. If there's no mitigation, work to gain a more detailed understanding of the bias._

If you conclude that bias has been detected and it can only be partially mitigated or not mitigated, this is not a failure. It's a likely outcome when you're trying to standardise a pipeline that handles thousands of different scenarios with varying impacts on results.

## üí≠ Final thoughts

This approach doesn‚Äôt aim to make statements about the _impact_ of the detected bias. In other words, detecting bias alone doesn‚Äôt allow you to draw conclusions on how the resulting linked data will look. This is because many other unrelated factors in the pipeline will interact with the scenarios and affect whether records are linked or not.

Therefore, the goal here is to better understand bias. This is done by highlighting specific scenarios, examining how the model handles them, and introducing mitigations directly. This approach allows for more transparent statements about bias during the development of a linkage pipeline.

If you have any ideas or feedback on this process or how it's working in the notebook - **please let us know!**