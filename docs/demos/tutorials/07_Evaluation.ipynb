{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57b58c35",
   "metadata": {},
   "source": [
    "## Evaluation of prediction results\n",
    "\n",
    " <a target=\"_blank\" href=\"https://colab.research.google.com/github/moj-analytical-services/splink/blob/splink4_dev/docs/demos/tutorials/07_Quality_assurance.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "In the previous tutorial, we looked at various ways to visualise the results of our model.\n",
    "These are useful for evaluating a linkage pipeline because they allow us to understand how our model works and verify that it is doing something sensible. They can also be useful to identify examples where the model is not performing as expected.\n",
    "\n",
    "In addition to these spot checks, Splink also has functions to perform more formal accuracy analysis. These functions allow you to understand the likely prevalence of false positives and false negatives in your linkage models.\n",
    "\n",
    "They rely on the existence of a sample of labelled (ground truth) matches, which may have been produced (for example) by human beings. For the accuracy analysis to be unbiased, the sample should be representative of the overall dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e08e61e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-20T20:18:19.257678Z",
     "iopub.status.busy": "2024-05-20T20:18:19.257377Z",
     "iopub.status.idle": "2024-05-20T20:18:19.262293Z",
     "shell.execute_reply": "2024-05-20T20:18:19.261603Z"
    }
   },
   "outputs": [],
   "source": [
    "# Uncomment and run this cell if you're running in Google Colab.\n",
    "# !pip install git+https://github.com/moj-analytical-services/splink.git@splink4_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb29d421",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-20T20:18:19.266250Z",
     "iopub.status.busy": "2024-05-20T20:18:19.265912Z",
     "iopub.status.idle": "2024-05-20T20:18:21.078187Z",
     "shell.execute_reply": "2024-05-20T20:18:21.077366Z"
    }
   },
   "outputs": [],
   "source": [
    "# Rerun our predictions to we're ready to view the charts\n",
    "import pandas as pd\n",
    "\n",
    "from splink import DuckDBAPI, Linker, splink_datasets\n",
    "\n",
    "pd.options.display.max_columns = 1000\n",
    "\n",
    "db_api = DuckDBAPI()\n",
    "df = splink_datasets.fake_1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f88cc1c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-20T20:18:21.082452Z",
     "iopub.status.busy": "2024-05-20T20:18:21.082130Z",
     "iopub.status.idle": "2024-05-20T20:18:21.943997Z",
     "shell.execute_reply": "2024-05-20T20:18:21.943335Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " -- WARNING --\n",
      "You have called predict(), but there are some parameter estimates which have neither been estimated or specified in your settings dictionary.  To produce predictions the following untrained trained parameters will use default values.\n",
      "Comparison: 'email':\n",
      "    m values not fully trained\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import urllib\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/moj-analytical-services/splink_demos/master/demo_settings/saved_model_from_demo.json\"\n",
    "\n",
    "with urllib.request.urlopen(url) as u:\n",
    "    settings = json.loads(u.read().decode())\n",
    "\n",
    "\n",
    "linker = Linker(df, settings, database_api=DuckDBAPI())\n",
    "df_predictions = linker.predict(threshold_match_probability=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0dedd9",
   "metadata": {},
   "source": [
    "## Load in labels\n",
    "\n",
    "The labels file contains a list of pairwise comparisons which represent matches and non-matches.\n",
    "\n",
    "The required format of the labels file is described [here](https://moj-analytical-services.github.io/splink/linkerqa.html#splink.linker.Linker.roc_chart_from_labels).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbfdc70c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-20T20:18:21.948155Z",
     "iopub.status.busy": "2024-05-20T20:18:21.947806Z",
     "iopub.status.idle": "2024-05-20T20:18:21.972544Z",
     "shell.execute_reply": "2024-05-20T20:18:21.971787Z"
    }
   },
   "outputs": [],
   "source": [
    "from splink.datasets import splink_dataset_labels\n",
    "\n",
    "df_labels = splink_dataset_labels.fake_1000_labels\n",
    "df_labels.head(5)\n",
    "labels_table = linker.register_labels_table(df_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030c4cde",
   "metadata": {},
   "source": [
    "### Threshold Selection chart\n",
    "\n",
    "Splink includes an interactive dashboard that shows key accuracy statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e83d9645",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-20T20:18:21.976526Z",
     "iopub.status.busy": "2024-05-20T20:18:21.976206Z",
     "iopub.status.idle": "2024-05-20T20:18:24.042552Z",
     "shell.execute_reply": "2024-05-20T20:18:24.041258Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-bbc5aab35c3e413db42ce95398a250dd.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-bbc5aab35c3e413db42ce95398a250dd.vega-embed details,\n",
       "  #altair-viz-bbc5aab35c3e413db42ce95398a250dd.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-bbc5aab35c3e413db42ce95398a250dd\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-bbc5aab35c3e413db42ce95398a250dd\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-bbc5aab35c3e413db42ce95398a250dd\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300, \"discreteHeight\": {\"step\": 150}, \"discreteWidth\": {\"step\": 150}}, \"axis\": {\"gridWidth\": 0.5, \"labelFontSize\": 12, \"titleFontSize\": 16}, \"axisX\": {\"format\": \"+.0f\", \"grid\": false, \"offset\": 20, \"values\": {\"expr\": \"[-25,-20,-15,-10,-5,0,5,10,15,20,25]\"}}, \"axisY\": {\"title\": \"Match probability threshold\", \"titleFontSize\": 16}, \"concat\": {\"spacing\": 40}}, \"hconcat\": [{\"vconcat\": [{\"layer\": [{\"layer\": [{\"mark\": {\"type\": \"rule\"}, \"encoding\": {\"opacity\": {\"condition\": {\"param\": \"threshold\", \"value\": 0.3, \"empty\": false}, \"value\": 0}, \"x\": {\"axis\": {\"orient\": \"bottom\"}, \"field\": \"truth_threshold\", \"scale\": {\"nice\": false}, \"title\": null, \"type\": \"quantitative\"}}, \"params\": [{\"name\": \"threshold\", \"select\": {\"type\": \"point\", \"encodings\": [\"x\"], \"fields\": [\"truth_threshold\"], \"nearest\": true, \"on\": \"mouseover\", \"toggle\": false}, \"value\": null}]}, {\"mark\": {\"type\": \"rule\"}, \"encoding\": {\"opacity\": {\"condition\": {\"param\": \"threshold\", \"value\": 0.3, \"empty\": false}, \"value\": 0}, \"y\": {\"axis\": {\"orient\": \"right\"}, \"field\": \"match_probability\", \"title\": \" \", \"type\": \"quantitative\"}}, \"params\": [{\"name\": \"prob\", \"select\": {\"type\": \"point\", \"encodings\": [\"y\"], \"fields\": [\"match_probability\"], \"nearest\": true, \"on\": \"mouseover\", \"toggle\": false}}]}]}, {\"layer\": [{\"mark\": {\"type\": \"text\", \"fontSize\": 14, \"fontWeight\": \"bold\", \"xOffset\": 25, \"yOffset\": 10}, \"encoding\": {\"text\": {\"aggregate\": \"min\", \"field\": \"truth_threshold\", \"format\": \"+.2f\"}, \"y\": {\"axis\": {\"orient\": \"left\"}, \"field\": \"match_probability\", \"title\": \"Match probability threshold\", \"type\": \"quantitative\"}}, \"transform\": [{\"filter\": {\"param\": \"threshold\", \"empty\": false}}]}, {\"mark\": {\"type\": \"text\", \"fontSize\": 14, \"xOffset\": -25, \"yOffset\": -10}, \"encoding\": {\"text\": {\"aggregate\": \"min\", \"field\": \"match_probability\", \"format\": \".3f\"}}, \"transform\": [{\"filter\": {\"param\": \"threshold\", \"empty\": false}}]}, {\"mark\": {\"type\": \"line\", \"color\": \"red\", \"opacity\": 0.5}}, {\"mark\": {\"type\": \"line\", \"color\": \"green\", \"opacity\": 0.5, \"strokeWidth\": 3}, \"transform\": [{\"filter\": \"datum.truth_threshold >= threshold.truth_threshold\"}]}, {\"mark\": {\"type\": \"point\", \"color\": \"green\", \"size\": 100}, \"encoding\": {\"opacity\": {\"condition\": {\"param\": \"threshold\", \"value\": 1, \"empty\": false}, \"value\": 0}}}], \"encoding\": {\"x\": {\"field\": \"truth_threshold\", \"type\": \"quantitative\", \"title\": \"Match weight threshold\", \"axis\": {\"orient\": \"top\"}}, \"y\": {\"field\": \"match_probability\", \"type\": \"quantitative\", \"title\": \"Match probability threshold\", \"axis\": {\"orient\": \"left\", \"titlePadding\": 10}}}}, {\"mark\": {\"type\": \"text\", \"align\": \"left\", \"color\": \"red\", \"fontSize\": 12, \"text\": \"Non-match\", \"x\": 0, \"y\": \"height\", \"yOffset\": 10}, \"data\": {\"values\": [{}]}}, {\"mark\": {\"type\": \"text\", \"align\": \"right\", \"color\": \"green\", \"fontSize\": 12, \"fontWeight\": \"bold\", \"text\": \"Match\", \"x\": \"width\", \"y\": 0, \"yOffset\": -10}, \"data\": {\"values\": [{}]}}], \"description\": \"Match weight vs probability\"}, {\"hconcat\": [{\"layer\": [{\"mark\": {\"type\": \"rect\", \"opacity\": 0.5}, \"encoding\": {\"color\": {\"field\": \"count\", \"legend\": null, \"scale\": {\"scheme\": \"reds\", \"zero\": true}, \"type\": \"quantitative\"}}, \"transform\": [{\"filter\": \"datum.predicted == 0\"}]}, {\"mark\": {\"type\": \"rect\", \"opacity\": 0.5}, \"encoding\": {\"color\": {\"field\": \"count\", \"legend\": null, \"scale\": {\"scheme\": \"greens\", \"zero\": true}, \"type\": \"quantitative\"}}, \"transform\": [{\"filter\": \"datum.predicted == 1\"}]}, {\"mark\": {\"type\": \"text\", \"fontSize\": 14, \"yOffset\": -40}, \"encoding\": {\"color\": {\"condition\": [{\"test\": \"datum.predicted==1 && datum.actual==1\", \"value\": \"darkgreen\"}, {\"test\": \"datum.predicted==0 && datum.actual==0\", \"value\": \"darkred\"}], \"value\": \"black\"}, \"opacity\": {\"condition\": {\"test\": \"datum.predicted != datum.actual\", \"value\": 1}, \"value\": 0.5}, \"text\": {\"field\": \"confusion_label\", \"type\": \"nominal\"}}}, {\"mark\": {\"type\": \"text\", \"fontSize\": 28, \"fontWeight\": \"bold\", \"yOffset\": 10}, \"encoding\": {\"color\": {\"condition\": [{\"test\": \"datum.predicted==1 && datum.actual==1\", \"value\": \"darkgreen\"}, {\"test\": \"datum.predicted==0 && datum.actual==0\", \"value\": \"darkred\"}], \"value\": \"black\"}, \"text\": {\"field\": \"count\", \"format\": \",\", \"type\": \"nominal\"}}}], \"description\": \"Confusion matrix\", \"encoding\": {\"x\": {\"field\": \"actual\", \"type\": \"nominal\", \"title\": \"Actual\", \"axis\": {\"domain\": false, \"labelAngle\": 0, \"labelExpr\": \"datum.label == 1 ? 'Match' : 'Non-match'\", \"labelFontSize\": 18, \"labelPadding\": 10, \"orient\": \"top\", \"ticks\": false, \"titleAngle\": 0, \"titleFontSize\": 20}, \"sort\": \"-x\"}, \"y\": {\"field\": \"predicted\", \"type\": \"nominal\", \"title\": \"Predicted\", \"axis\": {\"domain\": false, \"labelExpr\": \"datum.label == 1 ? 'Match' : 'Non-match'\", \"labelFontSize\": 18, \"labelPadding\": 10, \"ticks\": false, \"titleAngle\": 0, \"titleFontSize\": 20, \"titlePadding\": -30}, \"sort\": \"-y\"}}, \"resolve\": {\"scale\": {\"color\": \"independent\"}}, \"transform\": [{\"filter\": {\"or\": [{\"param\": \"threshold\", \"empty\": false}, {\"and\": [{\"param\": \"threshold\", \"empty\": true}, \"datum.truth_threshold == datum.median_threshold\"]}]}}]}], \"transform\": [{\"fold\": [\"tp\", \"tn\", \"fp\", \"fn\"], \"as\": [\"label\", \"count\"]}, {\"calculate\": \"datum.label === 'tp' ? 'True Positive (TP)' : datum.label === 'tn' ? 'True Negative (TN)' : datum.label === 'fp' ? 'False Positive (FP)' : 'False Negative (FN)'\", \"as\": \"confusion_label\"}, {\"calculate\": \"datum.label === 'tp' || datum.label === 'fp' ? 1 : 0\", \"as\": \"predicted\"}, {\"calculate\": \"datum.label === 'tp' || datum.label === 'fn' ? 1 : 0\", \"as\": \"actual\"}, {\"joinaggregate\": [{\"op\": \"median\", \"field\": \"truth_threshold\", \"as\": \"median_threshold\"}]}]}]}, {\"layer\": [{\"layer\": [{\"mark\": {\"type\": \"point\", \"size\": 100}, \"encoding\": {\"opacity\": {\"condition\": {\"param\": \"threshold\", \"value\": 1, \"empty\": false}, \"value\": 0}, \"tooltip\": [{\"field\": \"truth_threshold\", \"format\": \".3f\", \"title\": \"Match weight threshold\", \"type\": \"quantitative\"}, {\"field\": \"match_probability\", \"format\": \".3%\", \"title\": \"Match probability threshold\", \"type\": \"quantitative\"}, {\"field\": \"precision\", \"format\": \".4f\", \"title\": \"Precision\", \"type\": \"quantitative\"}, {\"field\": \"recall\", \"format\": \".4f\", \"title\": \"Recall (TPR)\", \"type\": \"quantitative\"}, {\"field\": \"fp_rate\", \"format\": \".4f\", \"title\": \"FPR\", \"type\": \"quantitative\"}], \"x\": {\"axis\": {\"orient\": \"top\"}, \"field\": \"truth_threshold\", \"title\": \"Match weight threshold\"}}, \"params\": [{\"name\": \"metric\", \"select\": {\"type\": \"point\", \"fields\": [\"metric\"]}, \"bind\": \"legend\", \"value\": [{\"metric\": \"precision\"}, {\"metric\": \"recall\"}]}, {\"name\": \"threshold\", \"select\": {\"type\": \"point\", \"encodings\": [\"x\"], \"fields\": [\"truth_threshold\"], \"nearest\": true, \"on\": \"mouseover\", \"toggle\": false}, \"value\": null}], \"transform\": [{\"filter\": {\"param\": \"metric\", \"empty\": true}}]}, {\"mark\": {\"type\": \"line\"}, \"encoding\": {\"opacity\": {\"condition\": {\"param\": \"metric\", \"value\": 1}, \"value\": 0.1}, \"x\": {\"axis\": {\"orient\": \"bottom\"}, \"field\": \"truth_threshold\", \"title\": null}}}], \"encoding\": {\"color\": {\"field\": \"metric\", \"type\": \"nominal\", \"sort\": [\"precision\", \"recall\", \"f1\"], \"title\": [\"Performance\", \"Metric\"], \"legend\": {\"fillColor\": \"whitesmoke\", \"labelExpr\": \"{'precision': 'Precision (PPV)', 'recall': 'Recall (TPR)', 'specificity': 'Specificity (TNR)', 'accuracy': 'Accuracy', 'npv': 'NPV', 'f1': 'F1', 'f2': 'F2', 'f0_5': 'F0.5', 'p4': 'P4', 'phi': '\\u03c6 (MCC)'}[datum.value]\", \"labelFontSize\": 14, \"legendX\": 800, \"legendY\": 160, \"orient\": \"none\", \"padding\": 10, \"titleFontSize\": 16, \"titlePadding\": 15}}, \"x\": {\"type\": \"quantitative\", \"field\": \"truth_threshold\"}, \"y\": {\"field\": \"value\", \"type\": \"quantitative\", \"axis\": {\"labelFontSize\": 12, \"title\": \"Performance metric score\", \"titleFontSize\": 18, \"titlePadding\": 10, \"values\": {\"expr\": \"[0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95,1.0]\"}}, \"scale\": {\"domain\": [0.5, 1]}}}}, {\"layer\": [{\"mark\": {\"type\": \"rule\", \"color\": \"gray\"}, \"encoding\": {\"x\": {\"field\": \"truth_threshold\", \"title\": null, \"type\": \"quantitative\"}}}, {\"layer\": [{\"mark\": {\"type\": \"rect\", \"fill\": \"whitesmoke\", \"x\": 200, \"x2\": 10, \"y2Offset\": 20, \"yOffset\": -20}, \"encoding\": {\"y2\": {\"field\": \"score_index\"}}}, {\"mark\": {\"type\": \"text\", \"align\": \"right\", \"baseline\": \"middle\", \"fontSize\": 16, \"x\": 200, \"xOffset\": -10}}], \"encoding\": {\"color\": {\"field\": \"metric\", \"sort\": [\"precision\", \"recall\", \"f1\"]}, \"text\": {\"field\": \"y_text\"}, \"y\": {\"field\": \"score_index\", \"type\": \"quantitative\"}}, \"transform\": [{\"filter\": {\"param\": \"metric\", \"empty\": true}}]}, {\"mark\": {\"type\": \"text\", \"fontSize\": 14, \"fontWeight\": \"bold\", \"xOffset\": 20, \"y\": 0, \"yOffset\": -10}, \"encoding\": {\"text\": {\"condition\": {\"param\": \"threshold\", \"aggregate\": \"min\", \"empty\": false, \"field\": \"truth_threshold\", \"format\": \"+.2f\", \"type\": \"nominal\"}, \"value\": \"\"}, \"x\": {\"field\": \"truth_threshold\", \"type\": \"quantitative\"}}}], \"transform\": [{\"filter\": {\"param\": \"threshold\", \"empty\": false}}]}], \"description\": \"Accuracy chart\", \"height\": 700, \"transform\": [{\"fold\": [\"precision\", \"recall\", \"f1\"], \"as\": [\"metric\", \"value\"]}, {\"calculate\": \"0.6375 - 0.025*indexof(['precision', 'recall', 'f1'], datum.metric)\", \"as\": \"score_index\"}, {\"calculate\": \"{'precision': 'Precision (PPV)', 'recall': 'Recall (TPR)', 'specificity': 'Specificity (TNR)', 'accuracy': 'Accuracy', 'npv': 'NPV', 'f1': 'F1', 'f2': 'F2', 'f0_5': 'F0.5', 'p4': 'P4', 'phi': '\\u03c6 (MCC)'}[datum.metric]\", \"as\": \"metric_text\"}, {\"calculate\": \"datum.metric_text + ' = ' + format(datum.value, ',.3g')\", \"as\": \"y_text\"}], \"width\": 500}], \"data\": {\"name\": \"data-104b66b3aabcce487b2509dece9e3881\"}, \"title\": {\"text\": \"Match Threshold Selection Tool\", \"anchor\": \"middle\", \"baseline\": \"line-bottom\", \"fontSize\": 28, \"subtitle\": [\"Hover over either line graph to show Confusion Matrix (bottom left) and selected performance metrics (right).\", \"\", \"Click a legend value to show a specific evaluation metric. Shift + Click to show multiple metrics\"], \"subtitleFontSize\": 14, \"subtitleFontStyle\": \"italic\"}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.14.1.json\", \"datasets\": {\"data-104b66b3aabcce487b2509dece9e3881\": [{\"truth_threshold\": -12.300000183284283, \"match_probability\": 0.00019826446591752426, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 1027.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1004.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.5056622353520434, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.49433776464795665, \"precision\": 1.0, \"recall\": 0.5056622353520434, \"specificity\": 1.0, \"npv\": 0.532805956258725, \"accuracy\": 0.6838790931989924, \"f1\": 0.671680837148463, \"f2\": 0.5611408589225221, \"f0_5\": 0.836455448770158, \"p4\": 0.6832397200141539, \"phi\": 0.5190566932914649}, {\"truth_threshold\": -5.800000086426735, \"match_probability\": 0.017631945325087592, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 1026.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1005.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.5051698670605613, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.4948301329394387, \"precision\": 1.0, \"recall\": 0.5051698670605613, \"specificity\": 1.0, \"npv\": 0.5325581395348837, \"accuracy\": 0.6835642317380353, \"f1\": 0.6712463199214916, \"f2\": 0.5606557377049181, \"f0_5\": 0.8361858190709046, \"p4\": 0.6829130052819856, \"phi\": 0.5186832603341437}, {\"truth_threshold\": -4.700000070035458, \"match_probability\": 0.037047907242669466, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 1025.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1006.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.5046774987690793, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.49532250123092075, \"precision\": 1.0, \"recall\": 0.5046774987690793, \"specificity\": 1.0, \"npv\": 0.5323105532310554, \"accuracy\": 0.6832493702770781, \"f1\": 0.6708115183246073, \"f2\": 0.5601705104382992, \"f0_5\": 0.8359158375468928, \"p4\": 0.6825861647803277, \"phi\": 0.518309905918297}, {\"truth_threshold\": -3.7000000551342964, \"match_probability\": 0.07144878715678568, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 1023.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1008.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.5036927621861153, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.4963072378138848, \"precision\": 1.0, \"recall\": 0.5036927621861153, \"specificity\": 1.0, \"npv\": 0.5318160705991639, \"accuracy\": 0.6826196473551638, \"f1\": 0.6699410609037328, \"f2\": 0.5591997376188914, \"f0_5\": 0.835374816266536, \"p4\": 0.6819321045764876, \"phi\": 0.5175634314507344}, {\"truth_threshold\": -3.200000047683716, \"match_probability\": 0.09813940308831819, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 1017.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1014.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.5007385524372231, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.49926144756277696, \"precision\": 1.0, \"recall\": 0.5007385524372231, \"specificity\": 1.0, \"npv\": 0.5303381194997684, \"accuracy\": 0.6807304785894207, \"f1\": 0.6673228346456693, \"f2\": 0.5562848703642927, \"f0_5\": 0.8337432365961633, \"p4\": 0.6799668560937839, \"phi\": 0.5153258602676495}, {\"truth_threshold\": -2.600000038743019, \"match_probability\": 0.1415855743659812, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 1016.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1015.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.5002461841457411, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.499753815854259, \"precision\": 1.0, \"recall\": 0.5002461841457411, \"specificity\": 1.0, \"npv\": 0.5300925925925926, \"accuracy\": 0.6804156171284634, \"f1\": 0.6668854611092878, \"f2\": 0.5557986870897156, \"f0_5\": 0.8334700574241182, \"p4\": 0.679638862253978, \"phi\": 0.5149531985417386}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.HConcatChart(...)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linker.accuracy_analysis_from_labels_table(\n",
    "    labels_table, output_type=\"threshold_selection\", add_metrics=[\"f1\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e4396d",
   "metadata": {},
   "source": [
    "## Receiver operating characteristic curve\n",
    "\n",
    "A [ROC chart](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) shows how the number of false positives and false negatives varies depending on the match threshold chosen. The match threshold is the match weight chosen as a cutoff for which pairwise comparisons to accept as matches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01dd7eec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-20T20:18:24.047880Z",
     "iopub.status.busy": "2024-05-20T20:18:24.047173Z",
     "iopub.status.idle": "2024-05-20T20:18:24.928457Z",
     "shell.execute_reply": "2024-05-20T20:18:24.927369Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-447e064aa33d48159edf370e27a8aade.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-447e064aa33d48159edf370e27a8aade.vega-embed details,\n",
       "  #altair-viz-447e064aa33d48159edf370e27a8aade.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-447e064aa33d48159edf370e27a8aade\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-447e064aa33d48159edf370e27a8aade\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-447e064aa33d48159edf370e27a8aade\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-679a32ee851a5db5902bffa3c670a370\"}, \"mark\": {\"type\": \"line\", \"clip\": true, \"point\": true}, \"encoding\": {\"tooltip\": [{\"field\": \"truth_threshold\", \"format\": \".4f\", \"type\": \"quantitative\"}, {\"field\": \"match_probability\", \"format\": \".4%\", \"type\": \"quantitative\"}, {\"field\": \"fp_rate\", \"format\": \".4f\", \"title\": \"FP_rate\", \"type\": \"quantitative\"}, {\"field\": \"tp_rate\", \"format\": \".4f\", \"title\": \"TP_rate\", \"type\": \"quantitative\"}, {\"field\": \"tp\", \"format\": \",.0f\", \"title\": \"TP\", \"type\": \"quantitative\"}, {\"field\": \"tn\", \"format\": \",.0f\", \"title\": \"TN\", \"type\": \"quantitative\"}, {\"field\": \"fp\", \"format\": \",.0f\", \"title\": \"FP\", \"type\": \"quantitative\"}, {\"field\": \"fn\", \"format\": \",.0f\", \"title\": \"FN\", \"type\": \"quantitative\"}, {\"field\": \"precision\", \"format\": \".4f\", \"type\": \"quantitative\"}, {\"field\": \"recall\", \"format\": \".4f\", \"type\": \"quantitative\"}, {\"field\": \"f1\", \"format\": \".4f\", \"title\": \"F1\", \"type\": \"quantitative\"}], \"x\": {\"field\": \"fp_rate\", \"sort\": [\"truth_threshold\"], \"title\": \"False Positive Rate amongst clerically reviewed records\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"tp_rate\", \"sort\": [\"truth_threshold\"], \"title\": \"True Positive Rate amongst clerically reviewed records\", \"type\": \"quantitative\"}}, \"height\": 400, \"params\": [{\"name\": \"mouse_zoom\", \"select\": {\"type\": \"interval\", \"encodings\": [\"x\"]}, \"bind\": \"scales\"}], \"title\": \"Receiver operating characteristic curve\", \"width\": 400, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.9.3.json\", \"datasets\": {\"data-679a32ee851a5db5902bffa3c670a370\": [{\"truth_threshold\": -12.300000183284283, \"match_probability\": 0.00019826446591752426, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 1027.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1004.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.5056622353520434, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.49433776464795665, \"precision\": 1.0, \"recall\": 0.5056622353520434, \"specificity\": 1.0, \"npv\": 0.532805956258725, \"accuracy\": 0.6838790931989924, \"f1\": 0.671680837148463, \"f2\": 0.5611408589225221, \"f0_5\": 0.836455448770158, \"p4\": 0.6832397200141539, \"phi\": 0.5190566932914649}, {\"truth_threshold\": -5.800000086426735, \"match_probability\": 0.017631945325087592, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 1026.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1005.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.5051698670605613, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.4948301329394387, \"precision\": 1.0, \"recall\": 0.5051698670605613, \"specificity\": 1.0, \"npv\": 0.5325581395348837, \"accuracy\": 0.6835642317380353, \"f1\": 0.6712463199214916, \"f2\": 0.5606557377049181, \"f0_5\": 0.8361858190709046, \"p4\": 0.6829130052819856, \"phi\": 0.5186832603341437}, {\"truth_threshold\": -4.700000070035458, \"match_probability\": 0.037047907242669466, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 1025.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1006.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.5046774987690793, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.49532250123092075, \"precision\": 1.0, \"recall\": 0.5046774987690793, \"specificity\": 1.0, \"npv\": 0.5323105532310554, \"accuracy\": 0.6832493702770781, \"f1\": 0.6708115183246073, \"f2\": 0.5601705104382992, \"f0_5\": 0.8359158375468928, \"p4\": 0.6825861647803277, \"phi\": 0.518309905918297}, {\"truth_threshold\": -3.7000000551342964, \"match_probability\": 0.07144878715678568, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 1023.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1008.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.5036927621861153, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.4963072378138848, \"precision\": 1.0, \"recall\": 0.5036927621861153, \"specificity\": 1.0, \"npv\": 0.5318160705991639, \"accuracy\": 0.6826196473551638, \"f1\": 0.6699410609037328, \"f2\": 0.5591997376188914, \"f0_5\": 0.835374816266536, \"p4\": 0.6819321045764876, \"phi\": 0.5175634314507344}, {\"truth_threshold\": -3.200000047683716, \"match_probability\": 0.09813940308831819, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 1017.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1014.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.5007385524372231, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.49926144756277696, \"precision\": 1.0, \"recall\": 0.5007385524372231, \"specificity\": 1.0, \"npv\": 0.5303381194997684, \"accuracy\": 0.6807304785894207, \"f1\": 0.6673228346456693, \"f2\": 0.5562848703642927, \"f0_5\": 0.8337432365961633, \"p4\": 0.6799668560937839, \"phi\": 0.5153258602676495}, {\"truth_threshold\": -2.600000038743019, \"match_probability\": 0.1415855743659812, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 1016.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1015.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.5002461841457411, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.499753815854259, \"precision\": 1.0, \"recall\": 0.5002461841457411, \"specificity\": 1.0, \"npv\": 0.5300925925925926, \"accuracy\": 0.6804156171284634, \"f1\": 0.6668854611092878, \"f2\": 0.5557986870897156, \"f0_5\": 0.8334700574241182, \"p4\": 0.679638862253978, \"phi\": 0.5149531985417386}, {\"truth_threshold\": -2.500000037252903, \"match_probability\": 0.15022110152606716, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 1007.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1024.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.4958148695224028, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5041851304775973, \"precision\": 1.0, \"recall\": 0.4958148695224028, \"specificity\": 1.0, \"npv\": 0.5278930382664823, \"accuracy\": 0.6775818639798489, \"f1\": 0.662936142198815, \"f2\": 0.551418245537181, \"f0_5\": 0.8309952137316389, \"p4\": 0.6766809845726959, \"phi\": 0.5116025976183864}, {\"truth_threshold\": -2.2000000327825546, \"match_probability\": 0.1787376058900962, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 999.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1032.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.4918759231905465, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5081240768094535, \"precision\": 1.0, \"recall\": 0.4918759231905465, \"specificity\": 1.0, \"npv\": 0.5259531465319247, \"accuracy\": 0.6750629722921915, \"f1\": 0.6594059405940594, \"f2\": 0.5475172640578757, \"f0_5\": 0.8287705326032853, \"p4\": 0.6740425938136967, \"phi\": 0.5086292259646149}, {\"truth_threshold\": -2.1000000312924385, \"match_probability\": 0.18913982061899084, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 997.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1034.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.49089118660758246, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5091088133924175, \"precision\": 1.0, \"recall\": 0.49089118660758246, \"specificity\": 1.0, \"npv\": 0.5254703992657183, \"accuracy\": 0.674433249370277, \"f1\": 0.6585204755614267, \"f2\": 0.5465409494572964, \"f0_5\": 0.8282106662236252, \"p4\": 0.6733816166373302, \"phi\": 0.5078865895283203}, {\"truth_threshold\": -2.0000000298023224, \"match_probability\": 0.19999999669481672, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 996.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1035.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.49039881831610044, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5096011816838996, \"precision\": 1.0, \"recall\": 0.49039881831610044, \"specificity\": 1.0, \"npv\": 0.5252293577981652, \"accuracy\": 0.6741183879093199, \"f1\": 0.6580773042616452, \"f2\": 0.5460526315789473, \"f0_5\": 0.827930174563591, \"p4\": 0.6730509183540228, \"phi\": 0.5075153755396427}, {\"truth_threshold\": -1.2000000178813934, \"match_probability\": 0.3032695424040186, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 995.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1036.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.4899064500246184, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5100935499753816, \"precision\": 1.0, \"recall\": 0.4899064500246184, \"specificity\": 1.0, \"npv\": 0.5249885373681797, \"accuracy\": 0.6738035264483627, \"f1\": 0.6576338400528751, \"f2\": 0.5455642066016011, \"f0_5\": 0.8276493095990684, \"p4\": 0.6727200795968197, \"phi\": 0.5071442306145872}, {\"truth_threshold\": -1.1000000163912773, \"match_probability\": 0.318111997717226, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 987.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1044.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.4859675036927622, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5140324963072378, \"precision\": 1.0, \"recall\": 0.4859675036927622, \"specificity\": 1.0, \"npv\": 0.5230698949291914, \"accuracy\": 0.6712846347607053, \"f1\": 0.6540755467196819, \"f2\": 0.5416529469871584, \"f0_5\": 0.8253888610135475, \"p4\": 0.6700682510685908, \"phi\": 0.5041775194270115}, {\"truth_threshold\": -0.7000000104308128, \"match_probability\": 0.38102425962470177, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 980.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1051.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.48252092565238797, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.517479074347612, \"precision\": 1.0, \"recall\": 0.48252092565238797, \"specificity\": 1.0, \"npv\": 0.5214025500910747, \"accuracy\": 0.6690806045340051, \"f1\": 0.6509465293922285, \"f2\": 0.5382249560632689, \"f0_5\": 0.8233910267181986, \"p4\": 0.6677402918128024, \"phi\": 0.5015851284751782}, {\"truth_threshold\": -0.10000000149011612, \"match_probability\": 0.48267825490990723, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 979.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1052.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.48202855736090594, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.517971442639094, \"precision\": 1.0, \"recall\": 0.48202855736090594, \"specificity\": 1.0, \"npv\": 0.5211652253072372, \"accuracy\": 0.6687657430730478, \"f1\": 0.6504983388704318, \"f2\": 0.5377348126991102, \"f0_5\": 0.8231040860938288, \"p4\": 0.6674071352914174, \"phi\": 0.5012150453662769}, {\"truth_threshold\": -0.0, \"match_probability\": 0.5, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 976.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1055.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.48055145248645986, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5194485475135401, \"precision\": 1.0, \"recall\": 0.48055145248645986, \"specificity\": 1.0, \"npv\": 0.5204545454545455, \"accuracy\": 0.6678211586901763, \"f1\": 0.6491519787163286, \"f2\": 0.5362637362637362, \"f0_5\": 0.8222409435551812, \"p4\": 0.6664067677092192, \"phi\": 0.5001051767092219}, {\"truth_threshold\": 0.9000000134110451, \"match_probability\": 0.6510896818658842, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 971.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1060.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.47808961102904973, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5219103889709503, \"precision\": 1.0, \"recall\": 0.47808961102904973, \"specificity\": 1.0, \"npv\": 0.5192743764172335, \"accuracy\": 0.6662468513853904, \"f1\": 0.6469020652898068, \"f2\": 0.5338097855964816, \"f0_5\": 0.8207945900253593, \"p4\": 0.6647364629140795, \"phi\": 0.49825664535324315}, {\"truth_threshold\": 1.0000000149011612, \"match_probability\": 0.6666666689619328, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 970.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1061.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.4775972427375677, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5224027572624323, \"precision\": 1.0, \"recall\": 0.4775972427375677, \"specificity\": 1.0, \"npv\": 0.5190389845874886, \"accuracy\": 0.6659319899244333, \"f1\": 0.6464511829390204, \"f2\": 0.5333186716516385, \"f0_5\": 0.8205041448147522, \"p4\": 0.6644019432852049, \"phi\": 0.4978871236658882}, {\"truth_threshold\": 1.5000000223517418, \"match_probability\": 0.7387961280260511, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 969.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1062.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.4771048744460857, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5228951255539144, \"precision\": 1.0, \"recall\": 0.4771048744460857, \"specificity\": 1.0, \"npv\": 0.5188038060715904, \"accuracy\": 0.6656171284634761, \"f1\": 0.646, \"f2\": 0.5328274496865721, \"f0_5\": 0.8202133062468258, \"p4\": 0.6640672695017632, \"phi\": 0.4975176627597633}, {\"truth_threshold\": 1.700000025331974, \"match_probability\": 0.7646510400908766, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 968.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1063.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.47661250615460365, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5233874938453964, \"precision\": 1.0, \"recall\": 0.47661250615460365, \"specificity\": 1.0, \"npv\": 0.5185688405797102, \"accuracy\": 0.6653022670025189, \"f1\": 0.6455485161720573, \"f2\": 0.5323361196656401, \"f0_5\": 0.819922073521938, \"p4\": 0.6637324410189356, \"phi\": 0.49714826231455617}, {\"truth_threshold\": 1.9000000283122063, \"match_probability\": 0.7886787621992872, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 967.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1064.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.47612013786312163, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5238798621368784, \"precision\": 1.0, \"recall\": 0.47612013786312163, \"specificity\": 1.0, \"npv\": 0.5183340878225441, \"accuracy\": 0.6649874055415617, \"f1\": 0.6450967311541027, \"f2\": 0.5318446815531844, \"f0_5\": 0.8196304458382777, \"p4\": 0.6633974572904727, \"phi\": 0.49677892200980617}, {\"truth_threshold\": 2.0000000298023224, \"match_probability\": 0.8000000033051833, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 961.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1070.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.4731659281142294, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5268340718857706, \"precision\": 1.0, \"recall\": 0.4731659281142294, \"specificity\": 1.0, \"npv\": 0.5169300225733634, \"accuracy\": 0.6630982367758187, \"f1\": 0.642379679144385, \"f2\": 0.5288937809576224, \"f0_5\": 0.8178723404255319, \"p4\": 0.661384263989902, \"phi\": 0.49456412516582227}, {\"truth_threshold\": 2.2000000327825546, \"match_probability\": 0.8212623941099038, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 960.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1071.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.4726735598227474, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5273264401772526, \"precision\": 1.0, \"recall\": 0.4726735598227474, \"specificity\": 1.0, \"npv\": 0.5166967509025271, \"accuracy\": 0.6627833753148614, \"f1\": 0.641925777331996, \"f2\": 0.5284015852047557, \"f0_5\": 0.8175779253960143, \"p4\": 0.6610481781257823, \"phi\": 0.49419519685843255}, {\"truth_threshold\": 2.400000035762787, \"match_probability\": 0.8407144092272857, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 959.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1072.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.47218119153126537, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5278188084687346, \"precision\": 1.0, \"recall\": 0.47218119153126537, \"specificity\": 1.0, \"npv\": 0.5164636896707262, \"accuracy\": 0.6624685138539043, \"f1\": 0.6414715719063545, \"f2\": 0.5279092810745348, \"f0_5\": 0.8172831089142663, \"p4\": 0.6607119325939106, \"phi\": 0.49382632612220784}, {\"truth_threshold\": 2.500000037252903, \"match_probability\": 0.8497788984739328, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 957.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1074.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.4711964549483013, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5288035450516987, \"precision\": 1.0, \"recall\": 0.4711964549483013, \"specificity\": 1.0, \"npv\": 0.51599819738621, \"accuracy\": 0.6618387909319899, \"f1\": 0.6405622489959839, \"f2\": 0.5269243475388173, \"f0_5\": 0.8166922683051715, \"p4\": 0.6600389602879736, \"phi\": 0.49308875607551217}, {\"truth_threshold\": 2.600000038743019, \"match_probability\": 0.8584144256340188, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 954.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1077.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.46971935007385524, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5302806499261448, \"precision\": 1.0, \"recall\": 0.46971935007385524, \"specificity\": 1.0, \"npv\": 0.5153015301530153, \"accuracy\": 0.6608942065491183, \"f1\": 0.6391959798994975, \"f2\": 0.5254461335095836, \"f0_5\": 0.8158029758850692, \"p4\": 0.6590282902884685, \"phi\": 0.4919828247363291}, {\"truth_threshold\": 2.8000000417232513, \"match_probability\": 0.8744413378412453, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 950.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1081.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.46774987690792713, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5322501230920729, \"precision\": 1.0, \"recall\": 0.46774987690792713, \"specificity\": 1.0, \"npv\": 0.5143755615453729, \"accuracy\": 0.6596347607052897, \"f1\": 0.637370010063737, \"f2\": 0.5234736610094777, \"f0_5\": 0.8146115589092779, \"p4\": 0.6576784449706342, \"phi\": 0.4905090270293647}, {\"truth_threshold\": 2.9000000432133675, \"match_probability\": 0.8818562391739494, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 941.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1090.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.4633185622845889, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5366814377154111, \"precision\": 1.0, \"recall\": 0.4633185622845889, \"specificity\": 1.0, \"npv\": 0.5123042505592841, \"accuracy\": 0.656801007556675, \"f1\": 0.6332436069986541, \"f2\": 0.5190292333149475, \"f0_5\": 0.8119068162208801, \"p4\": 0.6546315648771478, \"phi\": 0.4871961297274552}, {\"truth_threshold\": 3.0000000447034836, \"match_probability\": 0.8888888919492438, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 938.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1093.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.46184145741014276, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5381585425898572, \"precision\": 1.0, \"recall\": 0.46184145741014276, \"specificity\": 1.0, \"npv\": 0.5116175156389634, \"accuracy\": 0.6558564231738035, \"f1\": 0.6318625799932637, \"f2\": 0.5175457956301037, \"f0_5\": 0.8109977520318173, \"p4\": 0.6536128891991932, \"phi\": 0.48609276795613343}, {\"truth_threshold\": 3.1000000461935997, \"match_probability\": 0.8955524998434058, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 934.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1097.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.45987198424421466, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5401280157557853, \"precision\": 1.0, \"recall\": 0.45987198424421466, \"specificity\": 1.0, \"npv\": 0.5107047279214987, \"accuracy\": 0.6545969773299748, \"f1\": 0.6300168634064081, \"f2\": 0.5155663501876794, \"f0_5\": 0.8097797815155193, \"p4\": 0.6522522396145514, \"phi\": 0.4846223236626243}, {\"truth_threshold\": 3.200000047683716, \"match_probability\": 0.9018605969116819, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 933.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1098.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.45937961595273263, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5406203840472673, \"precision\": 1.0, \"recall\": 0.45937961595273263, \"specificity\": 1.0, \"npv\": 0.5104770396790014, \"accuracy\": 0.6542821158690176, \"f1\": 0.6295546558704453, \"f2\": 0.515071215634316, \"f0_5\": 0.8094742321707444, \"p4\": 0.6519116419396886, \"phi\": 0.48425483625920296}, {\"truth_threshold\": 3.400000050663948, \"match_probability\": 0.9134653434169965, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 932.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1099.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.4588872476612506, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5411127523387493, \"precision\": 1.0, \"recall\": 0.4588872476612506, \"specificity\": 1.0, \"npv\": 0.5102495543672014, \"accuracy\": 0.6539672544080605, \"f1\": 0.6290921363482956, \"f2\": 0.5145759717314488, \"f0_5\": 0.8091682583781906, \"p4\": 0.6515708689560343, \"phi\": 0.4838873976701033}, {\"truth_threshold\": 3.500000052154064, \"match_probability\": 0.9187896995557598, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 931.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1100.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.4583948793697686, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5416051206302314, \"precision\": 1.0, \"recall\": 0.4583948793697686, \"specificity\": 1.0, \"npv\": 0.5100222717149221, \"accuracy\": 0.6536523929471033, \"f1\": 0.6286293045239703, \"f2\": 0.5140806184428492, \"f0_5\": 0.8088618592528236, \"p4\": 0.6512299200620687, \"phi\": 0.4835200075681016}, {\"truth_threshold\": 3.6000000536441803, \"match_probability\": 0.9238137785296746, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 930.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1101.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.45790251107828656, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5420974889217134, \"precision\": 1.0, \"recall\": 0.45790251107828656, \"specificity\": 1.0, \"npv\": 0.5097951914514692, \"accuracy\": 0.6533375314861462, \"f1\": 0.6281661600810537, \"f2\": 0.513585155732273, \"f0_5\": 0.8085550339071466, \"p4\": 0.650888794654625, \"phi\": 0.48315266562574566}, {\"truth_threshold\": 3.7000000551342964, \"match_probability\": 0.9285512128432143, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 923.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1108.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.4544559330379124, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5455440669620877, \"precision\": 1.0, \"recall\": 0.4544559330379124, \"specificity\": 1.0, \"npv\": 0.5082112738570794, \"accuracy\": 0.6511335012594458, \"f1\": 0.6249153689911984, \"f2\": 0.5101138498949929, \"f0_5\": 0.8063952472479469, \"p4\": 0.6484959234103079, \"phi\": 0.4805825929443398}, {\"truth_threshold\": 3.9000000581145287, \"match_probability\": 0.9372195616099515, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 920.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1111.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.4529788281634663, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5470211718365338, \"precision\": 1.0, \"recall\": 0.4529788281634663, \"specificity\": 1.0, \"npv\": 0.5075354609929078, \"accuracy\": 0.6501889168765743, \"f1\": 0.6235174517112844, \"f2\": 0.508624502432552, \"f0_5\": 0.8054631413062511, \"p4\": 0.6474676984517845, \"phi\": 0.47948182277534984}, {\"truth_threshold\": 4.000000059604645, \"match_probability\": 0.9411764728755594, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 919.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1112.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.45248645987198427, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5475135401280158, \"precision\": 1.0, \"recall\": 0.45248645987198427, \"specificity\": 1.0, \"npv\": 0.5073105892778024, \"accuracy\": 0.6498740554156172, \"f1\": 0.6230508474576271, \"f2\": 0.5081278336835121, \"f0_5\": 0.8051515682495182, \"p4\": 0.6471245911096051, \"phi\": 0.4791149889096385}, {\"truth_threshold\": 4.100000061094761, \"match_probability\": 0.9448986513716398, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 916.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1115.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.45100935499753814, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5489906450024619, \"precision\": 1.0, \"recall\": 0.45100935499753814, \"specificity\": 1.0, \"npv\": 0.5066371681415929, \"accuracy\": 0.6489294710327456, \"f1\": 0.6216491347132678, \"f2\": 0.5066371681415929, \"f0_5\": 0.8042142230026339, \"p4\": 0.6460941632868983, \"phi\": 0.4780147512591208}, {\"truth_threshold\": 4.200000062584877, \"match_probability\": 0.9483982147343843, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 915.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1116.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.4505169867060561, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5494830132939439, \"precision\": 1.0, \"recall\": 0.4505169867060561, \"specificity\": 1.0, \"npv\": 0.5064130915524104, \"accuracy\": 0.6486146095717884, \"f1\": 0.6211812627291242, \"f2\": 0.5061400597411218, \"f0_5\": 0.8039008961518187, \"p4\": 0.6457503166575753, \"phi\": 0.477648092254842}, {\"truth_threshold\": 4.300000064074993, \"match_probability\": 0.9516868803254299, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 911.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1120.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.448547513540128, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.551452486459872, \"precision\": 1.0, \"recall\": 0.448547513540128, \"specificity\": 1.0, \"npv\": 0.5055187637969095, \"accuracy\": 0.6473551637279596, \"f1\": 0.619306594153637, \"f2\": 0.5041505257332596, \"f0_5\": 0.8026431718061674, \"p4\": 0.6443730598755232, \"phi\": 0.47618188179411347}, {\"truth_threshold\": 4.400000065565109, \"match_probability\": 0.9547759482410569, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 908.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1123.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.44707040866568193, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5529295913343181, \"precision\": 1.0, \"recall\": 0.44707040866568193, \"specificity\": 1.0, \"npv\": 0.5048500881834215, \"accuracy\": 0.6464105793450882, \"f1\": 0.6178972439605308, \"f2\": 0.5026572187776793, \"f0_5\": 0.8016952145505916, \"p4\": 0.6433381357110801, \"phi\": 0.4750826614801553}, {\"truth_threshold\": 4.500000067055225, \"match_probability\": 0.9576762895591182, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 905.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1126.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.44559330379123585, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5544066962087641, \"precision\": 1.0, \"recall\": 0.44559330379123585, \"specificity\": 1.0, \"npv\": 0.5041831792162044, \"accuracy\": 0.6454659949622166, \"f1\": 0.6164850136239782, \"f2\": 0.5011629194816701, \"f0_5\": 0.8007432312864979, \"p4\": 0.6423014938325172, \"phi\": 0.47398380620324704}, {\"truth_threshold\": 4.6000000685453415, \"match_probability\": 0.9603983391922627, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 904.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1127.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.4451009354997538, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5548990645002462, \"precision\": 1.0, \"recall\": 0.4451009354997538, \"specificity\": 1.0, \"npv\": 0.5039612676056338, \"accuracy\": 0.6451511335012594, \"f1\": 0.6160136286201022, \"f2\": 0.5006645990252547, \"f0_5\": 0.8004250044271295, \"p4\": 0.6419555618126742, \"phi\": 0.47361760067264114}, {\"truth_threshold\": 4.700000070035458, \"match_probability\": 0.9629520927573305, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 902.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1129.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.4441161989167898, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5558838010832102, \"precision\": 1.0, \"recall\": 0.4441161989167898, \"specificity\": 1.0, \"npv\": 0.5035180299032542, \"accuracy\": 0.6445214105793451, \"f1\": 0.6150698943061712, \"f2\": 0.49966762685575006, \"f0_5\": 0.7997871963114027, \"p4\": 0.6412631167843771, \"phi\": 0.4728853069473651}, {\"truth_threshold\": 4.90000007301569, \"match_probability\": 0.9675925026740654, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 896.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1135.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.44116198916789756, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5588380108321024, \"precision\": 1.0, \"recall\": 0.44116198916789756, \"specificity\": 1.0, \"npv\": 0.5021929824561403, \"accuracy\": 0.642632241813602, \"f1\": 0.612230953194397, \"f2\": 0.49667405764966743, \"f0_5\": 0.7978628673196795, \"p4\": 0.6391810866147006, \"phi\": 0.47068934031536125}, {\"truth_threshold\": 5.000000074505806, \"match_probability\": 0.969696971214501, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 875.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1156.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.43082225504677496, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.569177744953225, \"precision\": 1.0, \"recall\": 0.43082225504677496, \"specificity\": 1.0, \"npv\": 0.49760973489787047, \"accuracy\": 0.6360201511335013, \"f1\": 0.6022023399862354, \"f2\": 0.4861651294588288, \"f0_5\": 0.7909962032182245, \"p4\": 0.631836685786, \"phi\": 0.463013334712866}, {\"truth_threshold\": 5.100000075995922, \"match_probability\": 0.9716687817966767, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 873.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1158.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.4298375184638109, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5701624815361891, \"precision\": 1.0, \"recall\": 0.4298375184638109, \"specificity\": 1.0, \"npv\": 0.49717759444203213, \"accuracy\": 0.6353904282115869, \"f1\": 0.6012396694214877, \"f2\": 0.4851617205735245, \"f0_5\": 0.7903313416621401, \"p4\": 0.6311324185690581, \"phi\": 0.46228301226712853}, {\"truth_threshold\": 5.200000077486038, \"match_probability\": 0.9735157914041783, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 863.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1168.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.42491383554899065, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5750861644510094, \"precision\": 1.0, \"recall\": 0.42491383554899065, \"specificity\": 1.0, \"npv\": 0.4950281020319931, \"accuracy\": 0.6322418136020151, \"f1\": 0.5964063579820318, \"f2\": 0.48013797707800154, \"f0_5\": 0.7869779317891665, \"p4\": 0.6275980948521758, \"phi\": 0.458633066338387}, {\"truth_threshold\": 5.300000078976154, \"match_probability\": 0.9752454557772836, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 861.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1170.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.4239290989660266, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5760709010339734, \"precision\": 1.0, \"recall\": 0.4239290989660266, \"specificity\": 1.0, \"npv\": 0.4946004319654428, \"accuracy\": 0.6316120906801007, \"f1\": 0.5954356846473029, \"f2\": 0.4791318864774624, \"f0_5\": 0.7863013698630137, \"p4\": 0.6268885921404044, \"phi\": 0.4579033909803657}, {\"truth_threshold\": 5.4000000804662704, \"match_probability\": 0.9768648415470134, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 846.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1185.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.41654357459379615, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5834564254062038, \"precision\": 1.0, \"recall\": 0.41654357459379615, \"specificity\": 1.0, \"npv\": 0.49141630901287553, \"accuracy\": 0.6268891687657431, \"f1\": 0.5881126173096975, \"f2\": 0.47157190635451507, \"f0_5\": 0.7811634349030471, \"p4\": 0.6215384467313602, \"phi\": 0.4524337586541401}, {\"truth_threshold\": 5.500000081956387, \"match_probability\": 0.9783806392104205, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 843.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1188.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.4150664697193501, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5849335302806499, \"precision\": 1.0, \"recall\": 0.4150664697193501, \"specificity\": 1.0, \"npv\": 0.49078439777111016, \"accuracy\": 0.6259445843828715, \"f1\": 0.5866388308977035, \"f2\": 0.47005687520910006, \"f0_5\": 0.7801221543586896, \"p4\": 0.620462167129168, \"phi\": 0.45134038970182133}, {\"truth_threshold\": 5.600000083446503, \"match_probability\": 0.9797991767207457, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 840.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1191.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.413589364844904, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5864106351550961, \"precision\": 1.0, \"recall\": 0.413589364844904, \"specificity\": 1.0, \"npv\": 0.4901541095890411, \"accuracy\": 0.625, \"f1\": 0.5851619644723093, \"f2\": 0.4685408299866131, \"f0_5\": 0.7790762381747357, \"p4\": 0.6193837556660524, \"phi\": 0.4502471841789251}, {\"truth_threshold\": 5.700000084936619, \"match_probability\": 0.9811264334957893, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 838.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1193.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.41260462826193994, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5873953717380601, \"precision\": 1.0, \"recall\": 0.41260462826193994, \"specificity\": 1.0, \"npv\": 0.4897348160821215, \"accuracy\": 0.6243702770780857, \"f1\": 0.5841756709654932, \"f2\": 0.4675295692925686, \"f0_5\": 0.7783763700538733, \"p4\": 0.6186636199117341, \"phi\": 0.4495184665133272}, {\"truth_threshold\": 5.800000086426735, \"match_probability\": 0.9823680546749124, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 837.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1194.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.4121122599704579, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5878877400295421, \"precision\": 1.0, \"recall\": 0.4121122599704579, \"specificity\": 1.0, \"npv\": 0.48952543822146216, \"accuracy\": 0.6240554156171285, \"f1\": 0.5836820083682008, \"f2\": 0.4670237696685638, \"f0_5\": 0.7780256553262688, \"p4\": 0.6183031912422774, \"phi\": 0.44915413240721225}, {\"truth_threshold\": 5.900000087916851, \"match_probability\": 0.9835293654795508, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 835.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1196.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.41112752338749387, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5888724766125062, \"precision\": 1.0, \"recall\": 0.41112752338749387, \"specificity\": 1.0, \"npv\": 0.4891072191371209, \"accuracy\": 0.6234256926952141, \"f1\": 0.5826936496859735, \"f2\": 0.4660118316776426, \"f0_5\": 0.7773226587227704, \"p4\": 0.6175816083638278, \"phi\": 0.4484255118464925}, {\"truth_threshold\": 6.000000089406967, \"match_probability\": 0.9846153855541349, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 833.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1198.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.4101427868045298, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5898572131954702, \"precision\": 1.0, \"recall\": 0.4101427868045298, \"specificity\": 1.0, \"npv\": 0.4886897140418267, \"accuracy\": 0.6227959697732998, \"f1\": 0.5817039106145251, \"f2\": 0.46499944177738084, \"f0_5\": 0.7766175647958232, \"p4\": 0.6168590527979635, \"phi\": 0.44769695241292806}, {\"truth_threshold\": 6.200000092387199, \"match_probability\": 0.9865801893041345, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 811.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1220.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.3993106843919252, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6006893156080748, \"precision\": 1.0, \"recall\": 0.3993106843919252, \"specificity\": 1.0, \"npv\": 0.48414376321353064, \"accuracy\": 0.6158690176322418, \"f1\": 0.5707248416608023, \"f2\": 0.45383324006715164, \"f0_5\": 0.7687203791469195, \"p4\": 0.6088448866523514, \"phi\": 0.43968599867732555}, {\"truth_threshold\": 6.400000095367432, \"match_probability\": 0.9882970460445225, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 803.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1228.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.3953717380600689, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.604628261939931, \"precision\": 1.0, \"recall\": 0.3953717380600689, \"specificity\": 1.0, \"npv\": 0.48251158870627897, \"accuracy\": 0.6133501259445844, \"f1\": 0.5666901905434015, \"f2\": 0.4497591576117397, \"f0_5\": 0.7657829486934961, \"p4\": 0.6058995526108901, \"phi\": 0.436773906570581}, {\"truth_threshold\": 6.500000096857548, \"match_probability\": 0.9890721936212699, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 801.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1230.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.39438700147710487, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6056129985228951, \"precision\": 1.0, \"recall\": 0.39438700147710487, \"specificity\": 1.0, \"npv\": 0.48210526315789476, \"accuracy\": 0.6127204030226701, \"f1\": 0.565677966101695, \"f2\": 0.44873949579831934, \"f0_5\": 0.7650429799426934, \"p4\": 0.6051605368383547, \"phi\": 0.43604592548626425}, {\"truth_threshold\": 6.600000098347664, \"match_probability\": 0.9897965292084853, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 800.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1231.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.39389463318562284, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6061053668143771, \"precision\": 1.0, \"recall\": 0.39389463318562284, \"specificity\": 1.0, \"npv\": 0.4819023569023569, \"accuracy\": 0.6124055415617129, \"f1\": 0.5651713175556341, \"f2\": 0.44822949350067237, \"f0_5\": 0.7646721468170522, \"p4\": 0.6047906217838517, \"phi\": 0.43568193915210784}, {\"truth_threshold\": 6.70000009983778, \"match_probability\": 0.9904733155885336, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 798.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1233.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.3929098966026588, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6070901033973413, \"precision\": 1.0, \"recall\": 0.3929098966026588, \"specificity\": 1.0, \"npv\": 0.48149705634987383, \"accuracy\": 0.6117758186397985, \"f1\": 0.5641569459172853, \"f2\": 0.44720914593140554, \"f0_5\": 0.7639287765651924, \"p4\": 0.6040499729033301, \"phi\": 0.4349539729958948}, {\"truth_threshold\": 6.800000101327896, \"match_probability\": 0.9911056147706719, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 795.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1236.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.3914327917282127, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6085672082717873, \"precision\": 1.0, \"recall\": 0.3914327917282127, \"specificity\": 1.0, \"npv\": 0.48089038219235614, \"accuracy\": 0.610831234256927, \"f1\": 0.5626326963906582, \"f2\": 0.44567776656575847, \"f0_5\": 0.7628094415659182, \"p4\": 0.6029369392641755, \"phi\": 0.4338620343113708}, {\"truth_threshold\": 6.900000102818012, \"match_probability\": 0.9916962992137202, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 781.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1250.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.3845396356474643, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6154603643525357, \"precision\": 1.0, \"recall\": 0.3845396356474643, \"specificity\": 1.0, \"npv\": 0.4780793319415449, \"accuracy\": 0.6064231738035264, \"f1\": 0.5554765291607396, \"f2\": 0.43851768669286917, \"f0_5\": 0.7575169738118331, \"p4\": 0.5977094083362298, \"phi\": 0.4287661974962401}, {\"truth_threshold\": 7.1000001057982445, \"match_probability\": 0.9927634299608046, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 778.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1253.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.3830625307730182, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6169374692269818, \"precision\": 1.0, \"recall\": 0.3830625307730182, \"specificity\": 1.0, \"npv\": 0.4774812343619683, \"accuracy\": 0.6054785894206549, \"f1\": 0.553933784264863, \"f2\": 0.43698045383059986, \"f0_5\": 0.756367878670037, \"p4\": 0.5965819146561916, \"phi\": 0.4276741400076935}, {\"truth_threshold\": 7.200000107288361, \"match_probability\": 0.9932447677519157, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 776.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1255.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.38207779419005417, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6179222058099458, \"precision\": 1.0, \"recall\": 0.38207779419005417, \"specificity\": 1.0, \"npv\": 0.47708333333333336, \"accuracy\": 0.6048488664987406, \"f1\": 0.5529034556465978, \"f2\": 0.43595505617977526, \"f0_5\": 0.7555988315481986, \"p4\": 0.5958287894168168, \"phi\": 0.4269460711200401}, {\"truth_threshold\": 7.300000108778477, \"match_probability\": 0.9936942928922654, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 775.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1256.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.38158542589857214, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6184145741014279, \"precision\": 1.0, \"recall\": 0.38158542589857214, \"specificity\": 1.0, \"npv\": 0.47688463140358184, \"accuracy\": 0.6045340050377834, \"f1\": 0.5523877405559515, \"f2\": 0.4354421845151141, \"f0_5\": 0.7552134086922627, \"p4\": 0.5954517850971813, \"phi\": 0.426582026319229}, {\"truth_threshold\": 7.400000110268593, \"match_probability\": 0.9941140817673122, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 774.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1257.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.3810930576070901, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6189069423929099, \"precision\": 1.0, \"recall\": 0.3810930576070901, \"specificity\": 1.0, \"npv\": 0.47668609492089925, \"accuracy\": 0.6042191435768262, \"f1\": 0.5518716577540107, \"f2\": 0.4349291975724882, \"f0_5\": 0.7548273844353423, \"p4\": 0.5950744850307271, \"phi\": 0.4262179740839059}, {\"truth_threshold\": 7.500000111758709, \"match_probability\": 0.9945060786121668, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 765.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1266.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.3766617429837518, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6233382570162481, \"precision\": 1.0, \"recall\": 0.3766617429837518, \"specificity\": 1.0, \"npv\": 0.47490667772708417, \"accuracy\": 0.6013853904282116, \"f1\": 0.5472103004291845, \"f2\": 0.43030712116098546, \"f0_5\": 0.7513258691809075, \"p4\": 0.591665315716949, \"phi\": 0.42294110344976693}, {\"truth_threshold\": 7.600000113248825, \"match_probability\": 0.9948721034855129, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 761.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1270.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.3746922698178237, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6253077301821762, \"precision\": 1.0, \"recall\": 0.3746922698178237, \"specificity\": 1.0, \"npv\": 0.474120082815735, \"accuracy\": 0.6001259445843828, \"f1\": 0.5451289398280802, \"f2\": 0.4282498593134496, \"f0_5\": 0.7497536945812808, \"p4\": 0.5901422282424653, \"phi\": 0.4214844362446167}, {\"truth_threshold\": 7.700000114738941, \"match_probability\": 0.9952138598197071, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 759.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1272.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.37370753323485967, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6262924667651403, \"precision\": 1.0, \"recall\": 0.37370753323485967, \"specificity\": 1.0, \"npv\": 0.47372776168804304, \"accuracy\": 0.5994962216624685, \"f1\": 0.5440860215053763, \"f2\": 0.4272205336035123, \"f0_5\": 0.7489638839550029, \"p4\": 0.5893788313133635, \"phi\": 0.42075602579797955}, {\"truth_threshold\": 7.800000116229057, \"match_probability\": 0.9955329415617687, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 756.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1275.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.3722304283604136, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6277695716395865, \"precision\": 1.0, \"recall\": 0.3722304283604136, \"specificity\": 1.0, \"npv\": 0.4731404958677686, \"accuracy\": 0.5985516372795969, \"f1\": 0.542518837459634, \"f2\": 0.42567567567567566, \"f0_5\": 0.7477744807121661, \"p4\": 0.5882313967029971, \"phi\": 0.41966330486655373}, {\"truth_threshold\": 7.900000117719173, \"match_probability\": 0.99583083992065, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 750.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1281.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.36927621861152143, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6307237813884786, \"precision\": 1.0, \"recall\": 0.36927621861152143, \"specificity\": 1.0, \"npv\": 0.47197032151690027, \"accuracy\": 0.5966624685138538, \"f1\": 0.5393743257820928, \"f2\": 0.42258282623394183, \"f0_5\": 0.7453786523553966, \"p4\": 0.5859280050558678, \"phi\": 0.41747744325487207}, {\"truth_threshold\": 8.00000011920929, \"match_probability\": 0.9961089497366072, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 739.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1292.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.3638601674052191, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6361398325947809, \"precision\": 1.0, \"recall\": 0.3638601674052191, \"specificity\": 1.0, \"npv\": 0.4698399671727534, \"accuracy\": 0.593198992443325, \"f1\": 0.5335740072202166, \"f2\": 0.41690172627778405, \"f0_5\": 0.7409264086625226, \"p4\": 0.5816749582556248, \"phi\": 0.41346831693509567}, {\"truth_threshold\": 8.200000122189522, \"match_probability\": 0.9966109369567457, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 737.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1294.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.36287543082225504, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.637124569177745, \"precision\": 1.0, \"recall\": 0.36287543082225504, \"specificity\": 1.0, \"npv\": 0.4694546945469455, \"accuracy\": 0.5925692695214105, \"f1\": 0.5325144508670521, \"f2\": 0.41586728360230224, \"f0_5\": 0.7401084555131553, \"p4\": 0.5808973960068617, \"phi\": 0.41273911195239665}, {\"truth_threshold\": 8.300000123679638, \"match_probability\": 0.9968371745531442, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 735.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1296.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.361890694239291, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.638109305760709, \"precision\": 1.0, \"recall\": 0.361890694239291, \"specificity\": 1.0, \"npv\": 0.46907005325686196, \"accuracy\": 0.5919395465994962, \"f1\": 0.5314533622559653, \"f2\": 0.4148323738570945, \"f0_5\": 0.7392878696439348, \"p4\": 0.580118494928293, \"phi\": 0.412009814470465}, {\"truth_threshold\": 8.600000128149986, \"match_probability\": 0.9974294610402847, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 726.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1305.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.35745937961595275, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6425406203840472, \"precision\": 1.0, \"recall\": 0.35745937961595275, \"specificity\": 1.0, \"npv\": 0.4673469387755102, \"accuracy\": 0.5891057934508817, \"f1\": 0.5266594124047879, \"f2\": 0.4101694915254237, \"f0_5\": 0.7355623100303952, \"p4\": 0.5765966357046058, \"phi\": 0.40872673854313535}, {\"truth_threshold\": 8.700000129640102, \"match_probability\": 0.997601189412643, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 725.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1306.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.35696701132447073, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6430329886755293, \"precision\": 1.0, \"recall\": 0.35696701132447073, \"specificity\": 1.0, \"npv\": 0.467156262749898, \"accuracy\": 0.5887909319899244, \"f1\": 0.5261248185776488, \"f2\": 0.40965080800090403, \"f0_5\": 0.7351450010139932, \"p4\": 0.5762035983008024, \"phi\": 0.4083618186551483}, {\"truth_threshold\": 8.900000132620335, \"match_probability\": 0.9979110654305032, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 720.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1311.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.35450516986706054, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6454948301329394, \"precision\": 1.0, \"recall\": 0.35450516986706054, \"specificity\": 1.0, \"npv\": 0.46620521172638435, \"accuracy\": 0.5872166246851386, \"f1\": 0.5234460196292258, \"f2\": 0.40705563093622793, \"f0_5\": 0.7330482590103848, \"p4\": 0.5742331672939941, \"phi\": 0.406536785267915}, {\"truth_threshold\": 9.00000013411045, \"match_probability\": 0.9980506824420605, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 715.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1316.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.3520433284096504, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6479566715903495, \"precision\": 1.0, \"recall\": 0.3520433284096504, \"specificity\": 1.0, \"npv\": 0.46525802519301096, \"accuracy\": 0.5856423173803527, \"f1\": 0.5207574654042243, \"f2\": 0.4044575178187578, \"f0_5\": 0.7309343692496422, \"p4\": 0.5722538908091974, \"phi\": 0.4047109879386135}, {\"truth_threshold\": 9.100000135600567, \"match_probability\": 0.9981809849551747, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 710.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1321.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.3495814869522403, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6504185130477598, \"precision\": 1.0, \"recall\": 0.3495814869522403, \"specificity\": 1.0, \"npv\": 0.4643146796431468, \"accuracy\": 0.5840680100755667, \"f1\": 0.5180591025173295, \"f2\": 0.4018564636631198, \"f0_5\": 0.7288031205091356, \"p4\": 0.5702656229859941, \"phi\": 0.4028843706616135}, {\"truth_threshold\": 9.200000137090683, \"match_probability\": 0.9983025921847976, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 709.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1322.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.34908911866075826, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6509108813392418, \"precision\": 1.0, \"recall\": 0.34908911866075826, \"specificity\": 1.0, \"npv\": 0.4641264693960276, \"accuracy\": 0.5837531486146096, \"f1\": 0.5175182481751824, \"f2\": 0.40133589946790443, \"f0_5\": 0.7283747688514486, \"p4\": 0.5698668774463831, \"phi\": 0.40251894383816106}, {\"truth_threshold\": 9.300000138580799, \"match_probability\": 0.9984160824655384, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 705.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1326.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.34711964549483015, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6528803545051699, \"precision\": 1.0, \"recall\": 0.34711964549483015, \"specificity\": 1.0, \"npv\": 0.4633751517604209, \"accuracy\": 0.5824937027707808, \"f1\": 0.5153508771929824, \"f2\": 0.399252463472647, \"f0_5\": 0.7266542980828695, \"p4\": 0.5682682154171067, \"phi\": 0.40105687677708557}, {\"truth_threshold\": 9.400000140070915, \"match_probability\": 0.9985219959137808, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 697.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1334.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.3431806991629739, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6568193008370261, \"precision\": 1.0, \"recall\": 0.3431806991629739, \"specificity\": 1.0, \"npv\": 0.4618797902379992, \"accuracy\": 0.5799748110831234, \"f1\": 0.5109970674486803, \"f2\": 0.39507992291123456, \"f0_5\": 0.7231790827972608, \"p4\": 0.5650529747120447, \"phi\": 0.39813091985316124}, {\"truth_threshold\": 9.600000143051147, \"match_probability\": 0.9987130764898899, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 691.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1340.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.34022648941408173, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6597735105859183, \"precision\": 1.0, \"recall\": 0.34022648941408173, \"specificity\": 1.0, \"npv\": 0.4607645875251509, \"accuracy\": 0.5780856423173804, \"f1\": 0.5077149155033064, \"f2\": 0.39194554736245035, \"f0_5\": 0.7205422314911366, \"p4\": 0.5626255551091374, \"phi\": 0.39593473964784837}, {\"truth_threshold\": 9.700000144541264, \"match_probability\": 0.9987991544181472, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 689.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1342.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.3392417528311177, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6607582471688823, \"precision\": 1.0, \"recall\": 0.3392417528311177, \"specificity\": 1.0, \"npv\": 0.46039404905508646, \"accuracy\": 0.577455919395466, \"f1\": 0.5066176470588235, \"f2\": 0.3908998071031431, \"f0_5\": 0.7196574054731565, \"p4\": 0.5618133159380493, \"phi\": 0.39520233323509507}, {\"truth_threshold\": 9.80000014603138, \"match_probability\": 0.9988794813467569, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 683.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1348.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.3362875430822255, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6637124569177745, \"precision\": 1.0, \"recall\": 0.3362875430822255, \"specificity\": 1.0, \"npv\": 0.4592860008022463, \"accuracy\": 0.575566750629723, \"f1\": 0.5033161385408991, \"f2\": 0.3877597365731804, \"f0_5\": 0.7169850934285115, \"p4\": 0.5593671566036346, \"phi\": 0.3930040213303783}, {\"truth_threshold\": 9.900000147521496, \"match_probability\": 0.9989544406735176, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 670.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1361.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.32988675529295913, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6701132447070408, \"precision\": 1.0, \"recall\": 0.32988675529295913, \"specificity\": 1.0, \"npv\": 0.45690343176376697, \"accuracy\": 0.5714735516372796, \"f1\": 0.49611255090707146, \"f2\": 0.38094155105753924, \"f0_5\": 0.7111016769263426, \"p4\": 0.5540174206937766, \"phi\": 0.38823496826891707}, {\"truth_threshold\": 10.000000149011612, \"match_probability\": 0.9990243903445719, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 664.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1367.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.326932545544067, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6730674544559331, \"precision\": 1.0, \"recall\": 0.326932545544067, \"specificity\": 1.0, \"npv\": 0.45581210191082805, \"accuracy\": 0.5695843828715366, \"f1\": 0.49276437847866417, \"f2\": 0.377787892580792, \"f0_5\": 0.7083422231704715, \"p4\": 0.5515247216052599, \"phi\": 0.3860308417309408}, {\"truth_threshold\": 10.100000150501728, \"match_probability\": 0.9990896645300149, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 657.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1374.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.32348596750369274, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6765140324963073, \"precision\": 1.0, \"recall\": 0.32348596750369274, \"specificity\": 1.0, \"npv\": 0.45454545454545453, \"accuracy\": 0.5673803526448362, \"f1\": 0.4888392857142857, \"f2\": 0.37410317731465664, \"f0_5\": 0.7050869285254346, \"p4\": 0.5485971943887775, \"phi\": 0.38345674611100816}, {\"truth_threshold\": 10.30000015348196, \"match_probability\": 0.9992074135451509, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 653.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1378.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.32151649433776464, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6784835056622354, \"precision\": 1.0, \"recall\": 0.32151649433776464, \"specificity\": 1.0, \"npv\": 0.453824811732065, \"accuracy\": 0.5661209068010076, \"f1\": 0.48658718330849476, \"f2\": 0.3719949868975732, \"f0_5\": 0.7032091320267069, \"p4\": 0.5469147740904808, \"phi\": 0.38198450559098546}, {\"truth_threshold\": 10.400000154972076, \"match_probability\": 0.9992604514366183, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 652.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1379.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.3210241260462826, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6789758739537174, \"precision\": 1.0, \"recall\": 0.3210241260462826, \"specificity\": 1.0, \"npv\": 0.45364500792393025, \"accuracy\": 0.5658060453400504, \"f1\": 0.48602310846067837, \"f2\": 0.3714676390154968, \"f0_5\": 0.702737658978228, \"p4\": 0.5464930695544453, \"phi\": 0.381616289227856}, {\"truth_threshold\": 10.500000156462193, \"match_probability\": 0.9993099426168967, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 650.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1381.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.32003938946331856, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6799606105366814, \"precision\": 1.0, \"recall\": 0.32003938946331856, \"specificity\": 1.0, \"npv\": 0.45328582739509105, \"accuracy\": 0.565176322418136, \"f1\": 0.484893696381947, \"f2\": 0.3704125826304992, \"f0_5\": 0.7017922694882315, \"p4\": 0.5456483308421441, \"phi\": 0.3808796653168821}, {\"truth_threshold\": 10.600000157952309, \"match_probability\": 0.9993561239419685, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 645.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1386.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.3175775480059084, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6824224519940916, \"precision\": 1.0, \"recall\": 0.3175775480059084, \"specificity\": 1.0, \"npv\": 0.4523903595416831, \"accuracy\": 0.5636020151133502, \"f1\": 0.4820627802690583, \"f2\": 0.3677728361272665, \"f0_5\": 0.6994144437215355, \"p4\": 0.5435286584827342, \"phi\": 0.37903696538036896}, {\"truth_threshold\": 10.700000159442425, \"match_probability\": 0.9993992164911604, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 628.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1403.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.30920728705071393, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.690792712949286, \"precision\": 1.0, \"recall\": 0.30920728705071393, \"specificity\": 1.0, \"npv\": 0.44937205651491363, \"accuracy\": 0.5582493702770781, \"f1\": 0.47235802933433624, \"f2\": 0.3587751371115174, \"f0_5\": 0.6911732335461149, \"p4\": 0.5362358748780472, \"phi\": 0.37275878859039197}, {\"truth_threshold\": 10.800000160932541, \"match_probability\": 0.9994394266126935, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 626.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1405.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.3082225504677499, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6917774495322502, \"precision\": 1.0, \"recall\": 0.3082225504677499, \"specificity\": 1.0, \"npv\": 0.44901960784313727, \"accuracy\": 0.5576196473551638, \"f1\": 0.47120812946932633, \"f2\": 0.3577142857142857, \"f0_5\": 0.6901874310915105, \"p4\": 0.5353689358368804, \"phi\": 0.3720187747136435}, {\"truth_threshold\": 10.900000162422657, \"match_probability\": 0.9994769469006325, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 617.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1414.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.30379123584441164, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6962087641555884, \"precision\": 1.0, \"recall\": 0.30379123584441164, \"specificity\": 1.0, \"npv\": 0.44744040640875343, \"accuracy\": 0.5547858942065491, \"f1\": 0.466012084592145, \"f2\": 0.35293444685962705, \"f0_5\": 0.6857079350966881, \"p4\": 0.5314436122804537, \"phi\": 0.3686847895284548}, {\"truth_threshold\": 11.000000163912773, \"match_probability\": 0.9995119571076428, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 615.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1416.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.30280649926144754, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6971935007385525, \"precision\": 1.0, \"recall\": 0.30280649926144754, \"specificity\": 1.0, \"npv\": 0.4470909800859039, \"accuracy\": 0.5541561712846348, \"f1\": 0.46485260770975056, \"f2\": 0.35187092344661863, \"f0_5\": 0.6847027388109552, \"p4\": 0.5305658842011279, \"phi\": 0.3679430044601774}, {\"truth_threshold\": 11.10000016540289, \"match_probability\": 0.9995446249976983, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 609.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1422.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.2998522895125554, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.7001477104874446, \"precision\": 1.0, \"recall\": 0.2998522895125554, \"specificity\": 1.0, \"npv\": 0.4460459680560966, \"accuracy\": 0.5522670025188917, \"f1\": 0.46136363636363636, \"f2\": 0.3486774304362762, \"f0_5\": 0.6816655473472129, \"p4\": 0.5279206148758528, \"phi\": 0.3657156063794171}, {\"truth_threshold\": 11.200000166893005, \"match_probability\": 0.9995751071426191, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 597.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1434.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.29394387001477107, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.7060561299852289, \"precision\": 1.0, \"recall\": 0.29394387001477107, \"specificity\": 1.0, \"npv\": 0.4439705312136487, \"accuracy\": 0.5484886649874056, \"f1\": 0.454337899543379, \"f2\": 0.34227726178190576, \"f0_5\": 0.6754921928038018, \"p4\": 0.5225744454433701, \"phi\": 0.3612511814755124}, {\"truth_threshold\": 11.300000168383121, \"match_probability\": 0.9996035496660847, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 590.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1441.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.29049729197439683, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.7095027080256031, \"precision\": 1.0, \"recall\": 0.29049729197439683, \"specificity\": 1.0, \"npv\": 0.44276875483372, \"accuracy\": 0.5462846347607053, \"f1\": 0.45020984357115607, \"f2\": 0.33853568969474407, \"f0_5\": 0.6718287406057846, \"p4\": 0.5194206063238911, \"phi\": 0.3586406617354916}, {\"truth_threshold\": 11.400000169873238, \"match_probability\": 0.99963008893853, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 572.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1459.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.2816346627277203, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.7183653372722797, \"precision\": 1.0, \"recall\": 0.2816346627277203, \"specificity\": 1.0, \"npv\": 0.4397081413210445, \"accuracy\": 0.5406171284634761, \"f1\": 0.4394928928159816, \"f2\": 0.32888684452621897, \"f0_5\": 0.6621903218337578, \"p4\": 0.5111863478106634, \"phi\": 0.35190489351468984}, {\"truth_threshold\": 11.500000171363354, \"match_probability\": 0.999654852226126, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 564.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1467.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.2776957163958641, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.7223042836041359, \"precision\": 1.0, \"recall\": 0.2776957163958641, \"specificity\": 1.0, \"npv\": 0.4383614088820827, \"accuracy\": 0.5380982367758187, \"f1\": 0.4346820809248555, \"f2\": 0.324585635359116, \"f0_5\": 0.6578026592022393, \"p4\": 0.5074669616635656, \"phi\": 0.34889982155313615}, {\"truth_threshold\": 11.60000017285347, \"match_probability\": 0.9996779582968373, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 562.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1469.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.27671097981290005, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.7232890201871, \"precision\": 1.0, \"recall\": 0.27671097981290005, \"specificity\": 1.0, \"npv\": 0.43802601377199696, \"accuracy\": 0.5374685138539043, \"f1\": 0.433474739683764, \"f2\": 0.3235090950955561, \"f0_5\": 0.6566954896003739, \"p4\": 0.5065311855080239, \"phi\": 0.3481473932896642}, {\"truth_threshold\": 11.700000174343586, \"match_probability\": 0.9996995179863626, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 544.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1487.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.2678483505662235, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.7321516494337764, \"precision\": 1.0, \"recall\": 0.2678483505662235, \"specificity\": 1.0, \"npv\": 0.4350303951367781, \"accuracy\": 0.531801007556675, \"f1\": 0.4225242718446602, \"f2\": 0.3137978772496539, \"f0_5\": 0.6465414784882338, \"p4\": 0.49799892585801936, \"phi\": 0.3413534440774818}, {\"truth_threshold\": 11.800000175833702, \"match_probability\": 0.9997196347265854, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 532.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1499.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.2619399310684392, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.7380600689315608, \"precision\": 1.0, \"recall\": 0.2619399310684392, \"specificity\": 1.0, \"npv\": 0.43305597579425115, \"accuracy\": 0.5280226700251889, \"f1\": 0.4151385095591104, \"f2\": 0.30730129390018485, \"f0_5\": 0.6395768213512864, \"p4\": 0.49219626037015723, \"phi\": 0.33680061230395913}, {\"truth_threshold\": 11.900000177323818, \"match_probability\": 0.9997384050389891, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 531.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1500.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.2614475627769572, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.7385524372230429, \"precision\": 1.0, \"recall\": 0.2614475627769572, \"specificity\": 1.0, \"npv\": 0.43289224952741023, \"accuracy\": 0.5277078085642317, \"f1\": 0.41451990632318503, \"f2\": 0.30675909878682844, \"f0_5\": 0.6389891696750902, \"p4\": 0.4917084177453204, \"phi\": 0.3364203079244411}, {\"truth_threshold\": 12.000000178813934, \"match_probability\": 0.9997559189953416, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 525.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1506.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.258493353028065, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.741506646971935, \"precision\": 1.0, \"recall\": 0.258493353028065, \"specificity\": 1.0, \"npv\": 0.43191248585439457, \"accuracy\": 0.5258186397984886, \"f1\": 0.4107981220657277, \"f2\": 0.3035032951786334, \"f0_5\": 0.635439360929557, \"p4\": 0.4887671967996878, \"phi\": 0.33413546157687174}, {\"truth_threshold\": 12.10000018030405, \"match_probability\": 0.9997722606477963, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 523.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1508.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.2575086164451009, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.7424913835548991, \"precision\": 1.0, \"recall\": 0.2575086164451009, \"specificity\": 1.0, \"npv\": 0.43158688277421786, \"accuracy\": 0.5251889168765743, \"f1\": 0.4095536413469068, \"f2\": 0.3024170232450561, \"f0_5\": 0.6342469075915595, \"p4\": 0.4877813355532549, \"phi\": 0.33337267593347064}, {\"truth_threshold\": 12.200000181794167, \"match_probability\": 0.9997875084304283, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 522.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1509.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.2570162481536189, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.7429837518463811, \"precision\": 1.0, \"recall\": 0.2570162481536189, \"specificity\": 1.0, \"npv\": 0.43142426525998495, \"accuracy\": 0.5248740554156172, \"f1\": 0.408930669800235, \"f2\": 0.3018736988202637, \"f0_5\": 0.6336489439184269, \"p4\": 0.4872873712640354, \"phi\": 0.3329910599694127}, {\"truth_threshold\": 12.300000183284283, \"match_probability\": 0.9998017355340825, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 517.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1514.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.25455440669620877, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.7454455933037912, \"precision\": 1.0, \"recall\": 0.25455440669620877, \"specificity\": 1.0, \"npv\": 0.43061301241068073, \"accuracy\": 0.5232997481108312, \"f1\": 0.40580847723704866, \"f2\": 0.2991551903714848, \"f0_5\": 0.6306416199072945, \"p4\": 0.4848071139822707, \"phi\": 0.331080715067894}, {\"truth_threshold\": 12.400000184774399, \"match_probability\": 0.9998150102562988, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 503.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1528.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.24766125061546038, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.7523387493845396, \"precision\": 1.0, \"recall\": 0.24766125061546038, \"specificity\": 1.0, \"npv\": 0.4283576505798728, \"accuracy\": 0.5188916876574308, \"f1\": 0.39700078926598265, \"f2\": 0.2915266025269503, \"f0_5\": 0.6220628246351719, \"p4\": 0.4777676111660919, \"phi\": 0.3257109016494715}, {\"truth_threshold\": 12.500000186264515, \"match_probability\": 0.9998273963279586, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 488.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1543.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.24027572624322993, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.7597242737567701, \"precision\": 1.0, \"recall\": 0.24027572624322993, \"specificity\": 1.0, \"npv\": 0.4259672619047619, \"accuracy\": 0.514168765743073, \"f1\": 0.38745533942040494, \"f2\": 0.2833255921969345, \"f0_5\": 0.6126035651518955, \"p4\": 0.47006378241252605, \"phi\": 0.3199212296956968}, {\"truth_threshold\": 12.600000187754631, \"match_probability\": 0.9998389532181915, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 485.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1546.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.23879862136878385, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.7612013786312162, \"precision\": 1.0, \"recall\": 0.23879862136878385, \"specificity\": 1.0, \"npv\": 0.42549238201412115, \"accuracy\": 0.5132241813602015, \"f1\": 0.38553259141494434, \"f2\": 0.2816819607387618, \"f0_5\": 0.6106774112314278, \"p4\": 0.4685021766274086, \"phi\": 0.31875852024360396}, {\"truth_threshold\": 12.700000189244747, \"match_probability\": 0.9998497364189812, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 477.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1554.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.23485967503692762, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.7651403249630724, \"precision\": 1.0, \"recall\": 0.23485967503692762, \"specificity\": 1.0, \"npv\": 0.42423119673953313, \"accuracy\": 0.510705289672544, \"f1\": 0.3803827751196172, \"f2\": 0.27729333798395533, \"f0_5\": 0.6054836252856055, \"p4\": 0.46430283802256545, \"phi\": 0.31564980755066785}, {\"truth_threshold\": 12.800000190734863, \"match_probability\": 0.9998597977108138, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 457.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1574.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.22501230920728704, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.7749876907927129, \"precision\": 1.0, \"recall\": 0.22501230920728704, \"specificity\": 1.0, \"npv\": 0.42111070246414123, \"accuracy\": 0.5044080604534005, \"f1\": 0.36736334405144694, \"f2\": 0.2662859806549353, \"f0_5\": 0.592122311479658, \"p4\": 0.45357265449199585, \"phi\": 0.30782314986589165}, {\"truth_threshold\": 12.90000019222498, \"match_probability\": 0.9998691854106266, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 450.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1581.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.22156573116691286, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.7784342688330872, \"precision\": 1.0, \"recall\": 0.22156573116691286, \"specificity\": 1.0, \"npv\": 0.42002934702861333, \"accuracy\": 0.5022040302267002, \"f1\": 0.36275695284159615, \"f2\": 0.2624212736179146, \"f0_5\": 0.5873140172278778, \"p4\": 0.44973536314330326, \"phi\": 0.3050641070102409}, {\"truth_threshold\": 13.000000193715096, \"match_probability\": 0.9998779446032292, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 436.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1595.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.21467257508616444, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.7853274249138356, \"precision\": 1.0, \"recall\": 0.21467257508616444, \"specificity\": 1.0, \"npv\": 0.41788321167883213, \"accuracy\": 0.49779596977329976, \"f1\": 0.3534657478719092, \"f2\": 0.2546728971962617, \"f0_5\": 0.5774834437086093, \"p4\": 0.4419269349297068, \"phi\": 0.2995130466880727}, {\"truth_threshold\": 13.100000195205212, \"match_probability\": 0.9998861173572945, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 430.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1601.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.21171836533727229, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.7882816346627277, \"precision\": 1.0, \"recall\": 0.21171836533727229, \"specificity\": 1.0, \"npv\": 0.4169701383831027, \"accuracy\": 0.4959068010075567, \"f1\": 0.34945144250304755, \"f2\": 0.2513444002805705, \"f0_5\": 0.5731804852039456, \"p4\": 0.4385239285455831, \"phi\": 0.2971199018795724}, {\"truth_threshold\": 13.300000198185444, \"match_probability\": 0.9999008579398913, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 420.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1611.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.206794682422452, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.793205317577548, \"precision\": 1.0, \"recall\": 0.206794682422452, \"specificity\": 1.0, \"npv\": 0.41545718432510886, \"accuracy\": 0.4927581863979849, \"f1\": 0.3427172582619339, \"f2\": 0.24578651685393257, \"f0_5\": 0.5658852061438965, \"p4\": 0.43277391747463057, \"phi\": 0.29311147451547676}, {\"truth_threshold\": 13.500000201165676, \"match_probability\": 0.9999136907162209, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 416.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1615.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.2048252092565239, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.7951747907434761, \"precision\": 1.0, \"recall\": 0.2048252092565239, \"specificity\": 1.0, \"npv\": 0.4148550724637681, \"accuracy\": 0.49149874055415615, \"f1\": 0.340008173273396, \"f2\": 0.24355971896955503, \"f0_5\": 0.5629228687415426, \"p4\": 0.43044577914486043, \"phi\": 0.29150090399263207}, {\"truth_threshold\": 13.600000202655792, \"match_probability\": 0.9999194701253888, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 414.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1617.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.2038404726735598, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.7961595273264401, \"precision\": 1.0, \"recall\": 0.2038404726735598, \"specificity\": 1.0, \"npv\": 0.41455467052860245, \"accuracy\": 0.49086901763224183, \"f1\": 0.33865030674846625, \"f2\": 0.24244553759662685, \"f0_5\": 0.5614320585842149, \"p4\": 0.42927555505144527, \"phi\": 0.29069403156855866}, {\"truth_threshold\": 13.700000204145908, \"match_probability\": 0.9999248625650565, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 411.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1620.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.20236336779911374, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.7976366322008862, \"precision\": 1.0, \"recall\": 0.20236336779911374, \"specificity\": 1.0, \"npv\": 0.4141048824593128, \"accuracy\": 0.4899244332493703, \"f1\": 0.3366093366093366, \"f2\": 0.24077328646748683, \"f0_5\": 0.5591836734693878, \"p4\": 0.42751243442120324, \"phi\": 0.2894817069117195}, {\"truth_threshold\": 13.800000205636024, \"match_probability\": 0.999929893941616, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 404.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1627.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.19891678975873953, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8010832102412605, \"precision\": 1.0, \"recall\": 0.19891678975873953, \"specificity\": 1.0, \"npv\": 0.41305916305916307, \"accuracy\": 0.48772040302267, \"f1\": 0.33182751540041067, \"f2\": 0.23686679174484052, \"f0_5\": 0.5538799012887304, \"p4\": 0.4233615823022448, \"phi\": 0.28664333708663187}, {\"truth_threshold\": 13.90000020712614, \"match_probability\": 0.9999345884275949, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 399.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1632.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.1964549483013294, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8035450516986706, \"precision\": 1.0, \"recall\": 0.1964549483013294, \"specificity\": 1.0, \"npv\": 0.41231544832553113, \"accuracy\": 0.48614609571788414, \"f1\": 0.32839506172839505, \"f2\": 0.23407250967969026, \"f0_5\": 0.5500413564929694, \"p4\": 0.42036442533034846, \"phi\": 0.28460746667055603}, {\"truth_threshold\": 14.000000208616257, \"match_probability\": 0.9999389685776376, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 386.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1645.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.19005416051206303, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.809945839487937, \"precision\": 1.0, \"recall\": 0.19005416051206303, \"specificity\": 1.0, \"npv\": 0.4103942652329749, \"accuracy\": 0.4820528967254408, \"f1\": 0.31940422010757136, \"f2\": 0.22679200940070504, \"f0_5\": 0.5398601398601398, \"p4\": 0.4124417485637631, \"phi\": 0.2792796762348059}, {\"truth_threshold\": 14.100000210106373, \"match_probability\": 0.9999430554367367, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 371.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1660.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.18266863613983259, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8173313638601674, \"precision\": 1.0, \"recall\": 0.18266863613983259, \"specificity\": 1.0, \"npv\": 0.40819964349376114, \"accuracy\": 0.47732997481108314, \"f1\": 0.3089092422980849, \"f2\": 0.2183637433784579, \"f0_5\": 0.5277382645803699, \"p4\": 0.4030561657028185, \"phi\": 0.27306642442777773}, {\"truth_threshold\": 14.200000211596489, \"match_probability\": 0.9999468686412301, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 368.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1663.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.1811915312653865, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8188084687346134, \"precision\": 1.0, \"recall\": 0.1811915312653865, \"specificity\": 1.0, \"npv\": 0.4077635327635328, \"accuracy\": 0.4763853904282116, \"f1\": 0.3067944977073781, \"f2\": 0.2166745171926519, \"f0_5\": 0.5252640593776763, \"p4\": 0.40114633639560937, \"phi\": 0.2718148246428221}, {\"truth_threshold\": 14.300000213086605, \"match_probability\": 0.9999504265130488, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 361.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1670.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.1777449532250123, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8222550467749877, \"precision\": 1.0, \"recall\": 0.1777449532250123, \"specificity\": 1.0, \"npv\": 0.4067495559502664, \"accuracy\": 0.47418136020151136, \"f1\": 0.30183946488294316, \"f2\": 0.21272834413671185, \"f0_5\": 0.5194244604316547, \"p4\": 0.396646195182804, \"phi\": 0.26888228055540336}, {\"truth_threshold\": 14.400000214576721, \"match_probability\": 0.9999537461476637, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 360.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1671.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.17725258493353027, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8227474150664698, \"precision\": 1.0, \"recall\": 0.17725258493353027, \"specificity\": 1.0, \"npv\": 0.40660511363636365, \"accuracy\": 0.47386649874055414, \"f1\": 0.30112923462986196, \"f2\": 0.21216407355021216, \"f0_5\": 0.5185825410544511, \"p4\": 0.3959982275188506, \"phi\": 0.268461929217603}, {\"truth_threshold\": 14.500000216066837, \"match_probability\": 0.9999568434961527, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 352.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1679.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.17331363860167406, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.826686361398326, \"precision\": 1.0, \"recall\": 0.17331363860167406, \"specificity\": 1.0, \"npv\": 0.4054532577903683, \"accuracy\": 0.47134760705289674, \"f1\": 0.2954259336970206, \"f2\": 0.20764511562057575, \"f0_5\": 0.5117766792672288, \"p4\": 0.3907676489102373, \"phi\": 0.26508598490027957}, {\"truth_threshold\": 14.600000217556953, \"match_probability\": 0.9999597334417798, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 350.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1681.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.17232890201870998, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.82767109798129, \"precision\": 1.0, \"recall\": 0.17232890201870998, \"specificity\": 1.0, \"npv\": 0.4051663128096249, \"accuracy\": 0.47071788413098237, \"f1\": 0.2939941201175976, \"f2\": 0.20651404295492093, \"f0_5\": 0.5100553774409793, \"p4\": 0.389446793623568, \"phi\": 0.26423827470949746}, {\"truth_threshold\": 14.900000222027302, \"match_probability\": 0.9999672931444318, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 347.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1684.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.1708517971442639, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8291482028557361, \"precision\": 1.0, \"recall\": 0.1708517971442639, \"specificity\": 1.0, \"npv\": 0.4047366560622128, \"accuracy\": 0.46977329974811083, \"f1\": 0.29184188393608074, \"f2\": 0.20481643253452958, \"f0_5\": 0.5074583211465341, \"p4\": 0.38745542187943405, \"phi\": 0.2629638474360856}, {\"truth_threshold\": 15.000000223517418, \"match_probability\": 0.9999694833578969, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 346.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1685.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.17035942885278188, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8296405711472181, \"precision\": 1.0, \"recall\": 0.17035942885278188, \"specificity\": 1.0, \"npv\": 0.4045936395759717, \"accuracy\": 0.46945843828715367, \"f1\": 0.29112326461926796, \"f2\": 0.20425029515938606, \"f0_5\": 0.5065885797950219, \"p4\": 0.3867889182734259, \"phi\": 0.262538266459636}, {\"truth_threshold\": 15.100000225007534, \"match_probability\": 0.9999715269079685, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 345.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1686.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.16986706056129985, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8301329394387001, \"precision\": 1.0, \"recall\": 0.16986706056129985, \"specificity\": 1.0, \"npv\": 0.4044507241257506, \"accuracy\": 0.46914357682619645, \"f1\": 0.2904040404040404, \"f2\": 0.2036840240878498, \"f0_5\": 0.5057167985927881, \"p4\": 0.3861210486188493, \"phi\": 0.26211229587550916}, {\"truth_threshold\": 15.20000022649765, \"match_probability\": 0.9999734336151354, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 341.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1690.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.16789758739537175, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8321024126046282, \"precision\": 1.0, \"recall\": 0.16789758739537175, \"specificity\": 1.0, \"npv\": 0.4038800705467372, \"accuracy\": 0.46788413098236775, \"f1\": 0.2875210792580101, \"f2\": 0.20141760189013586, \"f0_5\": 0.5022091310751104, \"p4\": 0.38343579369132264, \"phi\": 0.2604044727762365}, {\"truth_threshold\": 15.300000227987766, \"match_probability\": 0.9999752126423825, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 336.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1695.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.1654357459379616, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8345642540620384, \"precision\": 1.0, \"recall\": 0.1654357459379616, \"specificity\": 1.0, \"npv\": 0.40316901408450706, \"accuracy\": 0.4663098236775819, \"f1\": 0.28390367553865653, \"f2\": 0.19858156028368795, \"f0_5\": 0.49777777777777776, \"p4\": 0.38004778751227103, \"phi\": 0.2582606562838075}, {\"truth_threshold\": 15.400000229477882, \"match_probability\": 0.9999768725392036, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 335.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1696.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.16494337764647957, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8350566223535204, \"precision\": 1.0, \"recall\": 0.16494337764647957, \"specificity\": 1.0, \"npv\": 0.40302710313269974, \"accuracy\": 0.4659949622166247, \"f1\": 0.28317836010143704, \"f2\": 0.19801394963943728, \"f0_5\": 0.4968851972708395, \"p4\": 0.3793659349517108, \"phi\": 0.2578306647274206}, {\"truth_threshold\": 15.500000230967999, \"match_probability\": 0.9999784212826682, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 326.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1705.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.1605120630231413, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8394879369768586, \"precision\": 1.0, \"recall\": 0.1605120630231413, \"specificity\": 1.0, \"npv\": 0.4017543859649123, \"accuracy\": 0.46316120906801006, \"f1\": 0.2766228256257955, \"f2\": 0.19289940828402366, \"f0_5\": 0.48875562218890556, \"p4\": 0.37316411468245886, \"phi\": 0.2539417754522155}, {\"truth_threshold\": 15.600000232458115, \"match_probability\": 0.9999798663157408, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 321.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1710.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.15805022156573117, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8419497784342689, \"precision\": 1.0, \"recall\": 0.15805022156573117, \"specificity\": 1.0, \"npv\": 0.4010507880910683, \"accuracy\": 0.4615869017632242, \"f1\": 0.2729591836734694, \"f2\": 0.19005328596802842, \"f0_5\": 0.4841628959276018, \"p4\": 0.369666887936757, \"phi\": 0.2517660936601759}, {\"truth_threshold\": 15.70000023394823, \"match_probability\": 0.9999812145830361, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 319.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1712.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.15706548498276712, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8429345150172329, \"precision\": 1.0, \"recall\": 0.15706548498276712, \"specificity\": 1.0, \"npv\": 0.4007700385019251, \"accuracy\": 0.4609571788413098, \"f1\": 0.2714893617021277, \"f2\": 0.1889138931659363, \"f0_5\": 0.4823102509827638, \"p4\": 0.3682574026034257, \"phi\": 0.2508926871470492}, {\"truth_threshold\": 15.800000235438347, \"match_probability\": 0.9999824725641815, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 316.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1715.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.15558838010832102, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.844411619891679, \"precision\": 1.0, \"recall\": 0.15558838010832102, \"specificity\": 1.0, \"npv\": 0.40034965034965037, \"accuracy\": 0.4600125944584383, \"f1\": 0.26927993182786536, \"f2\": 0.1872037914691943, \"f0_5\": 0.4795144157814871, \"p4\": 0.366131657936778, \"phi\": 0.2495791529251488}, {\"truth_threshold\": 15.900000236928463, \"match_probability\": 0.9999836463049459, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 313.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1718.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.15411127523387494, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8458887247661251, \"precision\": 1.0, \"recall\": 0.15411127523387494, \"specificity\": 1.0, \"npv\": 0.39993014320642684, \"accuracy\": 0.45906801007556675, \"f1\": 0.26706484641638223, \"f2\": 0.18549247362806684, \"f0_5\": 0.4766981419433445, \"p4\": 0.3639919317161557, \"phi\": 0.24826144359124447}, {\"truth_threshold\": 16.00000023841858, \"match_probability\": 0.9999847414462861, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 311.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1720.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.1531265386509109, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8468734613490891, \"precision\": 1.0, \"recall\": 0.1531265386509109, \"specificity\": 1.0, \"npv\": 0.39965095986038396, \"accuracy\": 0.45843828715365237, \"f1\": 0.2655849701110162, \"f2\": 0.18435091879075283, \"f0_5\": 0.47480916030534354, \"p4\": 0.3625575890243592, \"phi\": 0.2473806139371772}, {\"truth_threshold\": 16.100000239908695, \"match_probability\": 0.9999857632514492, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 307.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1724.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.15115706548498276, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8488429345150172, \"precision\": 1.0, \"recall\": 0.15115706548498276, \"specificity\": 1.0, \"npv\": 0.399093760892297, \"accuracy\": 0.45717884130982367, \"f1\": 0.262617621899059, \"f2\": 0.18206618431977226, \"f0_5\": 0.4710033752684873, \"p4\": 0.35966979322171594, \"phi\": 0.24561319539032303}, {\"truth_threshold\": 16.20000024139881, \"match_probability\": 0.9999867166312594, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 303.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1728.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.14918759231905465, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8508124076809453, \"precision\": 1.0, \"recall\": 0.14918759231905465, \"specificity\": 1.0, \"npv\": 0.39853811347024015, \"accuracy\": 0.45591939546599497, \"f1\": 0.2596401028277635, \"f2\": 0.17977928088287648, \"f0_5\": 0.4671600370027752, \"p4\": 0.3567561397717773, \"phi\": 0.24383794125607963}, {\"truth_threshold\": 16.300000242888927, \"match_probability\": 0.9999876061677141, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 300.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1731.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.14771048744460857, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8522895125553914, \"precision\": 1.0, \"recall\": 0.14771048744460857, \"specificity\": 1.0, \"npv\": 0.3981223922114047, \"accuracy\": 0.45497481108312343, \"f1\": 0.2574002574002574, \"f2\": 0.17806267806267806, \"f0_5\": 0.46425255338904364, \"p4\": 0.3545536533347784, \"phi\": 0.24250124250436372}, {\"truth_threshold\": 16.400000244379044, \"match_probability\": 0.9999884361359999, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 296.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1735.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.14574101427868044, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8542589857213195, \"precision\": 1.0, \"recall\": 0.14574101427868044, \"specificity\": 1.0, \"npv\": 0.3975694444444444, \"accuracy\": 0.45371536523929473, \"f1\": 0.25440481306403095, \"f2\": 0.17577197149643706, \"f0_5\": 0.4603421461897356, \"p4\": 0.35159363195589, \"phi\": 0.240711807104564}, {\"truth_threshold\": 16.50000024586916, \"match_probability\": 0.9999892105250341, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 295.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1736.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.14524864598719842, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8547513540128016, \"precision\": 1.0, \"recall\": 0.14524864598719842, \"specificity\": 1.0, \"npv\": 0.3974314474140923, \"accuracy\": 0.4534005037783375, \"f1\": 0.25365434221840066, \"f2\": 0.17519895474521915, \"f0_5\": 0.4593584553098723, \"p4\": 0.3508494030029032, \"phi\": 0.2402631465906275}, {\"truth_threshold\": 16.600000247359276, \"match_probability\": 0.9999899330566321, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 289.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1742.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.14229443623830626, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8577055637616937, \"precision\": 1.0, \"recall\": 0.14229443623830626, \"specificity\": 1.0, \"npv\": 0.39660547280914443, \"accuracy\": 0.45151133501259444, \"f1\": 0.24913793103448276, \"f2\": 0.17175799358136218, \"f0_5\": 0.45340445560087855, \"p4\": 0.346347962973042, \"phi\": 0.2375599969742467}, {\"truth_threshold\": 16.700000248849392, \"match_probability\": 0.9999906072033913, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 286.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1745.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.14081733136386015, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8591826686361398, \"precision\": 1.0, \"recall\": 0.14081733136386015, \"specificity\": 1.0, \"npv\": 0.3961937716262976, \"accuracy\": 0.4505667506297229, \"f1\": 0.2468709538195943, \"f2\": 0.1700356718192628, \"f0_5\": 0.4503937007874016, \"p4\": 0.34407370681446553, \"phi\": 0.236201078793891}, {\"truth_threshold\": 16.800000250339508, \"match_probability\": 0.9999912362053778, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 284.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1747.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.1398325947808961, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8601674052191038, \"precision\": 1.0, \"recall\": 0.1398325947808961, \"specificity\": 1.0, \"npv\": 0.39591977869986167, \"accuracy\": 0.4499370277078086, \"f1\": 0.24535637149028078, \"f2\": 0.16888677450047573, \"f0_5\": 0.4483738553836438, \"p4\": 0.34254867358249524, \"phi\": 0.23529235002583448}, {\"truth_threshold\": 17.00000025331974, \"match_probability\": 0.9999923706650156, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 283.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1748.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.13934022648941408, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8606597735105859, \"precision\": 1.0, \"recall\": 0.13934022648941408, \"specificity\": 1.0, \"npv\": 0.3957829243000346, \"accuracy\": 0.44962216624685136, \"f1\": 0.2445980985306828, \"f2\": 0.16831212085167122, \"f0_5\": 0.44736010116977554, \"p4\": 0.3417834740225152, \"phi\": 0.23483713997706887}, {\"truth_threshold\": 17.100000254809856, \"match_probability\": 0.9999928815751264, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 278.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1753.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.13687838503200395, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8631216149679961, \"precision\": 1.0, \"recall\": 0.13687838503200395, \"specificity\": 1.0, \"npv\": 0.3951000690131125, \"accuracy\": 0.4480478589420655, \"f1\": 0.2407968817669987, \"f2\": 0.1654368007617234, \"f0_5\": 0.44225262488068723, \"p4\": 0.3379303187916212, \"phi\": 0.23255248734973388}, {\"truth_threshold\": 17.200000256299973, \"match_probability\": 0.999993358271586, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 276.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1755.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.1358936484490399, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8641063515509602, \"precision\": 1.0, \"recall\": 0.1358936484490399, \"specificity\": 1.0, \"npv\": 0.39482758620689656, \"accuracy\": 0.4474181360201511, \"f1\": 0.23927178153446033, \"f2\": 0.16428571428571428, \"f0_5\": 0.44019138755980863, \"p4\": 0.33637622790175986, \"phi\": 0.23163454232472105}, {\"truth_threshold\": 17.30000025779009, \"match_probability\": 0.999993803045519, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 271.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1760.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.13343180699162974, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8665681930083703, \"precision\": 1.0, \"recall\": 0.13343180699162974, \"specificity\": 1.0, \"npv\": 0.39414802065404475, \"accuracy\": 0.44584382871536526, \"f1\": 0.23544743701129453, \"f2\": 0.16140559857057774, \"f0_5\": 0.434991974317817, \"p4\": 0.33245833489583054, \"phi\": 0.22932920140715485}, {\"truth_threshold\": 17.400000259280205, \"match_probability\": 0.9999942180346287, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 268.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1763.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.13195470211718366, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8680452978828164, \"precision\": 1.0, \"recall\": 0.13195470211718366, \"specificity\": 1.0, \"npv\": 0.3937414030261348, \"accuracy\": 0.4448992443324937, \"f1\": 0.23314484558503698, \"f2\": 0.15967588179218303, \"f0_5\": 0.43184015468901066, \"p4\": 0.3300848527615133, \"phi\": 0.22793865303523134}, {\"truth_threshold\": 17.50000026077032, \"match_probability\": 0.9999946052334694, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 262.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1769.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.12900049236829148, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8709995076317085, \"precision\": 1.0, \"recall\": 0.12900049236829148, \"specificity\": 1.0, \"npv\": 0.39293067947838023, \"accuracy\": 0.44301007556675065, \"f1\": 0.2285215874400349, \"f2\": 0.1562127355115669, \"f0_5\": 0.425462812601494, \"p4\": 0.32528558993728013, \"phi\": 0.22514051416686065}, {\"truth_threshold\": 17.600000262260437, \"match_probability\": 0.999994966503032, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 256.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1775.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.12604628261939932, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8739537173806007, \"precision\": 1.0, \"recall\": 0.12604628261939932, \"specificity\": 1.0, \"npv\": 0.3921232876712329, \"accuracy\": 0.4411209068010076, \"f1\": 0.22387407083515523, \"f2\": 0.15274463007159905, \"f0_5\": 0.41898527004909986, \"p4\": 0.3204149478514069, \"phi\": 0.22231887625538288}, {\"truth_threshold\": 17.700000263750553, \"match_probability\": 0.9999953035796879, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 250.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1781.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.12309207287050714, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8769079271294928, \"precision\": 1.0, \"recall\": 0.12309207287050714, \"specificity\": 1.0, \"npv\": 0.3913192071086808, \"accuracy\": 0.4392317380352645, \"f1\": 0.21920210434020165, \"f2\": 0.14927155481251492, \"f0_5\": 0.41240514681623225, \"p4\": 0.3154708850680329, \"phi\": 0.219472759943121}, {\"truth_threshold\": 17.80000026524067, \"match_probability\": 0.9999956180835331, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 246.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1785.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.12112259970457903, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8788774002954209, \"precision\": 1.0, \"recall\": 0.12112259970457903, \"specificity\": 1.0, \"npv\": 0.39078498293515357, \"accuracy\": 0.43797229219143574, \"f1\": 0.2160737812911726, \"f2\": 0.14695340501792115, \"f0_5\": 0.4079601990049751, \"p4\": 0.31213301141534483, \"phi\": 0.2175612397892036}, {\"truth_threshold\": 17.900000266730785, \"match_probability\": 0.9999959115261747, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 242.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1789.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.11915312653865091, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.880846873461349, \"precision\": 1.0, \"recall\": 0.11915312653865091, \"specificity\": 1.0, \"npv\": 0.39025221540558963, \"accuracy\": 0.43671284634760704, \"f1\": 0.21293444786625604, \"f2\": 0.14463303848912265, \"f0_5\": 0.40346782260753583, \"p4\": 0.30876091977525716, \"phi\": 0.21563805694777322}, {\"truth_threshold\": 18.0000002682209, \"match_probability\": 0.9999961853179954, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 239.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1792.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.11767602166420482, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8823239783357951, \"precision\": 1.0, \"recall\": 0.11767602166420482, \"specificity\": 1.0, \"npv\": 0.38985359210078313, \"accuracy\": 0.4357682619647355, \"f1\": 0.2105726872246696, \"f2\": 0.14289130694726773, \"f0_5\": 0.4000669568128557, \"p4\": 0.3062089971097263, \"phi\": 0.21418781419567226}, {\"truth_threshold\": 18.200000271201134, \"match_probability\": 0.9999966791247992, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 233.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1798.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.11472181191531265, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8852781880846874, \"precision\": 1.0, \"recall\": 0.11472181191531265, \"specificity\": 1.0, \"npv\": 0.3890587835541964, \"accuracy\": 0.4338790931989924, \"f1\": 0.2058303886925795, \"f2\": 0.13940409237764748, \"f0_5\": 0.3931825852176848, \"p4\": 0.30104514125543336, \"phi\": 0.21126648714574883}, {\"truth_threshold\": 18.30000027269125, \"match_probability\": 0.999996901513191, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 232.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1799.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.11422944362383063, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8857705563761694, \"precision\": 1.0, \"recall\": 0.11422944362383063, \"specificity\": 1.0, \"npv\": 0.3889266304347826, \"accuracy\": 0.43356423173803527, \"f1\": 0.20503756076005303, \"f2\": 0.13882240306366683, \"f0_5\": 0.39202433254477864, \"p4\": 0.3001765927459395, \"phi\": 0.21077683128146796}, {\"truth_threshold\": 18.400000274181366, \"match_probability\": 0.9999971090089864, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 229.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1802.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.11275233874938453, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8872476612506155, \"precision\": 1.0, \"recall\": 0.11275233874938453, \"specificity\": 1.0, \"npv\": 0.38853070919579236, \"accuracy\": 0.43261964735516373, \"f1\": 0.20265486725663717, \"f2\": 0.1370764994612714, \"f0_5\": 0.38853070919579236, \"p4\": 0.29755717255717257, \"phi\": 0.20930300078542255}, {\"truth_threshold\": 18.500000275671482, \"match_probability\": 0.9999973026094866, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 224.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1807.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.11029049729197439, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8897095027080256, \"precision\": 1.0, \"recall\": 0.11029049729197439, \"specificity\": 1.0, \"npv\": 0.38787262872628725, \"accuracy\": 0.4310453400503778, \"f1\": 0.19866962305986696, \"f2\": 0.13416387158600862, \"f0_5\": 0.38264434574649814, \"p4\": 0.29314487543657275, \"phi\": 0.20683003918233825}, {\"truth_threshold\": 18.600000277161598, \"match_probability\": 0.999997483245208, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 221.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1810.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.10881339241752831, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8911866075824717, \"precision\": 1.0, \"recall\": 0.10881339241752831, \"specificity\": 1.0, \"npv\": 0.38747884940778343, \"accuracy\": 0.4301007556675063, \"f1\": 0.19626998223801065, \"f2\": 0.1324146195326543, \"f0_5\": 0.379073756432247, \"p4\": 0.290469029799348, \"phi\": 0.20533603700788008}, {\"truth_threshold\": 18.700000278651714, \"match_probability\": 0.9999976517843541, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 218.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1813.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.10733628754308222, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8926637124569178, \"precision\": 1.0, \"recall\": 0.10733628754308222, \"specificity\": 1.0, \"npv\": 0.3870858688302907, \"accuracy\": 0.42915617128463474, \"f1\": 0.19386393952867942, \"f2\": 0.13066410932630065, \"f0_5\": 0.3754736479503961, \"p4\": 0.28777144299895174, \"phi\": 0.2038341485635611}, {\"truth_threshold\": 18.80000028014183, \"match_probability\": 0.9999978090369889, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 212.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1819.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.10438207779419005, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8956179222058099, \"precision\": 1.0, \"recall\": 0.10438207779419005, \"specificity\": 1.0, \"npv\": 0.38630229419703105, \"accuracy\": 0.4272670025188917, \"f1\": 0.18903254569772626, \"f2\": 0.12715930902111325, \"f0_5\": 0.3681833970128517, \"p4\": 0.2823097318295965, \"phi\": 0.20080596635794612}, {\"truth_threshold\": 18.900000281631947, \"match_probability\": 0.9999979557589296, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 209.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1822.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.10290497291974397, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.897095027080256, \"precision\": 1.0, \"recall\": 0.10290497291974397, \"specificity\": 1.0, \"npv\": 0.38591169531513314, \"accuracy\": 0.4263224181360202, \"f1\": 0.18660714285714286, \"f2\": 0.12540501620064803, \"f0_5\": 0.36449250087199164, \"p4\": 0.27954493418624105, \"phi\": 0.19927928280635765}, {\"truth_threshold\": 19.000000283122063, \"match_probability\": 0.9999980926553794, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 201.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1830.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.09896602658788774, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9010339734121122, \"precision\": 1.0, \"recall\": 0.09896602658788774, \"specificity\": 1.0, \"npv\": 0.38487394957983195, \"accuracy\": 0.4238035264483627, \"f1\": 0.18010752688172044, \"f2\": 0.12072072072072072, \"f0_5\": 0.3544973544973545, \"p4\": 0.2720583020072346, \"phi\": 0.19516517498545435}, {\"truth_threshold\": 19.10000028461218, \"match_probability\": 0.9999982203843173, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 194.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1837.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.09551944854751354, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9044805514524865, \"precision\": 1.0, \"recall\": 0.09551944854751354, \"specificity\": 1.0, \"npv\": 0.3839704896042924, \"accuracy\": 0.4215994962216625, \"f1\": 0.17438202247191012, \"f2\": 0.11661457081029093, \"f0_5\": 0.3455646597791236, \"p4\": 0.2653674457472427, \"phi\": 0.19151148640622262}, {\"truth_threshold\": 19.200000286102295, \"match_probability\": 0.9999983395596597, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 176.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1855.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.08665681930083703, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.913343180699163, \"precision\": 1.0, \"recall\": 0.08665681930083703, \"specificity\": 1.0, \"npv\": 0.38166666666666665, \"accuracy\": 0.41593198992443325, \"f1\": 0.15949252378794743, \"f2\": 0.10602409638554217, \"f0_5\": 0.3217550274223035, \"p4\": 0.24752689591851462, \"phi\": 0.18186263873178055}, {\"truth_threshold\": 19.30000028759241, \"match_probability\": 0.9999984507542113, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 165.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1866.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.08124076809453472, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9187592319054653, \"precision\": 1.0, \"recall\": 0.08124076809453472, \"specificity\": 1.0, \"npv\": 0.38027233477250083, \"accuracy\": 0.41246851385390426, \"f1\": 0.15027322404371585, \"f2\": 0.09952949692363373, \"f0_5\": 0.30657748049052397, \"p4\": 0.23614444277786112, \"phi\": 0.1757658003196868}, {\"truth_threshold\": 19.400000289082527, \"match_probability\": 0.9999985545024187, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 150.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1881.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.07385524372230429, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9261447562776958, \"precision\": 1.0, \"recall\": 0.07385524372230429, \"specificity\": 1.0, \"npv\": 0.37838730998017184, \"accuracy\": 0.4077455919395466, \"f1\": 0.1375515818431912, \"f2\": 0.09064539521392313, \"f0_5\": 0.28506271379703535, \"p4\": 0.21998818404076986, \"phi\": 0.16717023359441924}, {\"truth_threshold\": 19.500000290572643, \"match_probability\": 0.9999986513029383, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 148.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1883.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.07287050713934022, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9271294928606598, \"precision\": 1.0, \"recall\": 0.07287050713934022, \"specificity\": 1.0, \"npv\": 0.3781373844121532, \"accuracy\": 0.40711586901763225, \"f1\": 0.13584212941716384, \"f2\": 0.08945841392649903, \"f0_5\": 0.2821197102554327, \"p4\": 0.21777579155929253, \"phi\": 0.16599717759786536}, {\"truth_threshold\": 19.60000029206276, \"match_probability\": 0.9999987416210334, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 144.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1887.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.07090103397341212, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9290989660265879, \"precision\": 1.0, \"recall\": 0.07090103397341212, \"specificity\": 1.0, \"npv\": 0.3776385224274406, \"accuracy\": 0.40585642317380355, \"f1\": 0.13241379310344828, \"f2\": 0.08708272859216255, \"f0_5\": 0.2761795166858458, \"p4\": 0.21330828694544357, \"phi\": 0.16363056471300563}, {\"truth_threshold\": 19.80000029504299, \"match_probability\": 0.9999989045173057, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 143.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1888.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.07040866568193008, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.92959133431807, \"precision\": 1.0, \"recall\": 0.07040866568193008, \"specificity\": 1.0, \"npv\": 0.3775140125288493, \"accuracy\": 0.40554156171284633, \"f1\": 0.13155473781048757, \"f2\": 0.08648844804644974, \"f0_5\": 0.2746830580099885, \"p4\": 0.2121823937921731, \"phi\": 0.16303452977325913}, {\"truth_threshold\": 19.900000296533108, \"match_probability\": 0.9999989778784306, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 142.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1889.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.06991629739044805, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.930083702609552, \"precision\": 1.0, \"recall\": 0.06991629739044805, \"specificity\": 1.0, \"npv\": 0.3773895847066579, \"accuracy\": 0.40522670025188917, \"f1\": 0.13069489185457892, \"f2\": 0.08589402371158965, \"f0_5\": 0.27318199307425933, \"p4\": 0.21105285310447533, \"phi\": 0.16243670286117107}, {\"truth_threshold\": 20.000000298023224, \"match_probability\": 0.99999904632679, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 141.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1890.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.06942392909896603, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.930576070901034, \"precision\": 1.0, \"recall\": 0.06942392909896603, \"specificity\": 1.0, \"npv\": 0.3772652388797364, \"accuracy\": 0.404911838790932, \"f1\": 0.1298342541436464, \"f2\": 0.0852994555353902, \"f0_5\": 0.27167630057803466, \"p4\": 0.20991964425027304, \"phi\": 0.16183706372611714}, {\"truth_threshold\": 20.10000029951334, \"match_probability\": 0.9999991101913761, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 136.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1895.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.06696208764155588, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9330379123584441, \"precision\": 1.0, \"recall\": 0.06696208764155588, \"specificity\": 1.0, \"npv\": 0.37664473684210525, \"accuracy\": 0.4033375314861461, \"f1\": 0.12551915089986157, \"f2\": 0.08232445520581114, \"f0_5\": 0.26407766990291265, \"p4\": 0.2041978445273122, \"phi\": 0.158810950120424}, {\"truth_threshold\": 20.200000301003456, \"match_probability\": 0.9999991697791492, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 133.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1898.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.0654849827671098, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9345150172328902, \"precision\": 1.0, \"recall\": 0.0654849827671098, \"specificity\": 1.0, \"npv\": 0.37627341439369044, \"accuracy\": 0.40239294710327456, \"f1\": 0.12292051756007394, \"f2\": 0.08053772556618627, \"f0_5\": 0.259461568474444, \"p4\": 0.20071939222033594, \"phi\": 0.1569721569492258}, {\"truth_threshold\": 20.300000302493572, \"match_probability\": 0.9999992253765136, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 116.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1915.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.05711472181191531, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9428852781880847, \"precision\": 1.0, \"recall\": 0.05711472181191531, \"specificity\": 1.0, \"npv\": 0.3741830065359477, \"accuracy\": 0.39704030226700254, \"f1\": 0.10805775500698649, \"f2\": 0.0703883495145631, \"f0_5\": 0.23246492985971945, \"p4\": 0.18033362807377223, \"phi\": 0.1461894603760707}, {\"truth_threshold\": 20.40000030398369, \"match_probability\": 0.9999992772506945, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 102.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1929.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.050221565731166914, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9497784342688331, \"precision\": 1.0, \"recall\": 0.050221565731166914, \"specificity\": 1.0, \"npv\": 0.37247885491216653, \"accuracy\": 0.392632241813602, \"f1\": 0.09563994374120956, \"f2\": 0.06199854121079504, \"f0_5\": 0.2091020910209102, \"p4\": 0.16262489021357832, \"phi\": 0.13677160266459246}, {\"truth_threshold\": 20.500000305473804, \"match_probability\": 0.9999993256510213, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 88.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1943.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.043328409650418516, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9566715903495815, \"precision\": 1.0, \"recall\": 0.043328409650418516, \"specificity\": 1.0, \"npv\": 0.3707901554404145, \"accuracy\": 0.38822418136020154, \"f1\": 0.0830580462482303, \"f2\": 0.0535801266439357, \"f0_5\": 0.18464120856063784, \"p4\": 0.1440066829619842, \"phi\": 0.12675073076422336}, {\"truth_threshold\": 21.000000312924385, \"match_probability\": 0.9999995231631726, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 87.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1944.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.04283604135893648, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9571639586410635, \"precision\": 1.0, \"recall\": 0.04283604135893648, \"specificity\": 1.0, \"npv\": 0.370670119779864, \"accuracy\": 0.3879093198992443, \"f1\": 0.0821529745042493, \"f2\": 0.052977712824260136, \"f0_5\": 0.1828499369482976, \"p4\": 0.1426399013699101, \"phi\": 0.12600809728510384}, {\"truth_threshold\": 21.1000003144145, \"match_probability\": 0.9999995550954947, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 84.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1947.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.0413589364844904, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9586410635155096, \"precision\": 1.0, \"recall\": 0.0413589364844904, \"specificity\": 1.0, \"npv\": 0.3703104786545925, \"accuracy\": 0.3869647355163728, \"f1\": 0.07943262411347518, \"f2\": 0.05116959064327485, \"f0_5\": 0.17743979721166034, \"p4\": 0.13850891224492662, \"phi\": 0.12375640414223632}, {\"truth_threshold\": 21.200000315904617, \"match_probability\": 0.9999995848894065, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 82.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1949.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.04037419990152634, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9596258000984736, \"precision\": 1.0, \"recall\": 0.04037419990152634, \"specificity\": 1.0, \"npv\": 0.37007110536522303, \"accuracy\": 0.38633501259445846, \"f1\": 0.0776147657359205, \"f2\": 0.049963441384352915, \"f0_5\": 0.17380245866892752, \"p4\": 0.13572905941236357, \"phi\": 0.12223471186939629}, {\"truth_threshold\": 21.40000031888485, \"match_probability\": 0.9999996386252203, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 81.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1950.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.03988183161004431, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9601181683899557, \"precision\": 1.0, \"recall\": 0.03988183161004431, \"specificity\": 1.0, \"npv\": 0.36995153473344106, \"accuracy\": 0.38602015113350124, \"f1\": 0.07670454545454546, \"f2\": 0.04936014625228519, \"f0_5\": 0.17197452229299362, \"p4\": 0.1343312766142348, \"phi\": 0.12146746400627847}, {\"truth_threshold\": 21.500000320374966, \"match_probability\": 0.9999996628254004, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 78.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1953.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.03840472673559823, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9615952732644018, \"precision\": 1.0, \"recall\": 0.03840472673559823, \"specificity\": 1.0, \"npv\": 0.3695932859909619, \"accuracy\": 0.3850755667506297, \"f1\": 0.07396870554765292, \"f2\": 0.04754937820043892, \"f0_5\": 0.16645326504481434, \"p4\": 0.1301061018101006, \"phi\": 0.11913911679962505}, {\"truth_threshold\": 21.600000321865082, \"match_probability\": 0.999999685404968, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 72.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1959.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.03545051698670606, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9645494830132939, \"precision\": 1.0, \"recall\": 0.03545051698670606, \"specificity\": 1.0, \"npv\": 0.36887886597938147, \"accuracy\": 0.38318639798488663, \"f1\": 0.06847360912981455, \"f2\": 0.043923865300146414, \"f0_5\": 0.15523932729624837, \"p4\": 0.12150944981378942, \"phi\": 0.11435447741316879}, {\"truth_threshold\": 21.700000323355198, \"match_probability\": 0.9999997064724503, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 69.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1962.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.033973412112259974, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.96602658788774, \"precision\": 1.0, \"recall\": 0.033973412112259974, \"specificity\": 1.0, \"npv\": 0.3685226906984229, \"accuracy\": 0.3822418136020151, \"f1\": 0.06571428571428571, \"f2\": 0.04210911753936287, \"f0_5\": 0.14954486345903772, \"p4\": 0.11713607088211223, \"phi\": 0.11189268628385163}, {\"truth_threshold\": 21.800000324845314, \"match_probability\": 0.999999726129107, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 64.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1967.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.03151157065484983, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9684884293451502, \"precision\": 1.0, \"recall\": 0.03151157065484983, \"specificity\": 1.0, \"npv\": 0.3679305912596401, \"accuracy\": 0.38066750629722923, \"f1\": 0.06109785202863962, \"f2\": 0.039081582804103565, \"f0_5\": 0.139921294271972, \"p4\": 0.10973250829301784, \"phi\": 0.10767576710921925}, {\"truth_threshold\": 21.90000032633543, \"match_probability\": 0.9999997444694171, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 57.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1974.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.028064992614475627, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9719350073855244, \"precision\": 1.0, \"recall\": 0.028064992614475627, \"specificity\": 1.0, \"npv\": 0.367104841295287, \"accuracy\": 0.378463476070529, \"f1\": 0.05459770114942529, \"f2\": 0.034836817015034834, \"f0_5\": 0.12616201859229748, \"p4\": 0.09911884237575404, \"phi\": 0.10150268301720144}, {\"truth_threshold\": 22.20000033080578, \"match_probability\": 0.9999997924446623, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 56.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1975.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.027572624322993598, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9724273756770064, \"precision\": 1.0, \"recall\": 0.027572624322993598, \"specificity\": 1.0, \"npv\": 0.36698717948717946, \"accuracy\": 0.3781486146095718, \"f1\": 0.05366554863440345, \"f2\": 0.034229828850855744, \"f0_5\": 0.12416851441241686, \"p4\": 0.09757823512291441, \"phi\": 0.10059224438968953}, {\"truth_threshold\": 22.300000332295895, \"match_probability\": 0.9999998063440199, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 55.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1976.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.02708025603151157, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9729197439684885, \"precision\": 1.0, \"recall\": 0.02708025603151157, \"specificity\": 1.0, \"npv\": 0.3668695930791413, \"accuracy\": 0.3778337531486146, \"f1\": 0.052732502396931925, \"f2\": 0.03362269226066757, \"f0_5\": 0.12216792536650378, \"p4\": 0.0960314132133735, \"phi\": 0.09967408143925688}, {\"truth_threshold\": 22.500000335276127, \"match_probability\": 0.9999998314126736, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 54.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1977.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.026587887740029542, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9734121122599705, \"precision\": 1.0, \"recall\": 0.026587887740029542, \"specificity\": 1.0, \"npv\": 0.3667520819987188, \"accuracy\": 0.37751889168765745, \"f1\": 0.051798561151079135, \"f2\": 0.03301540719002201, \"f0_5\": 0.12016021361815754, \"p4\": 0.09447833496259946, \"phi\": 0.09874797812919536}, {\"truth_threshold\": 22.70000033825636, \"match_probability\": 0.9999998532362051, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 51.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1980.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.025110782865583457, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9748892171344166, \"precision\": 1.0, \"recall\": 0.025110782865583457, \"specificity\": 1.0, \"npv\": 0.3664, \"accuracy\": 0.3765743073047859, \"f1\": 0.04899135446685879, \"f2\": 0.031192660550458717, \"f0_5\": 0.11409395973154363, \"p4\": 0.08978113973386223, \"phi\": 0.09591971039337942}, {\"truth_threshold\": 22.800000339746475, \"match_probability\": 0.9999998630645361, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 50.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1981.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.024618414574101428, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9753815854258986, \"precision\": 1.0, \"recall\": 0.024618414574101428, \"specificity\": 1.0, \"npv\": 0.36628278950735765, \"accuracy\": 0.3762594458438287, \"f1\": 0.048053820278712155, \"f2\": 0.030584781012967948, \"f0_5\": 0.11205737337516809, \"p4\": 0.08820261179873627, \"phi\": 0.09495947326860263}, {\"truth_threshold\": 23.000000342726707, \"match_probability\": 0.999999880790753, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 49.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1982.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.0241260462826194, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9758739537173806, \"precision\": 1.0, \"recall\": 0.0241260462826194, \"specificity\": 1.0, \"npv\": 0.36616565398145184, \"accuracy\": 0.37594458438287154, \"f1\": 0.047115384615384615, \"f2\": 0.029976752722378562, \"f0_5\": 0.11001347103726987, \"p4\": 0.08661761345741757, \"phi\": 0.09399005008543249}, {\"truth_threshold\": 23.500000350177288, \"match_probability\": 0.9999999157063305, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 46.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1985.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.022648941408173313, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9773510585918267, \"precision\": 1.0, \"recall\": 0.022648941408173313, \"specificity\": 1.0, \"npv\": 0.365814696485623, \"accuracy\": 0.375, \"f1\": 0.04429465575349061, \"f2\": 0.028151774785801713, \"f0_5\": 0.1038374717832957, \"p4\": 0.08182335429924091, \"phi\": 0.0910237091474061}, {\"truth_threshold\": 23.70000035315752, \"match_probability\": 0.9999999266180979, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 44.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1987.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.021664204825209258, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9783357951747907, \"precision\": 1.0, \"recall\": 0.021664204825209258, \"specificity\": 1.0, \"npv\": 0.365581098339719, \"accuracy\": 0.3743702770780856, \"f1\": 0.042409638554216866, \"f2\": 0.02693437806072478, \"f0_5\": 0.09968282736746716, \"p4\": 0.07859401270561604, \"phi\": 0.08899451553133284}, {\"truth_threshold\": 23.900000356137753, \"match_probability\": 0.9999999361173434, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 42.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1989.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.0206794682422452, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9793205317577548, \"precision\": 1.0, \"recall\": 0.0206794682422452, \"specificity\": 1.0, \"npv\": 0.3653477983407786, \"accuracy\": 0.3737405541561713, \"f1\": 0.04052098408104197, \"f2\": 0.025716385011021307, \"f0_5\": 0.09549795361527967, \"p4\": 0.0753377096255321, \"phi\": 0.08692064307839845}, {\"truth_threshold\": 24.00000035762787, \"match_probability\": 0.9999999403953735, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 37.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1994.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.018217626784835055, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9817823732151649, \"precision\": 1.0, \"recall\": 0.018217626784835055, \"specificity\": 1.0, \"npv\": 0.3647658489964957, \"accuracy\": 0.3721662468513854, \"f1\": 0.035783365570599614, \"f2\": 0.022668790589388556, \"f0_5\": 0.08490133088572739, \"p4\": 0.06707653041837135, \"phi\": 0.08151790049352142}, {\"truth_threshold\": 24.100000359117985, \"match_probability\": 0.9999999443869169, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 32.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1999.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.015755785327424915, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9842442146725751, \"precision\": 1.0, \"recall\": 0.015755785327424915, \"specificity\": 1.0, \"npv\": 0.3641857506361323, \"accuracy\": 0.37059193954659947, \"f1\": 0.031022782355792537, \"f2\": 0.0196174595389897, \"f0_5\": 0.07410838351088467, \"p4\": 0.0586384719748834, \"phi\": 0.07574980202172149}, {\"truth_threshold\": 24.2000003606081, \"match_probability\": 0.9999999481111586, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 30.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 2001.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.014771048744460856, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9852289512555391, \"precision\": 1.0, \"recall\": 0.014771048744460856, \"specificity\": 1.0, \"npv\": 0.3639542275905912, \"accuracy\": 0.36996221662468515, \"f1\": 0.02911208151382824, \"f2\": 0.01839587932303164, \"f0_5\": 0.0697350069735007, \"p4\": 0.05521232030378831, \"phi\": 0.07332111317003598}, {\"truth_threshold\": 24.300000362098217, \"match_probability\": 0.999999951585999, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 27.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 2004.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.013293943870014771, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9867060561299852, \"precision\": 1.0, \"recall\": 0.013293943870014771, \"specificity\": 1.0, \"npv\": 0.3636074944426802, \"accuracy\": 0.3690176322418136, \"f1\": 0.026239067055393587, \"f2\": 0.016562384983437616, \"f0_5\": 0.06311360448807854, \"p4\": 0.050017230584043997, \"phi\": 0.06952537394245138}, {\"truth_threshold\": 24.50000036507845, \"match_probability\": 0.999999957853164, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 26.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 2005.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.012801575578532743, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9871984244214672, \"precision\": 1.0, \"recall\": 0.012801575578532743, \"specificity\": 1.0, \"npv\": 0.3634920634920635, \"accuracy\": 0.36870277078085645, \"f1\": 0.025279533300923675, \"f2\": 0.015950920245398775, \"f0_5\": 0.06088992974238876, \"p4\": 0.048270424636238894, \"phi\": 0.0682148893057115}, {\"truth_threshold\": 24.70000036805868, \"match_probability\": 0.999999963309048, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 25.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 2006.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.012309207287050714, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9876907927129492, \"precision\": 1.0, \"recall\": 0.012309207287050714, \"specificity\": 1.0, \"npv\": 0.3633767058076801, \"accuracy\": 0.36838790931989923, \"f1\": 0.024319066147859923, \"f2\": 0.015339305436249846, \"f0_5\": 0.05865790708587518, \"p4\": 0.04651597386980402, \"phi\": 0.06687958728246145}, {\"truth_threshold\": 24.800000369548798, \"match_probability\": 0.9999999657661313, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 23.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 2008.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.011324470704086657, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9886755292959133, \"precision\": 1.0, \"recall\": 0.011324470704086657, \"specificity\": 1.0, \"npv\": 0.36314620995876945, \"accuracy\": 0.3677581863979849, \"f1\": 0.022395326192794548, \"f2\": 0.014115625383576776, \"f0_5\": 0.054168629298162976, \"p4\": 0.042983917959230976, \"phi\": 0.06412829809045448}, {\"truth_threshold\": 25.00000037252903, \"match_probability\": 0.9999999701976862, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 22.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 2009.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.010832102412604629, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9891678975873953, \"precision\": 1.0, \"recall\": 0.010832102412604629, \"specificity\": 1.0, \"npv\": 0.3630310716550412, \"accuracy\": 0.3674433249370277, \"f1\": 0.02143205065757428, \"f2\": 0.013503560029462312, \"f0_5\": 0.05191127890514394, \"p4\": 0.04120620154151108, \"phi\": 0.06270876930003502}, {\"truth_threshold\": 25.100000374019146, \"match_probability\": 0.9999999721934579, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 18.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 2013.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.008862629246676515, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9911373707533235, \"precision\": 1.0, \"recall\": 0.008862629246676515, \"specificity\": 1.0, \"npv\": 0.36257124762507914, \"accuracy\": 0.366183879093199, \"f1\": 0.017569546120058566, \"f2\": 0.01105379513633014, \"f0_5\": 0.042796005706134094, \"p4\": 0.034016089560848325, \"phi\": 0.05668628179027108}, {\"truth_threshold\": 25.200000375509262, \"match_probability\": 0.9999999740555788, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 17.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 2014.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.008370260955194485, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9916297390448056, \"precision\": 1.0, \"recall\": 0.008370260955194485, \"specificity\": 1.0, \"npv\": 0.3624564735675847, \"accuracy\": 0.36586901763224183, \"f1\": 0.0166015625, \"f2\": 0.010440977766859108, \"f0_5\": 0.040495474035254886, \"p4\": 0.03219846095822884, \"phi\": 0.05508044361350257}, {\"truth_threshold\": 25.400000378489494, \"match_probability\": 0.9999999774140695, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 15.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 2016.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.007385524372230428, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9926144756277696, \"precision\": 1.0, \"recall\": 0.007385524372230428, \"specificity\": 1.0, \"npv\": 0.3622271433090794, \"accuracy\": 0.36523929471032746, \"f1\": 0.01466275659824047, \"f2\": 0.009214891264283081, \"f0_5\": 0.035868005738880916, \"p4\": 0.028538670521671944, \"phi\": 0.05172269709897784}, {\"truth_threshold\": 25.50000037997961, \"match_probability\": 0.9999999789265818, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 12.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 2019.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.005908419497784343, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9940915805022157, \"precision\": 1.0, \"recall\": 0.005908419497784343, \"specificity\": 1.0, \"npv\": 0.36188369152970923, \"accuracy\": 0.3642947103274559, \"f1\": 0.011747430249632892, \"f2\": 0.007374631268436578, \"f0_5\": 0.02886002886002886, \"p4\": 0.022986746233599045, \"phi\": 0.046240249339339734}, {\"truth_threshold\": 25.600000381469727, \"match_probability\": 0.9999999803378055, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 9.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 2022.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.004431314623338257, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9955686853766618, \"precision\": 1.0, \"recall\": 0.004431314623338257, \"specificity\": 1.0, \"npv\": 0.361540890432586, \"accuracy\": 0.3633501259445844, \"f1\": 0.008823529411764706, \"f2\": 0.005533013648100332, \"f0_5\": 0.02177068214804064, \"p4\": 0.017358654565300884, \"phi\": 0.040026259314463214}, {\"truth_threshold\": 25.700000382959843, \"match_probability\": 0.9999999816545239, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 8.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 2023.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.003938946331856229, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9960610536681438, \"precision\": 1.0, \"recall\": 0.003938946331856229, \"specificity\": 1.0, \"npv\": 0.3614267676767677, \"accuracy\": 0.3630352644836272, \"f1\": 0.00784698381559588, \"f2\": 0.004918839153959666, \"f0_5\": 0.019389238972370333, \"p4\": 0.015465403546152875, \"phi\": 0.03773116272757914}, {\"truth_threshold\": 25.900000385940075, \"match_probability\": 0.9999999840293354, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 6.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 2025.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.0029542097488921715, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9970457902511078, \"precision\": 1.0, \"recall\": 0.0029542097488921715, \"specificity\": 1.0, \"npv\": 0.361198738170347, \"accuracy\": 0.36240554156171284, \"f1\": 0.005891016200294551, \"f2\": 0.0036900369003690036, \"f0_5\": 0.014598540145985401, \"p4\": 0.011652683870064942, \"phi\": 0.032665835877723835}, {\"truth_threshold\": 26.00000038743019, \"match_probability\": 0.999999985098843, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 5.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 2026.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.002461841457410143, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9975381585425899, \"precision\": 1.0, \"recall\": 0.002461841457410143, \"specificity\": 1.0, \"npv\": 0.36108483128350677, \"accuracy\": 0.3620906801007557, \"f1\": 0.004911591355599214, \"f2\": 0.0030754090294009104, \"f0_5\": 0.01218917601170161, \"p4\": 0.009733083985039102, \"phi\": 0.02981498964104606}, {\"truth_threshold\": 26.100000388920307, \"match_probability\": 0.9999999860967289, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 4.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 2027.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.0019694731659281144, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9980305268340719, \"precision\": 1.0, \"recall\": 0.0019694731659281144, \"specificity\": 1.0, \"npv\": 0.3609709962168979, \"accuracy\": 0.36177581863979846, \"f1\": 0.003931203931203931, \"f2\": 0.0024606299212598425, \"f0_5\": 0.009770395701025891, \"p4\": 0.007804568825263287, \"phi\": 0.026663133550419747}, {\"truth_threshold\": 26.30000039190054, \"match_probability\": 0.9999999878964996, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 3.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 2028.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.0014771048744460858, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9985228951255539, \"precision\": 1.0, \"recall\": 0.0014771048744460858, \"specificity\": 1.0, \"npv\": 0.36085723290261584, \"accuracy\": 0.3614609571788413, \"f1\": 0.0029498525073746312, \"f2\": 0.0018456995201181247, \"f0_5\": 0.007342143906020558, \"p4\": 0.00586707112734875, \"phi\": 0.023087312050119223}, {\"truth_threshold\": 26.50000039488077, \"match_probability\": 0.9999999894632908, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 2.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 2029.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.0009847365829640572, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.999015263417036, \"precision\": 1.0, \"recall\": 0.0009847365829640572, \"specificity\": 1.0, \"npv\": 0.36074354127284186, \"accuracy\": 0.36114609571788414, \"f1\": 0.001967535661583866, \"f2\": 0.0012306177701206006, \"f0_5\": 0.004904364884747425, \"p4\": 0.003920522953249476, \"phi\": 0.018847741566547744}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linker.accuracy_analysis_from_labels_table(\n",
    "    labels_table, output_type=\"roc\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f749c3c",
   "metadata": {},
   "source": [
    "### Precision-recall chart\n",
    "\n",
    "An alternative representation of truth space is called a [precision recall curve](https://stats.stackexchange.com/questions/7207/roc-vs-precision-and-recall-curves).\n",
    "\n",
    "This can be plotted as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18d25327",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-20T20:18:24.935899Z",
     "iopub.status.busy": "2024-05-20T20:18:24.935297Z",
     "iopub.status.idle": "2024-05-20T20:18:25.555049Z",
     "shell.execute_reply": "2024-05-20T20:18:25.554412Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-ce24fa800c884ce1a01eeee3ca3f0e06.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-ce24fa800c884ce1a01eeee3ca3f0e06.vega-embed details,\n",
       "  #altair-viz-ce24fa800c884ce1a01eeee3ca3f0e06.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-ce24fa800c884ce1a01eeee3ca3f0e06\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-ce24fa800c884ce1a01eeee3ca3f0e06\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-ce24fa800c884ce1a01eeee3ca3f0e06\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-679a32ee851a5db5902bffa3c670a370\"}, \"mark\": {\"type\": \"line\", \"clip\": true, \"point\": true}, \"encoding\": {\"tooltip\": [{\"field\": \"truth_threshold\", \"format\": \".4f\", \"type\": \"quantitative\"}, {\"field\": \"match_probability\", \"format\": \".4%\", \"type\": \"quantitative\"}, {\"field\": \"fp_rate\", \"format\": \".4f\", \"title\": \"FP_rate\", \"type\": \"quantitative\"}, {\"field\": \"tp_rate\", \"format\": \".4f\", \"title\": \"TP_rate\", \"type\": \"quantitative\"}, {\"field\": \"tp\", \"format\": \",.0f\", \"title\": \"TP\", \"type\": \"quantitative\"}, {\"field\": \"tn\", \"format\": \",.0f\", \"title\": \"TN\", \"type\": \"quantitative\"}, {\"field\": \"fp\", \"format\": \",.0f\", \"title\": \"FP\", \"type\": \"quantitative\"}, {\"field\": \"fn\", \"format\": \",.0f\", \"title\": \"FN\", \"type\": \"quantitative\"}, {\"field\": \"precision\", \"format\": \".4f\", \"type\": \"quantitative\"}, {\"field\": \"recall\", \"format\": \".4f\", \"type\": \"quantitative\"}, {\"field\": \"f1\", \"format\": \".4f\", \"title\": \"F1\", \"type\": \"quantitative\"}], \"x\": {\"field\": \"recall\", \"sort\": [\"-recall\"], \"title\": \"Recall\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"precision\", \"sort\": [\"-precision\"], \"title\": \"Precision\", \"type\": \"quantitative\"}}, \"height\": 400, \"params\": [{\"name\": \"mouse_zoom\", \"select\": {\"type\": \"interval\", \"encodings\": [\"x\", \"y\"]}, \"bind\": \"scales\"}], \"title\": \"Precision-recall curve\", \"width\": 400, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.9.3.json\", \"datasets\": {\"data-679a32ee851a5db5902bffa3c670a370\": [{\"truth_threshold\": -12.300000183284283, \"match_probability\": 0.00019826446591752426, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 1027.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1004.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.5056622353520434, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.49433776464795665, \"precision\": 1.0, \"recall\": 0.5056622353520434, \"specificity\": 1.0, \"npv\": 0.532805956258725, \"accuracy\": 0.6838790931989924, \"f1\": 0.671680837148463, \"f2\": 0.5611408589225221, \"f0_5\": 0.836455448770158, \"p4\": 0.6832397200141539, \"phi\": 0.5190566932914649}, {\"truth_threshold\": -5.800000086426735, \"match_probability\": 0.017631945325087592, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 1026.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1005.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.5051698670605613, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.4948301329394387, \"precision\": 1.0, \"recall\": 0.5051698670605613, \"specificity\": 1.0, \"npv\": 0.5325581395348837, \"accuracy\": 0.6835642317380353, \"f1\": 0.6712463199214916, \"f2\": 0.5606557377049181, \"f0_5\": 0.8361858190709046, \"p4\": 0.6829130052819856, \"phi\": 0.5186832603341437}, {\"truth_threshold\": -4.700000070035458, \"match_probability\": 0.037047907242669466, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 1025.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1006.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.5046774987690793, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.49532250123092075, \"precision\": 1.0, \"recall\": 0.5046774987690793, \"specificity\": 1.0, \"npv\": 0.5323105532310554, \"accuracy\": 0.6832493702770781, \"f1\": 0.6708115183246073, \"f2\": 0.5601705104382992, \"f0_5\": 0.8359158375468928, \"p4\": 0.6825861647803277, \"phi\": 0.518309905918297}, {\"truth_threshold\": -3.7000000551342964, \"match_probability\": 0.07144878715678568, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 1023.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1008.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.5036927621861153, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.4963072378138848, \"precision\": 1.0, \"recall\": 0.5036927621861153, \"specificity\": 1.0, \"npv\": 0.5318160705991639, \"accuracy\": 0.6826196473551638, \"f1\": 0.6699410609037328, \"f2\": 0.5591997376188914, \"f0_5\": 0.835374816266536, \"p4\": 0.6819321045764876, \"phi\": 0.5175634314507344}, {\"truth_threshold\": -3.200000047683716, \"match_probability\": 0.09813940308831819, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 1017.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1014.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.5007385524372231, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.49926144756277696, \"precision\": 1.0, \"recall\": 0.5007385524372231, \"specificity\": 1.0, \"npv\": 0.5303381194997684, \"accuracy\": 0.6807304785894207, \"f1\": 0.6673228346456693, \"f2\": 0.5562848703642927, \"f0_5\": 0.8337432365961633, \"p4\": 0.6799668560937839, \"phi\": 0.5153258602676495}, {\"truth_threshold\": -2.600000038743019, \"match_probability\": 0.1415855743659812, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 1016.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1015.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.5002461841457411, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.499753815854259, \"precision\": 1.0, \"recall\": 0.5002461841457411, \"specificity\": 1.0, \"npv\": 0.5300925925925926, \"accuracy\": 0.6804156171284634, \"f1\": 0.6668854611092878, \"f2\": 0.5557986870897156, \"f0_5\": 0.8334700574241182, \"p4\": 0.679638862253978, \"phi\": 0.5149531985417386}, {\"truth_threshold\": -2.500000037252903, \"match_probability\": 0.15022110152606716, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 1007.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1024.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.4958148695224028, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5041851304775973, \"precision\": 1.0, \"recall\": 0.4958148695224028, \"specificity\": 1.0, \"npv\": 0.5278930382664823, \"accuracy\": 0.6775818639798489, \"f1\": 0.662936142198815, \"f2\": 0.551418245537181, \"f0_5\": 0.8309952137316389, \"p4\": 0.6766809845726959, \"phi\": 0.5116025976183864}, {\"truth_threshold\": -2.2000000327825546, \"match_probability\": 0.1787376058900962, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 999.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1032.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.4918759231905465, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5081240768094535, \"precision\": 1.0, \"recall\": 0.4918759231905465, \"specificity\": 1.0, \"npv\": 0.5259531465319247, \"accuracy\": 0.6750629722921915, \"f1\": 0.6594059405940594, \"f2\": 0.5475172640578757, \"f0_5\": 0.8287705326032853, \"p4\": 0.6740425938136967, \"phi\": 0.5086292259646149}, {\"truth_threshold\": -2.1000000312924385, \"match_probability\": 0.18913982061899084, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 997.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1034.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.49089118660758246, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5091088133924175, \"precision\": 1.0, \"recall\": 0.49089118660758246, \"specificity\": 1.0, \"npv\": 0.5254703992657183, \"accuracy\": 0.674433249370277, \"f1\": 0.6585204755614267, \"f2\": 0.5465409494572964, \"f0_5\": 0.8282106662236252, \"p4\": 0.6733816166373302, \"phi\": 0.5078865895283203}, {\"truth_threshold\": -2.0000000298023224, \"match_probability\": 0.19999999669481672, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 996.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1035.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.49039881831610044, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5096011816838996, \"precision\": 1.0, \"recall\": 0.49039881831610044, \"specificity\": 1.0, \"npv\": 0.5252293577981652, \"accuracy\": 0.6741183879093199, \"f1\": 0.6580773042616452, \"f2\": 0.5460526315789473, \"f0_5\": 0.827930174563591, \"p4\": 0.6730509183540228, \"phi\": 0.5075153755396427}, {\"truth_threshold\": -1.2000000178813934, \"match_probability\": 0.3032695424040186, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 995.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1036.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.4899064500246184, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5100935499753816, \"precision\": 1.0, \"recall\": 0.4899064500246184, \"specificity\": 1.0, \"npv\": 0.5249885373681797, \"accuracy\": 0.6738035264483627, \"f1\": 0.6576338400528751, \"f2\": 0.5455642066016011, \"f0_5\": 0.8276493095990684, \"p4\": 0.6727200795968197, \"phi\": 0.5071442306145872}, {\"truth_threshold\": -1.1000000163912773, \"match_probability\": 0.318111997717226, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 987.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1044.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.4859675036927622, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5140324963072378, \"precision\": 1.0, \"recall\": 0.4859675036927622, \"specificity\": 1.0, \"npv\": 0.5230698949291914, \"accuracy\": 0.6712846347607053, \"f1\": 0.6540755467196819, \"f2\": 0.5416529469871584, \"f0_5\": 0.8253888610135475, \"p4\": 0.6700682510685908, \"phi\": 0.5041775194270115}, {\"truth_threshold\": -0.7000000104308128, \"match_probability\": 0.38102425962470177, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 980.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1051.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.48252092565238797, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.517479074347612, \"precision\": 1.0, \"recall\": 0.48252092565238797, \"specificity\": 1.0, \"npv\": 0.5214025500910747, \"accuracy\": 0.6690806045340051, \"f1\": 0.6509465293922285, \"f2\": 0.5382249560632689, \"f0_5\": 0.8233910267181986, \"p4\": 0.6677402918128024, \"phi\": 0.5015851284751782}, {\"truth_threshold\": -0.10000000149011612, \"match_probability\": 0.48267825490990723, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 979.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1052.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.48202855736090594, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.517971442639094, \"precision\": 1.0, \"recall\": 0.48202855736090594, \"specificity\": 1.0, \"npv\": 0.5211652253072372, \"accuracy\": 0.6687657430730478, \"f1\": 0.6504983388704318, \"f2\": 0.5377348126991102, \"f0_5\": 0.8231040860938288, \"p4\": 0.6674071352914174, \"phi\": 0.5012150453662769}, {\"truth_threshold\": -0.0, \"match_probability\": 0.5, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 976.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1055.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.48055145248645986, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5194485475135401, \"precision\": 1.0, \"recall\": 0.48055145248645986, \"specificity\": 1.0, \"npv\": 0.5204545454545455, \"accuracy\": 0.6678211586901763, \"f1\": 0.6491519787163286, \"f2\": 0.5362637362637362, \"f0_5\": 0.8222409435551812, \"p4\": 0.6664067677092192, \"phi\": 0.5001051767092219}, {\"truth_threshold\": 0.9000000134110451, \"match_probability\": 0.6510896818658842, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 971.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1060.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.47808961102904973, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5219103889709503, \"precision\": 1.0, \"recall\": 0.47808961102904973, \"specificity\": 1.0, \"npv\": 0.5192743764172335, \"accuracy\": 0.6662468513853904, \"f1\": 0.6469020652898068, \"f2\": 0.5338097855964816, \"f0_5\": 0.8207945900253593, \"p4\": 0.6647364629140795, \"phi\": 0.49825664535324315}, {\"truth_threshold\": 1.0000000149011612, \"match_probability\": 0.6666666689619328, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 970.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1061.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.4775972427375677, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5224027572624323, \"precision\": 1.0, \"recall\": 0.4775972427375677, \"specificity\": 1.0, \"npv\": 0.5190389845874886, \"accuracy\": 0.6659319899244333, \"f1\": 0.6464511829390204, \"f2\": 0.5333186716516385, \"f0_5\": 0.8205041448147522, \"p4\": 0.6644019432852049, \"phi\": 0.4978871236658882}, {\"truth_threshold\": 1.5000000223517418, \"match_probability\": 0.7387961280260511, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 969.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1062.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.4771048744460857, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5228951255539144, \"precision\": 1.0, \"recall\": 0.4771048744460857, \"specificity\": 1.0, \"npv\": 0.5188038060715904, \"accuracy\": 0.6656171284634761, \"f1\": 0.646, \"f2\": 0.5328274496865721, \"f0_5\": 0.8202133062468258, \"p4\": 0.6640672695017632, \"phi\": 0.4975176627597633}, {\"truth_threshold\": 1.700000025331974, \"match_probability\": 0.7646510400908766, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 968.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1063.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.47661250615460365, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5233874938453964, \"precision\": 1.0, \"recall\": 0.47661250615460365, \"specificity\": 1.0, \"npv\": 0.5185688405797102, \"accuracy\": 0.6653022670025189, \"f1\": 0.6455485161720573, \"f2\": 0.5323361196656401, \"f0_5\": 0.819922073521938, \"p4\": 0.6637324410189356, \"phi\": 0.49714826231455617}, {\"truth_threshold\": 1.9000000283122063, \"match_probability\": 0.7886787621992872, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 967.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1064.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.47612013786312163, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5238798621368784, \"precision\": 1.0, \"recall\": 0.47612013786312163, \"specificity\": 1.0, \"npv\": 0.5183340878225441, \"accuracy\": 0.6649874055415617, \"f1\": 0.6450967311541027, \"f2\": 0.5318446815531844, \"f0_5\": 0.8196304458382777, \"p4\": 0.6633974572904727, \"phi\": 0.49677892200980617}, {\"truth_threshold\": 2.0000000298023224, \"match_probability\": 0.8000000033051833, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 961.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1070.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.4731659281142294, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5268340718857706, \"precision\": 1.0, \"recall\": 0.4731659281142294, \"specificity\": 1.0, \"npv\": 0.5169300225733634, \"accuracy\": 0.6630982367758187, \"f1\": 0.642379679144385, \"f2\": 0.5288937809576224, \"f0_5\": 0.8178723404255319, \"p4\": 0.661384263989902, \"phi\": 0.49456412516582227}, {\"truth_threshold\": 2.2000000327825546, \"match_probability\": 0.8212623941099038, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 960.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1071.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.4726735598227474, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5273264401772526, \"precision\": 1.0, \"recall\": 0.4726735598227474, \"specificity\": 1.0, \"npv\": 0.5166967509025271, \"accuracy\": 0.6627833753148614, \"f1\": 0.641925777331996, \"f2\": 0.5284015852047557, \"f0_5\": 0.8175779253960143, \"p4\": 0.6610481781257823, \"phi\": 0.49419519685843255}, {\"truth_threshold\": 2.400000035762787, \"match_probability\": 0.8407144092272857, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 959.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1072.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.47218119153126537, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5278188084687346, \"precision\": 1.0, \"recall\": 0.47218119153126537, \"specificity\": 1.0, \"npv\": 0.5164636896707262, \"accuracy\": 0.6624685138539043, \"f1\": 0.6414715719063545, \"f2\": 0.5279092810745348, \"f0_5\": 0.8172831089142663, \"p4\": 0.6607119325939106, \"phi\": 0.49382632612220784}, {\"truth_threshold\": 2.500000037252903, \"match_probability\": 0.8497788984739328, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 957.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1074.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.4711964549483013, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5288035450516987, \"precision\": 1.0, \"recall\": 0.4711964549483013, \"specificity\": 1.0, \"npv\": 0.51599819738621, \"accuracy\": 0.6618387909319899, \"f1\": 0.6405622489959839, \"f2\": 0.5269243475388173, \"f0_5\": 0.8166922683051715, \"p4\": 0.6600389602879736, \"phi\": 0.49308875607551217}, {\"truth_threshold\": 2.600000038743019, \"match_probability\": 0.8584144256340188, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 954.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1077.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.46971935007385524, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5302806499261448, \"precision\": 1.0, \"recall\": 0.46971935007385524, \"specificity\": 1.0, \"npv\": 0.5153015301530153, \"accuracy\": 0.6608942065491183, \"f1\": 0.6391959798994975, \"f2\": 0.5254461335095836, \"f0_5\": 0.8158029758850692, \"p4\": 0.6590282902884685, \"phi\": 0.4919828247363291}, {\"truth_threshold\": 2.8000000417232513, \"match_probability\": 0.8744413378412453, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 950.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1081.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.46774987690792713, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5322501230920729, \"precision\": 1.0, \"recall\": 0.46774987690792713, \"specificity\": 1.0, \"npv\": 0.5143755615453729, \"accuracy\": 0.6596347607052897, \"f1\": 0.637370010063737, \"f2\": 0.5234736610094777, \"f0_5\": 0.8146115589092779, \"p4\": 0.6576784449706342, \"phi\": 0.4905090270293647}, {\"truth_threshold\": 2.9000000432133675, \"match_probability\": 0.8818562391739494, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 941.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1090.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.4633185622845889, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5366814377154111, \"precision\": 1.0, \"recall\": 0.4633185622845889, \"specificity\": 1.0, \"npv\": 0.5123042505592841, \"accuracy\": 0.656801007556675, \"f1\": 0.6332436069986541, \"f2\": 0.5190292333149475, \"f0_5\": 0.8119068162208801, \"p4\": 0.6546315648771478, \"phi\": 0.4871961297274552}, {\"truth_threshold\": 3.0000000447034836, \"match_probability\": 0.8888888919492438, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 938.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1093.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.46184145741014276, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5381585425898572, \"precision\": 1.0, \"recall\": 0.46184145741014276, \"specificity\": 1.0, \"npv\": 0.5116175156389634, \"accuracy\": 0.6558564231738035, \"f1\": 0.6318625799932637, \"f2\": 0.5175457956301037, \"f0_5\": 0.8109977520318173, \"p4\": 0.6536128891991932, \"phi\": 0.48609276795613343}, {\"truth_threshold\": 3.1000000461935997, \"match_probability\": 0.8955524998434058, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 934.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1097.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.45987198424421466, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5401280157557853, \"precision\": 1.0, \"recall\": 0.45987198424421466, \"specificity\": 1.0, \"npv\": 0.5107047279214987, \"accuracy\": 0.6545969773299748, \"f1\": 0.6300168634064081, \"f2\": 0.5155663501876794, \"f0_5\": 0.8097797815155193, \"p4\": 0.6522522396145514, \"phi\": 0.4846223236626243}, {\"truth_threshold\": 3.200000047683716, \"match_probability\": 0.9018605969116819, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 933.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1098.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.45937961595273263, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5406203840472673, \"precision\": 1.0, \"recall\": 0.45937961595273263, \"specificity\": 1.0, \"npv\": 0.5104770396790014, \"accuracy\": 0.6542821158690176, \"f1\": 0.6295546558704453, \"f2\": 0.515071215634316, \"f0_5\": 0.8094742321707444, \"p4\": 0.6519116419396886, \"phi\": 0.48425483625920296}, {\"truth_threshold\": 3.400000050663948, \"match_probability\": 0.9134653434169965, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 932.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1099.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.4588872476612506, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5411127523387493, \"precision\": 1.0, \"recall\": 0.4588872476612506, \"specificity\": 1.0, \"npv\": 0.5102495543672014, \"accuracy\": 0.6539672544080605, \"f1\": 0.6290921363482956, \"f2\": 0.5145759717314488, \"f0_5\": 0.8091682583781906, \"p4\": 0.6515708689560343, \"phi\": 0.4838873976701033}, {\"truth_threshold\": 3.500000052154064, \"match_probability\": 0.9187896995557598, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 931.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1100.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.4583948793697686, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5416051206302314, \"precision\": 1.0, \"recall\": 0.4583948793697686, \"specificity\": 1.0, \"npv\": 0.5100222717149221, \"accuracy\": 0.6536523929471033, \"f1\": 0.6286293045239703, \"f2\": 0.5140806184428492, \"f0_5\": 0.8088618592528236, \"p4\": 0.6512299200620687, \"phi\": 0.4835200075681016}, {\"truth_threshold\": 3.6000000536441803, \"match_probability\": 0.9238137785296746, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 930.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1101.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.45790251107828656, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5420974889217134, \"precision\": 1.0, \"recall\": 0.45790251107828656, \"specificity\": 1.0, \"npv\": 0.5097951914514692, \"accuracy\": 0.6533375314861462, \"f1\": 0.6281661600810537, \"f2\": 0.513585155732273, \"f0_5\": 0.8085550339071466, \"p4\": 0.650888794654625, \"phi\": 0.48315266562574566}, {\"truth_threshold\": 3.7000000551342964, \"match_probability\": 0.9285512128432143, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 923.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1108.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.4544559330379124, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5455440669620877, \"precision\": 1.0, \"recall\": 0.4544559330379124, \"specificity\": 1.0, \"npv\": 0.5082112738570794, \"accuracy\": 0.6511335012594458, \"f1\": 0.6249153689911984, \"f2\": 0.5101138498949929, \"f0_5\": 0.8063952472479469, \"p4\": 0.6484959234103079, \"phi\": 0.4805825929443398}, {\"truth_threshold\": 3.9000000581145287, \"match_probability\": 0.9372195616099515, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 920.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1111.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.4529788281634663, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5470211718365338, \"precision\": 1.0, \"recall\": 0.4529788281634663, \"specificity\": 1.0, \"npv\": 0.5075354609929078, \"accuracy\": 0.6501889168765743, \"f1\": 0.6235174517112844, \"f2\": 0.508624502432552, \"f0_5\": 0.8054631413062511, \"p4\": 0.6474676984517845, \"phi\": 0.47948182277534984}, {\"truth_threshold\": 4.000000059604645, \"match_probability\": 0.9411764728755594, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 919.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1112.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.45248645987198427, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5475135401280158, \"precision\": 1.0, \"recall\": 0.45248645987198427, \"specificity\": 1.0, \"npv\": 0.5073105892778024, \"accuracy\": 0.6498740554156172, \"f1\": 0.6230508474576271, \"f2\": 0.5081278336835121, \"f0_5\": 0.8051515682495182, \"p4\": 0.6471245911096051, \"phi\": 0.4791149889096385}, {\"truth_threshold\": 4.100000061094761, \"match_probability\": 0.9448986513716398, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 916.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1115.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.45100935499753814, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5489906450024619, \"precision\": 1.0, \"recall\": 0.45100935499753814, \"specificity\": 1.0, \"npv\": 0.5066371681415929, \"accuracy\": 0.6489294710327456, \"f1\": 0.6216491347132678, \"f2\": 0.5066371681415929, \"f0_5\": 0.8042142230026339, \"p4\": 0.6460941632868983, \"phi\": 0.4780147512591208}, {\"truth_threshold\": 4.200000062584877, \"match_probability\": 0.9483982147343843, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 915.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1116.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.4505169867060561, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5494830132939439, \"precision\": 1.0, \"recall\": 0.4505169867060561, \"specificity\": 1.0, \"npv\": 0.5064130915524104, \"accuracy\": 0.6486146095717884, \"f1\": 0.6211812627291242, \"f2\": 0.5061400597411218, \"f0_5\": 0.8039008961518187, \"p4\": 0.6457503166575753, \"phi\": 0.477648092254842}, {\"truth_threshold\": 4.300000064074993, \"match_probability\": 0.9516868803254299, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 911.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1120.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.448547513540128, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.551452486459872, \"precision\": 1.0, \"recall\": 0.448547513540128, \"specificity\": 1.0, \"npv\": 0.5055187637969095, \"accuracy\": 0.6473551637279596, \"f1\": 0.619306594153637, \"f2\": 0.5041505257332596, \"f0_5\": 0.8026431718061674, \"p4\": 0.6443730598755232, \"phi\": 0.47618188179411347}, {\"truth_threshold\": 4.400000065565109, \"match_probability\": 0.9547759482410569, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 908.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1123.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.44707040866568193, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5529295913343181, \"precision\": 1.0, \"recall\": 0.44707040866568193, \"specificity\": 1.0, \"npv\": 0.5048500881834215, \"accuracy\": 0.6464105793450882, \"f1\": 0.6178972439605308, \"f2\": 0.5026572187776793, \"f0_5\": 0.8016952145505916, \"p4\": 0.6433381357110801, \"phi\": 0.4750826614801553}, {\"truth_threshold\": 4.500000067055225, \"match_probability\": 0.9576762895591182, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 905.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1126.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.44559330379123585, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5544066962087641, \"precision\": 1.0, \"recall\": 0.44559330379123585, \"specificity\": 1.0, \"npv\": 0.5041831792162044, \"accuracy\": 0.6454659949622166, \"f1\": 0.6164850136239782, \"f2\": 0.5011629194816701, \"f0_5\": 0.8007432312864979, \"p4\": 0.6423014938325172, \"phi\": 0.47398380620324704}, {\"truth_threshold\": 4.6000000685453415, \"match_probability\": 0.9603983391922627, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 904.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1127.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.4451009354997538, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5548990645002462, \"precision\": 1.0, \"recall\": 0.4451009354997538, \"specificity\": 1.0, \"npv\": 0.5039612676056338, \"accuracy\": 0.6451511335012594, \"f1\": 0.6160136286201022, \"f2\": 0.5006645990252547, \"f0_5\": 0.8004250044271295, \"p4\": 0.6419555618126742, \"phi\": 0.47361760067264114}, {\"truth_threshold\": 4.700000070035458, \"match_probability\": 0.9629520927573305, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 902.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1129.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.4441161989167898, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5558838010832102, \"precision\": 1.0, \"recall\": 0.4441161989167898, \"specificity\": 1.0, \"npv\": 0.5035180299032542, \"accuracy\": 0.6445214105793451, \"f1\": 0.6150698943061712, \"f2\": 0.49966762685575006, \"f0_5\": 0.7997871963114027, \"p4\": 0.6412631167843771, \"phi\": 0.4728853069473651}, {\"truth_threshold\": 4.90000007301569, \"match_probability\": 0.9675925026740654, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 896.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1135.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.44116198916789756, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5588380108321024, \"precision\": 1.0, \"recall\": 0.44116198916789756, \"specificity\": 1.0, \"npv\": 0.5021929824561403, \"accuracy\": 0.642632241813602, \"f1\": 0.612230953194397, \"f2\": 0.49667405764966743, \"f0_5\": 0.7978628673196795, \"p4\": 0.6391810866147006, \"phi\": 0.47068934031536125}, {\"truth_threshold\": 5.000000074505806, \"match_probability\": 0.969696971214501, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 875.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1156.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.43082225504677496, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.569177744953225, \"precision\": 1.0, \"recall\": 0.43082225504677496, \"specificity\": 1.0, \"npv\": 0.49760973489787047, \"accuracy\": 0.6360201511335013, \"f1\": 0.6022023399862354, \"f2\": 0.4861651294588288, \"f0_5\": 0.7909962032182245, \"p4\": 0.631836685786, \"phi\": 0.463013334712866}, {\"truth_threshold\": 5.100000075995922, \"match_probability\": 0.9716687817966767, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 873.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1158.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.4298375184638109, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5701624815361891, \"precision\": 1.0, \"recall\": 0.4298375184638109, \"specificity\": 1.0, \"npv\": 0.49717759444203213, \"accuracy\": 0.6353904282115869, \"f1\": 0.6012396694214877, \"f2\": 0.4851617205735245, \"f0_5\": 0.7903313416621401, \"p4\": 0.6311324185690581, \"phi\": 0.46228301226712853}, {\"truth_threshold\": 5.200000077486038, \"match_probability\": 0.9735157914041783, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 863.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1168.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.42491383554899065, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5750861644510094, \"precision\": 1.0, \"recall\": 0.42491383554899065, \"specificity\": 1.0, \"npv\": 0.4950281020319931, \"accuracy\": 0.6322418136020151, \"f1\": 0.5964063579820318, \"f2\": 0.48013797707800154, \"f0_5\": 0.7869779317891665, \"p4\": 0.6275980948521758, \"phi\": 0.458633066338387}, {\"truth_threshold\": 5.300000078976154, \"match_probability\": 0.9752454557772836, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 861.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1170.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.4239290989660266, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5760709010339734, \"precision\": 1.0, \"recall\": 0.4239290989660266, \"specificity\": 1.0, \"npv\": 0.4946004319654428, \"accuracy\": 0.6316120906801007, \"f1\": 0.5954356846473029, \"f2\": 0.4791318864774624, \"f0_5\": 0.7863013698630137, \"p4\": 0.6268885921404044, \"phi\": 0.4579033909803657}, {\"truth_threshold\": 5.4000000804662704, \"match_probability\": 0.9768648415470134, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 846.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1185.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.41654357459379615, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5834564254062038, \"precision\": 1.0, \"recall\": 0.41654357459379615, \"specificity\": 1.0, \"npv\": 0.49141630901287553, \"accuracy\": 0.6268891687657431, \"f1\": 0.5881126173096975, \"f2\": 0.47157190635451507, \"f0_5\": 0.7811634349030471, \"p4\": 0.6215384467313602, \"phi\": 0.4524337586541401}, {\"truth_threshold\": 5.500000081956387, \"match_probability\": 0.9783806392104205, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 843.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1188.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.4150664697193501, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5849335302806499, \"precision\": 1.0, \"recall\": 0.4150664697193501, \"specificity\": 1.0, \"npv\": 0.49078439777111016, \"accuracy\": 0.6259445843828715, \"f1\": 0.5866388308977035, \"f2\": 0.47005687520910006, \"f0_5\": 0.7801221543586896, \"p4\": 0.620462167129168, \"phi\": 0.45134038970182133}, {\"truth_threshold\": 5.600000083446503, \"match_probability\": 0.9797991767207457, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 840.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1191.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.413589364844904, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5864106351550961, \"precision\": 1.0, \"recall\": 0.413589364844904, \"specificity\": 1.0, \"npv\": 0.4901541095890411, \"accuracy\": 0.625, \"f1\": 0.5851619644723093, \"f2\": 0.4685408299866131, \"f0_5\": 0.7790762381747357, \"p4\": 0.6193837556660524, \"phi\": 0.4502471841789251}, {\"truth_threshold\": 5.700000084936619, \"match_probability\": 0.9811264334957893, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 838.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1193.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.41260462826193994, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5873953717380601, \"precision\": 1.0, \"recall\": 0.41260462826193994, \"specificity\": 1.0, \"npv\": 0.4897348160821215, \"accuracy\": 0.6243702770780857, \"f1\": 0.5841756709654932, \"f2\": 0.4675295692925686, \"f0_5\": 0.7783763700538733, \"p4\": 0.6186636199117341, \"phi\": 0.4495184665133272}, {\"truth_threshold\": 5.800000086426735, \"match_probability\": 0.9823680546749124, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 837.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1194.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.4121122599704579, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5878877400295421, \"precision\": 1.0, \"recall\": 0.4121122599704579, \"specificity\": 1.0, \"npv\": 0.48952543822146216, \"accuracy\": 0.6240554156171285, \"f1\": 0.5836820083682008, \"f2\": 0.4670237696685638, \"f0_5\": 0.7780256553262688, \"p4\": 0.6183031912422774, \"phi\": 0.44915413240721225}, {\"truth_threshold\": 5.900000087916851, \"match_probability\": 0.9835293654795508, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 835.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1196.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.41112752338749387, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5888724766125062, \"precision\": 1.0, \"recall\": 0.41112752338749387, \"specificity\": 1.0, \"npv\": 0.4891072191371209, \"accuracy\": 0.6234256926952141, \"f1\": 0.5826936496859735, \"f2\": 0.4660118316776426, \"f0_5\": 0.7773226587227704, \"p4\": 0.6175816083638278, \"phi\": 0.4484255118464925}, {\"truth_threshold\": 6.000000089406967, \"match_probability\": 0.9846153855541349, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 833.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1198.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.4101427868045298, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5898572131954702, \"precision\": 1.0, \"recall\": 0.4101427868045298, \"specificity\": 1.0, \"npv\": 0.4886897140418267, \"accuracy\": 0.6227959697732998, \"f1\": 0.5817039106145251, \"f2\": 0.46499944177738084, \"f0_5\": 0.7766175647958232, \"p4\": 0.6168590527979635, \"phi\": 0.44769695241292806}, {\"truth_threshold\": 6.200000092387199, \"match_probability\": 0.9865801893041345, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 811.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1220.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.3993106843919252, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6006893156080748, \"precision\": 1.0, \"recall\": 0.3993106843919252, \"specificity\": 1.0, \"npv\": 0.48414376321353064, \"accuracy\": 0.6158690176322418, \"f1\": 0.5707248416608023, \"f2\": 0.45383324006715164, \"f0_5\": 0.7687203791469195, \"p4\": 0.6088448866523514, \"phi\": 0.43968599867732555}, {\"truth_threshold\": 6.400000095367432, \"match_probability\": 0.9882970460445225, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 803.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1228.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.3953717380600689, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.604628261939931, \"precision\": 1.0, \"recall\": 0.3953717380600689, \"specificity\": 1.0, \"npv\": 0.48251158870627897, \"accuracy\": 0.6133501259445844, \"f1\": 0.5666901905434015, \"f2\": 0.4497591576117397, \"f0_5\": 0.7657829486934961, \"p4\": 0.6058995526108901, \"phi\": 0.436773906570581}, {\"truth_threshold\": 6.500000096857548, \"match_probability\": 0.9890721936212699, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 801.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1230.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.39438700147710487, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6056129985228951, \"precision\": 1.0, \"recall\": 0.39438700147710487, \"specificity\": 1.0, \"npv\": 0.48210526315789476, \"accuracy\": 0.6127204030226701, \"f1\": 0.565677966101695, \"f2\": 0.44873949579831934, \"f0_5\": 0.7650429799426934, \"p4\": 0.6051605368383547, \"phi\": 0.43604592548626425}, {\"truth_threshold\": 6.600000098347664, \"match_probability\": 0.9897965292084853, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 800.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1231.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.39389463318562284, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6061053668143771, \"precision\": 1.0, \"recall\": 0.39389463318562284, \"specificity\": 1.0, \"npv\": 0.4819023569023569, \"accuracy\": 0.6124055415617129, \"f1\": 0.5651713175556341, \"f2\": 0.44822949350067237, \"f0_5\": 0.7646721468170522, \"p4\": 0.6047906217838517, \"phi\": 0.43568193915210784}, {\"truth_threshold\": 6.70000009983778, \"match_probability\": 0.9904733155885336, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 798.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1233.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.3929098966026588, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6070901033973413, \"precision\": 1.0, \"recall\": 0.3929098966026588, \"specificity\": 1.0, \"npv\": 0.48149705634987383, \"accuracy\": 0.6117758186397985, \"f1\": 0.5641569459172853, \"f2\": 0.44720914593140554, \"f0_5\": 0.7639287765651924, \"p4\": 0.6040499729033301, \"phi\": 0.4349539729958948}, {\"truth_threshold\": 6.800000101327896, \"match_probability\": 0.9911056147706719, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 795.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1236.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.3914327917282127, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6085672082717873, \"precision\": 1.0, \"recall\": 0.3914327917282127, \"specificity\": 1.0, \"npv\": 0.48089038219235614, \"accuracy\": 0.610831234256927, \"f1\": 0.5626326963906582, \"f2\": 0.44567776656575847, \"f0_5\": 0.7628094415659182, \"p4\": 0.6029369392641755, \"phi\": 0.4338620343113708}, {\"truth_threshold\": 6.900000102818012, \"match_probability\": 0.9916962992137202, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 781.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1250.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.3845396356474643, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6154603643525357, \"precision\": 1.0, \"recall\": 0.3845396356474643, \"specificity\": 1.0, \"npv\": 0.4780793319415449, \"accuracy\": 0.6064231738035264, \"f1\": 0.5554765291607396, \"f2\": 0.43851768669286917, \"f0_5\": 0.7575169738118331, \"p4\": 0.5977094083362298, \"phi\": 0.4287661974962401}, {\"truth_threshold\": 7.1000001057982445, \"match_probability\": 0.9927634299608046, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 778.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1253.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.3830625307730182, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6169374692269818, \"precision\": 1.0, \"recall\": 0.3830625307730182, \"specificity\": 1.0, \"npv\": 0.4774812343619683, \"accuracy\": 0.6054785894206549, \"f1\": 0.553933784264863, \"f2\": 0.43698045383059986, \"f0_5\": 0.756367878670037, \"p4\": 0.5965819146561916, \"phi\": 0.4276741400076935}, {\"truth_threshold\": 7.200000107288361, \"match_probability\": 0.9932447677519157, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 776.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1255.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.38207779419005417, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6179222058099458, \"precision\": 1.0, \"recall\": 0.38207779419005417, \"specificity\": 1.0, \"npv\": 0.47708333333333336, \"accuracy\": 0.6048488664987406, \"f1\": 0.5529034556465978, \"f2\": 0.43595505617977526, \"f0_5\": 0.7555988315481986, \"p4\": 0.5958287894168168, \"phi\": 0.4269460711200401}, {\"truth_threshold\": 7.300000108778477, \"match_probability\": 0.9936942928922654, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 775.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1256.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.38158542589857214, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6184145741014279, \"precision\": 1.0, \"recall\": 0.38158542589857214, \"specificity\": 1.0, \"npv\": 0.47688463140358184, \"accuracy\": 0.6045340050377834, \"f1\": 0.5523877405559515, \"f2\": 0.4354421845151141, \"f0_5\": 0.7552134086922627, \"p4\": 0.5954517850971813, \"phi\": 0.426582026319229}, {\"truth_threshold\": 7.400000110268593, \"match_probability\": 0.9941140817673122, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 774.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1257.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.3810930576070901, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6189069423929099, \"precision\": 1.0, \"recall\": 0.3810930576070901, \"specificity\": 1.0, \"npv\": 0.47668609492089925, \"accuracy\": 0.6042191435768262, \"f1\": 0.5518716577540107, \"f2\": 0.4349291975724882, \"f0_5\": 0.7548273844353423, \"p4\": 0.5950744850307271, \"phi\": 0.4262179740839059}, {\"truth_threshold\": 7.500000111758709, \"match_probability\": 0.9945060786121668, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 765.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1266.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.3766617429837518, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6233382570162481, \"precision\": 1.0, \"recall\": 0.3766617429837518, \"specificity\": 1.0, \"npv\": 0.47490667772708417, \"accuracy\": 0.6013853904282116, \"f1\": 0.5472103004291845, \"f2\": 0.43030712116098546, \"f0_5\": 0.7513258691809075, \"p4\": 0.591665315716949, \"phi\": 0.42294110344976693}, {\"truth_threshold\": 7.600000113248825, \"match_probability\": 0.9948721034855129, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 761.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1270.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.3746922698178237, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6253077301821762, \"precision\": 1.0, \"recall\": 0.3746922698178237, \"specificity\": 1.0, \"npv\": 0.474120082815735, \"accuracy\": 0.6001259445843828, \"f1\": 0.5451289398280802, \"f2\": 0.4282498593134496, \"f0_5\": 0.7497536945812808, \"p4\": 0.5901422282424653, \"phi\": 0.4214844362446167}, {\"truth_threshold\": 7.700000114738941, \"match_probability\": 0.9952138598197071, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 759.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1272.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.37370753323485967, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6262924667651403, \"precision\": 1.0, \"recall\": 0.37370753323485967, \"specificity\": 1.0, \"npv\": 0.47372776168804304, \"accuracy\": 0.5994962216624685, \"f1\": 0.5440860215053763, \"f2\": 0.4272205336035123, \"f0_5\": 0.7489638839550029, \"p4\": 0.5893788313133635, \"phi\": 0.42075602579797955}, {\"truth_threshold\": 7.800000116229057, \"match_probability\": 0.9955329415617687, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 756.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1275.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.3722304283604136, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6277695716395865, \"precision\": 1.0, \"recall\": 0.3722304283604136, \"specificity\": 1.0, \"npv\": 0.4731404958677686, \"accuracy\": 0.5985516372795969, \"f1\": 0.542518837459634, \"f2\": 0.42567567567567566, \"f0_5\": 0.7477744807121661, \"p4\": 0.5882313967029971, \"phi\": 0.41966330486655373}, {\"truth_threshold\": 7.900000117719173, \"match_probability\": 0.99583083992065, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 750.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1281.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.36927621861152143, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6307237813884786, \"precision\": 1.0, \"recall\": 0.36927621861152143, \"specificity\": 1.0, \"npv\": 0.47197032151690027, \"accuracy\": 0.5966624685138538, \"f1\": 0.5393743257820928, \"f2\": 0.42258282623394183, \"f0_5\": 0.7453786523553966, \"p4\": 0.5859280050558678, \"phi\": 0.41747744325487207}, {\"truth_threshold\": 8.00000011920929, \"match_probability\": 0.9961089497366072, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 739.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1292.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.3638601674052191, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6361398325947809, \"precision\": 1.0, \"recall\": 0.3638601674052191, \"specificity\": 1.0, \"npv\": 0.4698399671727534, \"accuracy\": 0.593198992443325, \"f1\": 0.5335740072202166, \"f2\": 0.41690172627778405, \"f0_5\": 0.7409264086625226, \"p4\": 0.5816749582556248, \"phi\": 0.41346831693509567}, {\"truth_threshold\": 8.200000122189522, \"match_probability\": 0.9966109369567457, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 737.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1294.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.36287543082225504, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.637124569177745, \"precision\": 1.0, \"recall\": 0.36287543082225504, \"specificity\": 1.0, \"npv\": 0.4694546945469455, \"accuracy\": 0.5925692695214105, \"f1\": 0.5325144508670521, \"f2\": 0.41586728360230224, \"f0_5\": 0.7401084555131553, \"p4\": 0.5808973960068617, \"phi\": 0.41273911195239665}, {\"truth_threshold\": 8.300000123679638, \"match_probability\": 0.9968371745531442, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 735.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1296.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.361890694239291, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.638109305760709, \"precision\": 1.0, \"recall\": 0.361890694239291, \"specificity\": 1.0, \"npv\": 0.46907005325686196, \"accuracy\": 0.5919395465994962, \"f1\": 0.5314533622559653, \"f2\": 0.4148323738570945, \"f0_5\": 0.7392878696439348, \"p4\": 0.580118494928293, \"phi\": 0.412009814470465}, {\"truth_threshold\": 8.600000128149986, \"match_probability\": 0.9974294610402847, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 726.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1305.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.35745937961595275, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6425406203840472, \"precision\": 1.0, \"recall\": 0.35745937961595275, \"specificity\": 1.0, \"npv\": 0.4673469387755102, \"accuracy\": 0.5891057934508817, \"f1\": 0.5266594124047879, \"f2\": 0.4101694915254237, \"f0_5\": 0.7355623100303952, \"p4\": 0.5765966357046058, \"phi\": 0.40872673854313535}, {\"truth_threshold\": 8.700000129640102, \"match_probability\": 0.997601189412643, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 725.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1306.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.35696701132447073, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6430329886755293, \"precision\": 1.0, \"recall\": 0.35696701132447073, \"specificity\": 1.0, \"npv\": 0.467156262749898, \"accuracy\": 0.5887909319899244, \"f1\": 0.5261248185776488, \"f2\": 0.40965080800090403, \"f0_5\": 0.7351450010139932, \"p4\": 0.5762035983008024, \"phi\": 0.4083618186551483}, {\"truth_threshold\": 8.900000132620335, \"match_probability\": 0.9979110654305032, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 720.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1311.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.35450516986706054, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6454948301329394, \"precision\": 1.0, \"recall\": 0.35450516986706054, \"specificity\": 1.0, \"npv\": 0.46620521172638435, \"accuracy\": 0.5872166246851386, \"f1\": 0.5234460196292258, \"f2\": 0.40705563093622793, \"f0_5\": 0.7330482590103848, \"p4\": 0.5742331672939941, \"phi\": 0.406536785267915}, {\"truth_threshold\": 9.00000013411045, \"match_probability\": 0.9980506824420605, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 715.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1316.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.3520433284096504, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6479566715903495, \"precision\": 1.0, \"recall\": 0.3520433284096504, \"specificity\": 1.0, \"npv\": 0.46525802519301096, \"accuracy\": 0.5856423173803527, \"f1\": 0.5207574654042243, \"f2\": 0.4044575178187578, \"f0_5\": 0.7309343692496422, \"p4\": 0.5722538908091974, \"phi\": 0.4047109879386135}, {\"truth_threshold\": 9.100000135600567, \"match_probability\": 0.9981809849551747, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 710.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1321.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.3495814869522403, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6504185130477598, \"precision\": 1.0, \"recall\": 0.3495814869522403, \"specificity\": 1.0, \"npv\": 0.4643146796431468, \"accuracy\": 0.5840680100755667, \"f1\": 0.5180591025173295, \"f2\": 0.4018564636631198, \"f0_5\": 0.7288031205091356, \"p4\": 0.5702656229859941, \"phi\": 0.4028843706616135}, {\"truth_threshold\": 9.200000137090683, \"match_probability\": 0.9983025921847976, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 709.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1322.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.34908911866075826, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6509108813392418, \"precision\": 1.0, \"recall\": 0.34908911866075826, \"specificity\": 1.0, \"npv\": 0.4641264693960276, \"accuracy\": 0.5837531486146096, \"f1\": 0.5175182481751824, \"f2\": 0.40133589946790443, \"f0_5\": 0.7283747688514486, \"p4\": 0.5698668774463831, \"phi\": 0.40251894383816106}, {\"truth_threshold\": 9.300000138580799, \"match_probability\": 0.9984160824655384, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 705.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1326.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.34711964549483015, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6528803545051699, \"precision\": 1.0, \"recall\": 0.34711964549483015, \"specificity\": 1.0, \"npv\": 0.4633751517604209, \"accuracy\": 0.5824937027707808, \"f1\": 0.5153508771929824, \"f2\": 0.399252463472647, \"f0_5\": 0.7266542980828695, \"p4\": 0.5682682154171067, \"phi\": 0.40105687677708557}, {\"truth_threshold\": 9.400000140070915, \"match_probability\": 0.9985219959137808, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 697.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1334.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.3431806991629739, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6568193008370261, \"precision\": 1.0, \"recall\": 0.3431806991629739, \"specificity\": 1.0, \"npv\": 0.4618797902379992, \"accuracy\": 0.5799748110831234, \"f1\": 0.5109970674486803, \"f2\": 0.39507992291123456, \"f0_5\": 0.7231790827972608, \"p4\": 0.5650529747120447, \"phi\": 0.39813091985316124}, {\"truth_threshold\": 9.600000143051147, \"match_probability\": 0.9987130764898899, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 691.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1340.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.34022648941408173, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6597735105859183, \"precision\": 1.0, \"recall\": 0.34022648941408173, \"specificity\": 1.0, \"npv\": 0.4607645875251509, \"accuracy\": 0.5780856423173804, \"f1\": 0.5077149155033064, \"f2\": 0.39194554736245035, \"f0_5\": 0.7205422314911366, \"p4\": 0.5626255551091374, \"phi\": 0.39593473964784837}, {\"truth_threshold\": 9.700000144541264, \"match_probability\": 0.9987991544181472, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 689.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1342.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.3392417528311177, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6607582471688823, \"precision\": 1.0, \"recall\": 0.3392417528311177, \"specificity\": 1.0, \"npv\": 0.46039404905508646, \"accuracy\": 0.577455919395466, \"f1\": 0.5066176470588235, \"f2\": 0.3908998071031431, \"f0_5\": 0.7196574054731565, \"p4\": 0.5618133159380493, \"phi\": 0.39520233323509507}, {\"truth_threshold\": 9.80000014603138, \"match_probability\": 0.9988794813467569, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 683.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1348.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.3362875430822255, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6637124569177745, \"precision\": 1.0, \"recall\": 0.3362875430822255, \"specificity\": 1.0, \"npv\": 0.4592860008022463, \"accuracy\": 0.575566750629723, \"f1\": 0.5033161385408991, \"f2\": 0.3877597365731804, \"f0_5\": 0.7169850934285115, \"p4\": 0.5593671566036346, \"phi\": 0.3930040213303783}, {\"truth_threshold\": 9.900000147521496, \"match_probability\": 0.9989544406735176, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 670.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1361.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.32988675529295913, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6701132447070408, \"precision\": 1.0, \"recall\": 0.32988675529295913, \"specificity\": 1.0, \"npv\": 0.45690343176376697, \"accuracy\": 0.5714735516372796, \"f1\": 0.49611255090707146, \"f2\": 0.38094155105753924, \"f0_5\": 0.7111016769263426, \"p4\": 0.5540174206937766, \"phi\": 0.38823496826891707}, {\"truth_threshold\": 10.000000149011612, \"match_probability\": 0.9990243903445719, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 664.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1367.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.326932545544067, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6730674544559331, \"precision\": 1.0, \"recall\": 0.326932545544067, \"specificity\": 1.0, \"npv\": 0.45581210191082805, \"accuracy\": 0.5695843828715366, \"f1\": 0.49276437847866417, \"f2\": 0.377787892580792, \"f0_5\": 0.7083422231704715, \"p4\": 0.5515247216052599, \"phi\": 0.3860308417309408}, {\"truth_threshold\": 10.100000150501728, \"match_probability\": 0.9990896645300149, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 657.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1374.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.32348596750369274, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6765140324963073, \"precision\": 1.0, \"recall\": 0.32348596750369274, \"specificity\": 1.0, \"npv\": 0.45454545454545453, \"accuracy\": 0.5673803526448362, \"f1\": 0.4888392857142857, \"f2\": 0.37410317731465664, \"f0_5\": 0.7050869285254346, \"p4\": 0.5485971943887775, \"phi\": 0.38345674611100816}, {\"truth_threshold\": 10.30000015348196, \"match_probability\": 0.9992074135451509, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 653.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1378.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.32151649433776464, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6784835056622354, \"precision\": 1.0, \"recall\": 0.32151649433776464, \"specificity\": 1.0, \"npv\": 0.453824811732065, \"accuracy\": 0.5661209068010076, \"f1\": 0.48658718330849476, \"f2\": 0.3719949868975732, \"f0_5\": 0.7032091320267069, \"p4\": 0.5469147740904808, \"phi\": 0.38198450559098546}, {\"truth_threshold\": 10.400000154972076, \"match_probability\": 0.9992604514366183, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 652.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1379.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.3210241260462826, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6789758739537174, \"precision\": 1.0, \"recall\": 0.3210241260462826, \"specificity\": 1.0, \"npv\": 0.45364500792393025, \"accuracy\": 0.5658060453400504, \"f1\": 0.48602310846067837, \"f2\": 0.3714676390154968, \"f0_5\": 0.702737658978228, \"p4\": 0.5464930695544453, \"phi\": 0.381616289227856}, {\"truth_threshold\": 10.500000156462193, \"match_probability\": 0.9993099426168967, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 650.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1381.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.32003938946331856, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6799606105366814, \"precision\": 1.0, \"recall\": 0.32003938946331856, \"specificity\": 1.0, \"npv\": 0.45328582739509105, \"accuracy\": 0.565176322418136, \"f1\": 0.484893696381947, \"f2\": 0.3704125826304992, \"f0_5\": 0.7017922694882315, \"p4\": 0.5456483308421441, \"phi\": 0.3808796653168821}, {\"truth_threshold\": 10.600000157952309, \"match_probability\": 0.9993561239419685, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 645.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1386.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.3175775480059084, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6824224519940916, \"precision\": 1.0, \"recall\": 0.3175775480059084, \"specificity\": 1.0, \"npv\": 0.4523903595416831, \"accuracy\": 0.5636020151133502, \"f1\": 0.4820627802690583, \"f2\": 0.3677728361272665, \"f0_5\": 0.6994144437215355, \"p4\": 0.5435286584827342, \"phi\": 0.37903696538036896}, {\"truth_threshold\": 10.700000159442425, \"match_probability\": 0.9993992164911604, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 628.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1403.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.30920728705071393, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.690792712949286, \"precision\": 1.0, \"recall\": 0.30920728705071393, \"specificity\": 1.0, \"npv\": 0.44937205651491363, \"accuracy\": 0.5582493702770781, \"f1\": 0.47235802933433624, \"f2\": 0.3587751371115174, \"f0_5\": 0.6911732335461149, \"p4\": 0.5362358748780472, \"phi\": 0.37275878859039197}, {\"truth_threshold\": 10.800000160932541, \"match_probability\": 0.9994394266126935, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 626.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1405.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.3082225504677499, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6917774495322502, \"precision\": 1.0, \"recall\": 0.3082225504677499, \"specificity\": 1.0, \"npv\": 0.44901960784313727, \"accuracy\": 0.5576196473551638, \"f1\": 0.47120812946932633, \"f2\": 0.3577142857142857, \"f0_5\": 0.6901874310915105, \"p4\": 0.5353689358368804, \"phi\": 0.3720187747136435}, {\"truth_threshold\": 10.900000162422657, \"match_probability\": 0.9994769469006325, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 617.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1414.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.30379123584441164, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6962087641555884, \"precision\": 1.0, \"recall\": 0.30379123584441164, \"specificity\": 1.0, \"npv\": 0.44744040640875343, \"accuracy\": 0.5547858942065491, \"f1\": 0.466012084592145, \"f2\": 0.35293444685962705, \"f0_5\": 0.6857079350966881, \"p4\": 0.5314436122804537, \"phi\": 0.3686847895284548}, {\"truth_threshold\": 11.000000163912773, \"match_probability\": 0.9995119571076428, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 615.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1416.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.30280649926144754, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6971935007385525, \"precision\": 1.0, \"recall\": 0.30280649926144754, \"specificity\": 1.0, \"npv\": 0.4470909800859039, \"accuracy\": 0.5541561712846348, \"f1\": 0.46485260770975056, \"f2\": 0.35187092344661863, \"f0_5\": 0.6847027388109552, \"p4\": 0.5305658842011279, \"phi\": 0.3679430044601774}, {\"truth_threshold\": 11.10000016540289, \"match_probability\": 0.9995446249976983, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 609.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1422.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.2998522895125554, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.7001477104874446, \"precision\": 1.0, \"recall\": 0.2998522895125554, \"specificity\": 1.0, \"npv\": 0.4460459680560966, \"accuracy\": 0.5522670025188917, \"f1\": 0.46136363636363636, \"f2\": 0.3486774304362762, \"f0_5\": 0.6816655473472129, \"p4\": 0.5279206148758528, \"phi\": 0.3657156063794171}, {\"truth_threshold\": 11.200000166893005, \"match_probability\": 0.9995751071426191, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 597.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1434.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.29394387001477107, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.7060561299852289, \"precision\": 1.0, \"recall\": 0.29394387001477107, \"specificity\": 1.0, \"npv\": 0.4439705312136487, \"accuracy\": 0.5484886649874056, \"f1\": 0.454337899543379, \"f2\": 0.34227726178190576, \"f0_5\": 0.6754921928038018, \"p4\": 0.5225744454433701, \"phi\": 0.3612511814755124}, {\"truth_threshold\": 11.300000168383121, \"match_probability\": 0.9996035496660847, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 590.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1441.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.29049729197439683, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.7095027080256031, \"precision\": 1.0, \"recall\": 0.29049729197439683, \"specificity\": 1.0, \"npv\": 0.44276875483372, \"accuracy\": 0.5462846347607053, \"f1\": 0.45020984357115607, \"f2\": 0.33853568969474407, \"f0_5\": 0.6718287406057846, \"p4\": 0.5194206063238911, \"phi\": 0.3586406617354916}, {\"truth_threshold\": 11.400000169873238, \"match_probability\": 0.99963008893853, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 572.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1459.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.2816346627277203, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.7183653372722797, \"precision\": 1.0, \"recall\": 0.2816346627277203, \"specificity\": 1.0, \"npv\": 0.4397081413210445, \"accuracy\": 0.5406171284634761, \"f1\": 0.4394928928159816, \"f2\": 0.32888684452621897, \"f0_5\": 0.6621903218337578, \"p4\": 0.5111863478106634, \"phi\": 0.35190489351468984}, {\"truth_threshold\": 11.500000171363354, \"match_probability\": 0.999654852226126, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 564.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1467.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.2776957163958641, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.7223042836041359, \"precision\": 1.0, \"recall\": 0.2776957163958641, \"specificity\": 1.0, \"npv\": 0.4383614088820827, \"accuracy\": 0.5380982367758187, \"f1\": 0.4346820809248555, \"f2\": 0.324585635359116, \"f0_5\": 0.6578026592022393, \"p4\": 0.5074669616635656, \"phi\": 0.34889982155313615}, {\"truth_threshold\": 11.60000017285347, \"match_probability\": 0.9996779582968373, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 562.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1469.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.27671097981290005, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.7232890201871, \"precision\": 1.0, \"recall\": 0.27671097981290005, \"specificity\": 1.0, \"npv\": 0.43802601377199696, \"accuracy\": 0.5374685138539043, \"f1\": 0.433474739683764, \"f2\": 0.3235090950955561, \"f0_5\": 0.6566954896003739, \"p4\": 0.5065311855080239, \"phi\": 0.3481473932896642}, {\"truth_threshold\": 11.700000174343586, \"match_probability\": 0.9996995179863626, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 544.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1487.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.2678483505662235, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.7321516494337764, \"precision\": 1.0, \"recall\": 0.2678483505662235, \"specificity\": 1.0, \"npv\": 0.4350303951367781, \"accuracy\": 0.531801007556675, \"f1\": 0.4225242718446602, \"f2\": 0.3137978772496539, \"f0_5\": 0.6465414784882338, \"p4\": 0.49799892585801936, \"phi\": 0.3413534440774818}, {\"truth_threshold\": 11.800000175833702, \"match_probability\": 0.9997196347265854, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 532.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1499.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.2619399310684392, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.7380600689315608, \"precision\": 1.0, \"recall\": 0.2619399310684392, \"specificity\": 1.0, \"npv\": 0.43305597579425115, \"accuracy\": 0.5280226700251889, \"f1\": 0.4151385095591104, \"f2\": 0.30730129390018485, \"f0_5\": 0.6395768213512864, \"p4\": 0.49219626037015723, \"phi\": 0.33680061230395913}, {\"truth_threshold\": 11.900000177323818, \"match_probability\": 0.9997384050389891, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 531.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1500.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.2614475627769572, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.7385524372230429, \"precision\": 1.0, \"recall\": 0.2614475627769572, \"specificity\": 1.0, \"npv\": 0.43289224952741023, \"accuracy\": 0.5277078085642317, \"f1\": 0.41451990632318503, \"f2\": 0.30675909878682844, \"f0_5\": 0.6389891696750902, \"p4\": 0.4917084177453204, \"phi\": 0.3364203079244411}, {\"truth_threshold\": 12.000000178813934, \"match_probability\": 0.9997559189953416, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 525.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1506.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.258493353028065, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.741506646971935, \"precision\": 1.0, \"recall\": 0.258493353028065, \"specificity\": 1.0, \"npv\": 0.43191248585439457, \"accuracy\": 0.5258186397984886, \"f1\": 0.4107981220657277, \"f2\": 0.3035032951786334, \"f0_5\": 0.635439360929557, \"p4\": 0.4887671967996878, \"phi\": 0.33413546157687174}, {\"truth_threshold\": 12.10000018030405, \"match_probability\": 0.9997722606477963, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 523.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1508.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.2575086164451009, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.7424913835548991, \"precision\": 1.0, \"recall\": 0.2575086164451009, \"specificity\": 1.0, \"npv\": 0.43158688277421786, \"accuracy\": 0.5251889168765743, \"f1\": 0.4095536413469068, \"f2\": 0.3024170232450561, \"f0_5\": 0.6342469075915595, \"p4\": 0.4877813355532549, \"phi\": 0.33337267593347064}, {\"truth_threshold\": 12.200000181794167, \"match_probability\": 0.9997875084304283, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 522.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1509.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.2570162481536189, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.7429837518463811, \"precision\": 1.0, \"recall\": 0.2570162481536189, \"specificity\": 1.0, \"npv\": 0.43142426525998495, \"accuracy\": 0.5248740554156172, \"f1\": 0.408930669800235, \"f2\": 0.3018736988202637, \"f0_5\": 0.6336489439184269, \"p4\": 0.4872873712640354, \"phi\": 0.3329910599694127}, {\"truth_threshold\": 12.300000183284283, \"match_probability\": 0.9998017355340825, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 517.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1514.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.25455440669620877, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.7454455933037912, \"precision\": 1.0, \"recall\": 0.25455440669620877, \"specificity\": 1.0, \"npv\": 0.43061301241068073, \"accuracy\": 0.5232997481108312, \"f1\": 0.40580847723704866, \"f2\": 0.2991551903714848, \"f0_5\": 0.6306416199072945, \"p4\": 0.4848071139822707, \"phi\": 0.331080715067894}, {\"truth_threshold\": 12.400000184774399, \"match_probability\": 0.9998150102562988, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 503.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1528.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.24766125061546038, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.7523387493845396, \"precision\": 1.0, \"recall\": 0.24766125061546038, \"specificity\": 1.0, \"npv\": 0.4283576505798728, \"accuracy\": 0.5188916876574308, \"f1\": 0.39700078926598265, \"f2\": 0.2915266025269503, \"f0_5\": 0.6220628246351719, \"p4\": 0.4777676111660919, \"phi\": 0.3257109016494715}, {\"truth_threshold\": 12.500000186264515, \"match_probability\": 0.9998273963279586, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 488.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1543.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.24027572624322993, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.7597242737567701, \"precision\": 1.0, \"recall\": 0.24027572624322993, \"specificity\": 1.0, \"npv\": 0.4259672619047619, \"accuracy\": 0.514168765743073, \"f1\": 0.38745533942040494, \"f2\": 0.2833255921969345, \"f0_5\": 0.6126035651518955, \"p4\": 0.47006378241252605, \"phi\": 0.3199212296956968}, {\"truth_threshold\": 12.600000187754631, \"match_probability\": 0.9998389532181915, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 485.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1546.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.23879862136878385, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.7612013786312162, \"precision\": 1.0, \"recall\": 0.23879862136878385, \"specificity\": 1.0, \"npv\": 0.42549238201412115, \"accuracy\": 0.5132241813602015, \"f1\": 0.38553259141494434, \"f2\": 0.2816819607387618, \"f0_5\": 0.6106774112314278, \"p4\": 0.4685021766274086, \"phi\": 0.31875852024360396}, {\"truth_threshold\": 12.700000189244747, \"match_probability\": 0.9998497364189812, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 477.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1554.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.23485967503692762, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.7651403249630724, \"precision\": 1.0, \"recall\": 0.23485967503692762, \"specificity\": 1.0, \"npv\": 0.42423119673953313, \"accuracy\": 0.510705289672544, \"f1\": 0.3803827751196172, \"f2\": 0.27729333798395533, \"f0_5\": 0.6054836252856055, \"p4\": 0.46430283802256545, \"phi\": 0.31564980755066785}, {\"truth_threshold\": 12.800000190734863, \"match_probability\": 0.9998597977108138, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 457.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1574.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.22501230920728704, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.7749876907927129, \"precision\": 1.0, \"recall\": 0.22501230920728704, \"specificity\": 1.0, \"npv\": 0.42111070246414123, \"accuracy\": 0.5044080604534005, \"f1\": 0.36736334405144694, \"f2\": 0.2662859806549353, \"f0_5\": 0.592122311479658, \"p4\": 0.45357265449199585, \"phi\": 0.30782314986589165}, {\"truth_threshold\": 12.90000019222498, \"match_probability\": 0.9998691854106266, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 450.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1581.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.22156573116691286, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.7784342688330872, \"precision\": 1.0, \"recall\": 0.22156573116691286, \"specificity\": 1.0, \"npv\": 0.42002934702861333, \"accuracy\": 0.5022040302267002, \"f1\": 0.36275695284159615, \"f2\": 0.2624212736179146, \"f0_5\": 0.5873140172278778, \"p4\": 0.44973536314330326, \"phi\": 0.3050641070102409}, {\"truth_threshold\": 13.000000193715096, \"match_probability\": 0.9998779446032292, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 436.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1595.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.21467257508616444, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.7853274249138356, \"precision\": 1.0, \"recall\": 0.21467257508616444, \"specificity\": 1.0, \"npv\": 0.41788321167883213, \"accuracy\": 0.49779596977329976, \"f1\": 0.3534657478719092, \"f2\": 0.2546728971962617, \"f0_5\": 0.5774834437086093, \"p4\": 0.4419269349297068, \"phi\": 0.2995130466880727}, {\"truth_threshold\": 13.100000195205212, \"match_probability\": 0.9998861173572945, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 430.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1601.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.21171836533727229, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.7882816346627277, \"precision\": 1.0, \"recall\": 0.21171836533727229, \"specificity\": 1.0, \"npv\": 0.4169701383831027, \"accuracy\": 0.4959068010075567, \"f1\": 0.34945144250304755, \"f2\": 0.2513444002805705, \"f0_5\": 0.5731804852039456, \"p4\": 0.4385239285455831, \"phi\": 0.2971199018795724}, {\"truth_threshold\": 13.300000198185444, \"match_probability\": 0.9999008579398913, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 420.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1611.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.206794682422452, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.793205317577548, \"precision\": 1.0, \"recall\": 0.206794682422452, \"specificity\": 1.0, \"npv\": 0.41545718432510886, \"accuracy\": 0.4927581863979849, \"f1\": 0.3427172582619339, \"f2\": 0.24578651685393257, \"f0_5\": 0.5658852061438965, \"p4\": 0.43277391747463057, \"phi\": 0.29311147451547676}, {\"truth_threshold\": 13.500000201165676, \"match_probability\": 0.9999136907162209, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 416.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1615.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.2048252092565239, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.7951747907434761, \"precision\": 1.0, \"recall\": 0.2048252092565239, \"specificity\": 1.0, \"npv\": 0.4148550724637681, \"accuracy\": 0.49149874055415615, \"f1\": 0.340008173273396, \"f2\": 0.24355971896955503, \"f0_5\": 0.5629228687415426, \"p4\": 0.43044577914486043, \"phi\": 0.29150090399263207}, {\"truth_threshold\": 13.600000202655792, \"match_probability\": 0.9999194701253888, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 414.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1617.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.2038404726735598, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.7961595273264401, \"precision\": 1.0, \"recall\": 0.2038404726735598, \"specificity\": 1.0, \"npv\": 0.41455467052860245, \"accuracy\": 0.49086901763224183, \"f1\": 0.33865030674846625, \"f2\": 0.24244553759662685, \"f0_5\": 0.5614320585842149, \"p4\": 0.42927555505144527, \"phi\": 0.29069403156855866}, {\"truth_threshold\": 13.700000204145908, \"match_probability\": 0.9999248625650565, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 411.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1620.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.20236336779911374, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.7976366322008862, \"precision\": 1.0, \"recall\": 0.20236336779911374, \"specificity\": 1.0, \"npv\": 0.4141048824593128, \"accuracy\": 0.4899244332493703, \"f1\": 0.3366093366093366, \"f2\": 0.24077328646748683, \"f0_5\": 0.5591836734693878, \"p4\": 0.42751243442120324, \"phi\": 0.2894817069117195}, {\"truth_threshold\": 13.800000205636024, \"match_probability\": 0.999929893941616, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 404.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1627.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.19891678975873953, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8010832102412605, \"precision\": 1.0, \"recall\": 0.19891678975873953, \"specificity\": 1.0, \"npv\": 0.41305916305916307, \"accuracy\": 0.48772040302267, \"f1\": 0.33182751540041067, \"f2\": 0.23686679174484052, \"f0_5\": 0.5538799012887304, \"p4\": 0.4233615823022448, \"phi\": 0.28664333708663187}, {\"truth_threshold\": 13.90000020712614, \"match_probability\": 0.9999345884275949, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 399.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1632.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.1964549483013294, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8035450516986706, \"precision\": 1.0, \"recall\": 0.1964549483013294, \"specificity\": 1.0, \"npv\": 0.41231544832553113, \"accuracy\": 0.48614609571788414, \"f1\": 0.32839506172839505, \"f2\": 0.23407250967969026, \"f0_5\": 0.5500413564929694, \"p4\": 0.42036442533034846, \"phi\": 0.28460746667055603}, {\"truth_threshold\": 14.000000208616257, \"match_probability\": 0.9999389685776376, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 386.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1645.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.19005416051206303, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.809945839487937, \"precision\": 1.0, \"recall\": 0.19005416051206303, \"specificity\": 1.0, \"npv\": 0.4103942652329749, \"accuracy\": 0.4820528967254408, \"f1\": 0.31940422010757136, \"f2\": 0.22679200940070504, \"f0_5\": 0.5398601398601398, \"p4\": 0.4124417485637631, \"phi\": 0.2792796762348059}, {\"truth_threshold\": 14.100000210106373, \"match_probability\": 0.9999430554367367, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 371.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1660.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.18266863613983259, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8173313638601674, \"precision\": 1.0, \"recall\": 0.18266863613983259, \"specificity\": 1.0, \"npv\": 0.40819964349376114, \"accuracy\": 0.47732997481108314, \"f1\": 0.3089092422980849, \"f2\": 0.2183637433784579, \"f0_5\": 0.5277382645803699, \"p4\": 0.4030561657028185, \"phi\": 0.27306642442777773}, {\"truth_threshold\": 14.200000211596489, \"match_probability\": 0.9999468686412301, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 368.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1663.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.1811915312653865, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8188084687346134, \"precision\": 1.0, \"recall\": 0.1811915312653865, \"specificity\": 1.0, \"npv\": 0.4077635327635328, \"accuracy\": 0.4763853904282116, \"f1\": 0.3067944977073781, \"f2\": 0.2166745171926519, \"f0_5\": 0.5252640593776763, \"p4\": 0.40114633639560937, \"phi\": 0.2718148246428221}, {\"truth_threshold\": 14.300000213086605, \"match_probability\": 0.9999504265130488, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 361.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1670.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.1777449532250123, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8222550467749877, \"precision\": 1.0, \"recall\": 0.1777449532250123, \"specificity\": 1.0, \"npv\": 0.4067495559502664, \"accuracy\": 0.47418136020151136, \"f1\": 0.30183946488294316, \"f2\": 0.21272834413671185, \"f0_5\": 0.5194244604316547, \"p4\": 0.396646195182804, \"phi\": 0.26888228055540336}, {\"truth_threshold\": 14.400000214576721, \"match_probability\": 0.9999537461476637, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 360.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1671.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.17725258493353027, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8227474150664698, \"precision\": 1.0, \"recall\": 0.17725258493353027, \"specificity\": 1.0, \"npv\": 0.40660511363636365, \"accuracy\": 0.47386649874055414, \"f1\": 0.30112923462986196, \"f2\": 0.21216407355021216, \"f0_5\": 0.5185825410544511, \"p4\": 0.3959982275188506, \"phi\": 0.268461929217603}, {\"truth_threshold\": 14.500000216066837, \"match_probability\": 0.9999568434961527, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 352.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1679.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.17331363860167406, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.826686361398326, \"precision\": 1.0, \"recall\": 0.17331363860167406, \"specificity\": 1.0, \"npv\": 0.4054532577903683, \"accuracy\": 0.47134760705289674, \"f1\": 0.2954259336970206, \"f2\": 0.20764511562057575, \"f0_5\": 0.5117766792672288, \"p4\": 0.3907676489102373, \"phi\": 0.26508598490027957}, {\"truth_threshold\": 14.600000217556953, \"match_probability\": 0.9999597334417798, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 350.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1681.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.17232890201870998, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.82767109798129, \"precision\": 1.0, \"recall\": 0.17232890201870998, \"specificity\": 1.0, \"npv\": 0.4051663128096249, \"accuracy\": 0.47071788413098237, \"f1\": 0.2939941201175976, \"f2\": 0.20651404295492093, \"f0_5\": 0.5100553774409793, \"p4\": 0.389446793623568, \"phi\": 0.26423827470949746}, {\"truth_threshold\": 14.900000222027302, \"match_probability\": 0.9999672931444318, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 347.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1684.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.1708517971442639, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8291482028557361, \"precision\": 1.0, \"recall\": 0.1708517971442639, \"specificity\": 1.0, \"npv\": 0.4047366560622128, \"accuracy\": 0.46977329974811083, \"f1\": 0.29184188393608074, \"f2\": 0.20481643253452958, \"f0_5\": 0.5074583211465341, \"p4\": 0.38745542187943405, \"phi\": 0.2629638474360856}, {\"truth_threshold\": 15.000000223517418, \"match_probability\": 0.9999694833578969, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 346.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1685.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.17035942885278188, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8296405711472181, \"precision\": 1.0, \"recall\": 0.17035942885278188, \"specificity\": 1.0, \"npv\": 0.4045936395759717, \"accuracy\": 0.46945843828715367, \"f1\": 0.29112326461926796, \"f2\": 0.20425029515938606, \"f0_5\": 0.5065885797950219, \"p4\": 0.3867889182734259, \"phi\": 0.262538266459636}, {\"truth_threshold\": 15.100000225007534, \"match_probability\": 0.9999715269079685, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 345.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1686.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.16986706056129985, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8301329394387001, \"precision\": 1.0, \"recall\": 0.16986706056129985, \"specificity\": 1.0, \"npv\": 0.4044507241257506, \"accuracy\": 0.46914357682619645, \"f1\": 0.2904040404040404, \"f2\": 0.2036840240878498, \"f0_5\": 0.5057167985927881, \"p4\": 0.3861210486188493, \"phi\": 0.26211229587550916}, {\"truth_threshold\": 15.20000022649765, \"match_probability\": 0.9999734336151354, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 341.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1690.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.16789758739537175, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8321024126046282, \"precision\": 1.0, \"recall\": 0.16789758739537175, \"specificity\": 1.0, \"npv\": 0.4038800705467372, \"accuracy\": 0.46788413098236775, \"f1\": 0.2875210792580101, \"f2\": 0.20141760189013586, \"f0_5\": 0.5022091310751104, \"p4\": 0.38343579369132264, \"phi\": 0.2604044727762365}, {\"truth_threshold\": 15.300000227987766, \"match_probability\": 0.9999752126423825, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 336.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1695.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.1654357459379616, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8345642540620384, \"precision\": 1.0, \"recall\": 0.1654357459379616, \"specificity\": 1.0, \"npv\": 0.40316901408450706, \"accuracy\": 0.4663098236775819, \"f1\": 0.28390367553865653, \"f2\": 0.19858156028368795, \"f0_5\": 0.49777777777777776, \"p4\": 0.38004778751227103, \"phi\": 0.2582606562838075}, {\"truth_threshold\": 15.400000229477882, \"match_probability\": 0.9999768725392036, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 335.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1696.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.16494337764647957, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8350566223535204, \"precision\": 1.0, \"recall\": 0.16494337764647957, \"specificity\": 1.0, \"npv\": 0.40302710313269974, \"accuracy\": 0.4659949622166247, \"f1\": 0.28317836010143704, \"f2\": 0.19801394963943728, \"f0_5\": 0.4968851972708395, \"p4\": 0.3793659349517108, \"phi\": 0.2578306647274206}, {\"truth_threshold\": 15.500000230967999, \"match_probability\": 0.9999784212826682, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 326.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1705.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.1605120630231413, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8394879369768586, \"precision\": 1.0, \"recall\": 0.1605120630231413, \"specificity\": 1.0, \"npv\": 0.4017543859649123, \"accuracy\": 0.46316120906801006, \"f1\": 0.2766228256257955, \"f2\": 0.19289940828402366, \"f0_5\": 0.48875562218890556, \"p4\": 0.37316411468245886, \"phi\": 0.2539417754522155}, {\"truth_threshold\": 15.600000232458115, \"match_probability\": 0.9999798663157408, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 321.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1710.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.15805022156573117, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8419497784342689, \"precision\": 1.0, \"recall\": 0.15805022156573117, \"specificity\": 1.0, \"npv\": 0.4010507880910683, \"accuracy\": 0.4615869017632242, \"f1\": 0.2729591836734694, \"f2\": 0.19005328596802842, \"f0_5\": 0.4841628959276018, \"p4\": 0.369666887936757, \"phi\": 0.2517660936601759}, {\"truth_threshold\": 15.70000023394823, \"match_probability\": 0.9999812145830361, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 319.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1712.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.15706548498276712, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8429345150172329, \"precision\": 1.0, \"recall\": 0.15706548498276712, \"specificity\": 1.0, \"npv\": 0.4007700385019251, \"accuracy\": 0.4609571788413098, \"f1\": 0.2714893617021277, \"f2\": 0.1889138931659363, \"f0_5\": 0.4823102509827638, \"p4\": 0.3682574026034257, \"phi\": 0.2508926871470492}, {\"truth_threshold\": 15.800000235438347, \"match_probability\": 0.9999824725641815, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 316.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1715.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.15558838010832102, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.844411619891679, \"precision\": 1.0, \"recall\": 0.15558838010832102, \"specificity\": 1.0, \"npv\": 0.40034965034965037, \"accuracy\": 0.4600125944584383, \"f1\": 0.26927993182786536, \"f2\": 0.1872037914691943, \"f0_5\": 0.4795144157814871, \"p4\": 0.366131657936778, \"phi\": 0.2495791529251488}, {\"truth_threshold\": 15.900000236928463, \"match_probability\": 0.9999836463049459, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 313.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1718.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.15411127523387494, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8458887247661251, \"precision\": 1.0, \"recall\": 0.15411127523387494, \"specificity\": 1.0, \"npv\": 0.39993014320642684, \"accuracy\": 0.45906801007556675, \"f1\": 0.26706484641638223, \"f2\": 0.18549247362806684, \"f0_5\": 0.4766981419433445, \"p4\": 0.3639919317161557, \"phi\": 0.24826144359124447}, {\"truth_threshold\": 16.00000023841858, \"match_probability\": 0.9999847414462861, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 311.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1720.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.1531265386509109, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8468734613490891, \"precision\": 1.0, \"recall\": 0.1531265386509109, \"specificity\": 1.0, \"npv\": 0.39965095986038396, \"accuracy\": 0.45843828715365237, \"f1\": 0.2655849701110162, \"f2\": 0.18435091879075283, \"f0_5\": 0.47480916030534354, \"p4\": 0.3625575890243592, \"phi\": 0.2473806139371772}, {\"truth_threshold\": 16.100000239908695, \"match_probability\": 0.9999857632514492, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 307.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1724.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.15115706548498276, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8488429345150172, \"precision\": 1.0, \"recall\": 0.15115706548498276, \"specificity\": 1.0, \"npv\": 0.399093760892297, \"accuracy\": 0.45717884130982367, \"f1\": 0.262617621899059, \"f2\": 0.18206618431977226, \"f0_5\": 0.4710033752684873, \"p4\": 0.35966979322171594, \"phi\": 0.24561319539032303}, {\"truth_threshold\": 16.20000024139881, \"match_probability\": 0.9999867166312594, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 303.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1728.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.14918759231905465, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8508124076809453, \"precision\": 1.0, \"recall\": 0.14918759231905465, \"specificity\": 1.0, \"npv\": 0.39853811347024015, \"accuracy\": 0.45591939546599497, \"f1\": 0.2596401028277635, \"f2\": 0.17977928088287648, \"f0_5\": 0.4671600370027752, \"p4\": 0.3567561397717773, \"phi\": 0.24383794125607963}, {\"truth_threshold\": 16.300000242888927, \"match_probability\": 0.9999876061677141, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 300.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1731.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.14771048744460857, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8522895125553914, \"precision\": 1.0, \"recall\": 0.14771048744460857, \"specificity\": 1.0, \"npv\": 0.3981223922114047, \"accuracy\": 0.45497481108312343, \"f1\": 0.2574002574002574, \"f2\": 0.17806267806267806, \"f0_5\": 0.46425255338904364, \"p4\": 0.3545536533347784, \"phi\": 0.24250124250436372}, {\"truth_threshold\": 16.400000244379044, \"match_probability\": 0.9999884361359999, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 296.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1735.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.14574101427868044, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8542589857213195, \"precision\": 1.0, \"recall\": 0.14574101427868044, \"specificity\": 1.0, \"npv\": 0.3975694444444444, \"accuracy\": 0.45371536523929473, \"f1\": 0.25440481306403095, \"f2\": 0.17577197149643706, \"f0_5\": 0.4603421461897356, \"p4\": 0.35159363195589, \"phi\": 0.240711807104564}, {\"truth_threshold\": 16.50000024586916, \"match_probability\": 0.9999892105250341, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 295.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1736.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.14524864598719842, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8547513540128016, \"precision\": 1.0, \"recall\": 0.14524864598719842, \"specificity\": 1.0, \"npv\": 0.3974314474140923, \"accuracy\": 0.4534005037783375, \"f1\": 0.25365434221840066, \"f2\": 0.17519895474521915, \"f0_5\": 0.4593584553098723, \"p4\": 0.3508494030029032, \"phi\": 0.2402631465906275}, {\"truth_threshold\": 16.600000247359276, \"match_probability\": 0.9999899330566321, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 289.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1742.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.14229443623830626, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8577055637616937, \"precision\": 1.0, \"recall\": 0.14229443623830626, \"specificity\": 1.0, \"npv\": 0.39660547280914443, \"accuracy\": 0.45151133501259444, \"f1\": 0.24913793103448276, \"f2\": 0.17175799358136218, \"f0_5\": 0.45340445560087855, \"p4\": 0.346347962973042, \"phi\": 0.2375599969742467}, {\"truth_threshold\": 16.700000248849392, \"match_probability\": 0.9999906072033913, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 286.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1745.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.14081733136386015, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8591826686361398, \"precision\": 1.0, \"recall\": 0.14081733136386015, \"specificity\": 1.0, \"npv\": 0.3961937716262976, \"accuracy\": 0.4505667506297229, \"f1\": 0.2468709538195943, \"f2\": 0.1700356718192628, \"f0_5\": 0.4503937007874016, \"p4\": 0.34407370681446553, \"phi\": 0.236201078793891}, {\"truth_threshold\": 16.800000250339508, \"match_probability\": 0.9999912362053778, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 284.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1747.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.1398325947808961, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8601674052191038, \"precision\": 1.0, \"recall\": 0.1398325947808961, \"specificity\": 1.0, \"npv\": 0.39591977869986167, \"accuracy\": 0.4499370277078086, \"f1\": 0.24535637149028078, \"f2\": 0.16888677450047573, \"f0_5\": 0.4483738553836438, \"p4\": 0.34254867358249524, \"phi\": 0.23529235002583448}, {\"truth_threshold\": 17.00000025331974, \"match_probability\": 0.9999923706650156, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 283.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1748.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.13934022648941408, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8606597735105859, \"precision\": 1.0, \"recall\": 0.13934022648941408, \"specificity\": 1.0, \"npv\": 0.3957829243000346, \"accuracy\": 0.44962216624685136, \"f1\": 0.2445980985306828, \"f2\": 0.16831212085167122, \"f0_5\": 0.44736010116977554, \"p4\": 0.3417834740225152, \"phi\": 0.23483713997706887}, {\"truth_threshold\": 17.100000254809856, \"match_probability\": 0.9999928815751264, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 278.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1753.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.13687838503200395, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8631216149679961, \"precision\": 1.0, \"recall\": 0.13687838503200395, \"specificity\": 1.0, \"npv\": 0.3951000690131125, \"accuracy\": 0.4480478589420655, \"f1\": 0.2407968817669987, \"f2\": 0.1654368007617234, \"f0_5\": 0.44225262488068723, \"p4\": 0.3379303187916212, \"phi\": 0.23255248734973388}, {\"truth_threshold\": 17.200000256299973, \"match_probability\": 0.999993358271586, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 276.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1755.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.1358936484490399, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8641063515509602, \"precision\": 1.0, \"recall\": 0.1358936484490399, \"specificity\": 1.0, \"npv\": 0.39482758620689656, \"accuracy\": 0.4474181360201511, \"f1\": 0.23927178153446033, \"f2\": 0.16428571428571428, \"f0_5\": 0.44019138755980863, \"p4\": 0.33637622790175986, \"phi\": 0.23163454232472105}, {\"truth_threshold\": 17.30000025779009, \"match_probability\": 0.999993803045519, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 271.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1760.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.13343180699162974, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8665681930083703, \"precision\": 1.0, \"recall\": 0.13343180699162974, \"specificity\": 1.0, \"npv\": 0.39414802065404475, \"accuracy\": 0.44584382871536526, \"f1\": 0.23544743701129453, \"f2\": 0.16140559857057774, \"f0_5\": 0.434991974317817, \"p4\": 0.33245833489583054, \"phi\": 0.22932920140715485}, {\"truth_threshold\": 17.400000259280205, \"match_probability\": 0.9999942180346287, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 268.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1763.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.13195470211718366, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8680452978828164, \"precision\": 1.0, \"recall\": 0.13195470211718366, \"specificity\": 1.0, \"npv\": 0.3937414030261348, \"accuracy\": 0.4448992443324937, \"f1\": 0.23314484558503698, \"f2\": 0.15967588179218303, \"f0_5\": 0.43184015468901066, \"p4\": 0.3300848527615133, \"phi\": 0.22793865303523134}, {\"truth_threshold\": 17.50000026077032, \"match_probability\": 0.9999946052334694, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 262.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1769.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.12900049236829148, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8709995076317085, \"precision\": 1.0, \"recall\": 0.12900049236829148, \"specificity\": 1.0, \"npv\": 0.39293067947838023, \"accuracy\": 0.44301007556675065, \"f1\": 0.2285215874400349, \"f2\": 0.1562127355115669, \"f0_5\": 0.425462812601494, \"p4\": 0.32528558993728013, \"phi\": 0.22514051416686065}, {\"truth_threshold\": 17.600000262260437, \"match_probability\": 0.999994966503032, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 256.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1775.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.12604628261939932, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8739537173806007, \"precision\": 1.0, \"recall\": 0.12604628261939932, \"specificity\": 1.0, \"npv\": 0.3921232876712329, \"accuracy\": 0.4411209068010076, \"f1\": 0.22387407083515523, \"f2\": 0.15274463007159905, \"f0_5\": 0.41898527004909986, \"p4\": 0.3204149478514069, \"phi\": 0.22231887625538288}, {\"truth_threshold\": 17.700000263750553, \"match_probability\": 0.9999953035796879, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 250.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1781.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.12309207287050714, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8769079271294928, \"precision\": 1.0, \"recall\": 0.12309207287050714, \"specificity\": 1.0, \"npv\": 0.3913192071086808, \"accuracy\": 0.4392317380352645, \"f1\": 0.21920210434020165, \"f2\": 0.14927155481251492, \"f0_5\": 0.41240514681623225, \"p4\": 0.3154708850680329, \"phi\": 0.219472759943121}, {\"truth_threshold\": 17.80000026524067, \"match_probability\": 0.9999956180835331, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 246.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1785.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.12112259970457903, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8788774002954209, \"precision\": 1.0, \"recall\": 0.12112259970457903, \"specificity\": 1.0, \"npv\": 0.39078498293515357, \"accuracy\": 0.43797229219143574, \"f1\": 0.2160737812911726, \"f2\": 0.14695340501792115, \"f0_5\": 0.4079601990049751, \"p4\": 0.31213301141534483, \"phi\": 0.2175612397892036}, {\"truth_threshold\": 17.900000266730785, \"match_probability\": 0.9999959115261747, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 242.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1789.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.11915312653865091, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.880846873461349, \"precision\": 1.0, \"recall\": 0.11915312653865091, \"specificity\": 1.0, \"npv\": 0.39025221540558963, \"accuracy\": 0.43671284634760704, \"f1\": 0.21293444786625604, \"f2\": 0.14463303848912265, \"f0_5\": 0.40346782260753583, \"p4\": 0.30876091977525716, \"phi\": 0.21563805694777322}, {\"truth_threshold\": 18.0000002682209, \"match_probability\": 0.9999961853179954, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 239.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1792.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.11767602166420482, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8823239783357951, \"precision\": 1.0, \"recall\": 0.11767602166420482, \"specificity\": 1.0, \"npv\": 0.38985359210078313, \"accuracy\": 0.4357682619647355, \"f1\": 0.2105726872246696, \"f2\": 0.14289130694726773, \"f0_5\": 0.4000669568128557, \"p4\": 0.3062089971097263, \"phi\": 0.21418781419567226}, {\"truth_threshold\": 18.200000271201134, \"match_probability\": 0.9999966791247992, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 233.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1798.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.11472181191531265, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8852781880846874, \"precision\": 1.0, \"recall\": 0.11472181191531265, \"specificity\": 1.0, \"npv\": 0.3890587835541964, \"accuracy\": 0.4338790931989924, \"f1\": 0.2058303886925795, \"f2\": 0.13940409237764748, \"f0_5\": 0.3931825852176848, \"p4\": 0.30104514125543336, \"phi\": 0.21126648714574883}, {\"truth_threshold\": 18.30000027269125, \"match_probability\": 0.999996901513191, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 232.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1799.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.11422944362383063, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8857705563761694, \"precision\": 1.0, \"recall\": 0.11422944362383063, \"specificity\": 1.0, \"npv\": 0.3889266304347826, \"accuracy\": 0.43356423173803527, \"f1\": 0.20503756076005303, \"f2\": 0.13882240306366683, \"f0_5\": 0.39202433254477864, \"p4\": 0.3001765927459395, \"phi\": 0.21077683128146796}, {\"truth_threshold\": 18.400000274181366, \"match_probability\": 0.9999971090089864, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 229.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1802.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.11275233874938453, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8872476612506155, \"precision\": 1.0, \"recall\": 0.11275233874938453, \"specificity\": 1.0, \"npv\": 0.38853070919579236, \"accuracy\": 0.43261964735516373, \"f1\": 0.20265486725663717, \"f2\": 0.1370764994612714, \"f0_5\": 0.38853070919579236, \"p4\": 0.29755717255717257, \"phi\": 0.20930300078542255}, {\"truth_threshold\": 18.500000275671482, \"match_probability\": 0.9999973026094866, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 224.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1807.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.11029049729197439, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8897095027080256, \"precision\": 1.0, \"recall\": 0.11029049729197439, \"specificity\": 1.0, \"npv\": 0.38787262872628725, \"accuracy\": 0.4310453400503778, \"f1\": 0.19866962305986696, \"f2\": 0.13416387158600862, \"f0_5\": 0.38264434574649814, \"p4\": 0.29314487543657275, \"phi\": 0.20683003918233825}, {\"truth_threshold\": 18.600000277161598, \"match_probability\": 0.999997483245208, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 221.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1810.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.10881339241752831, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8911866075824717, \"precision\": 1.0, \"recall\": 0.10881339241752831, \"specificity\": 1.0, \"npv\": 0.38747884940778343, \"accuracy\": 0.4301007556675063, \"f1\": 0.19626998223801065, \"f2\": 0.1324146195326543, \"f0_5\": 0.379073756432247, \"p4\": 0.290469029799348, \"phi\": 0.20533603700788008}, {\"truth_threshold\": 18.700000278651714, \"match_probability\": 0.9999976517843541, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 218.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1813.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.10733628754308222, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8926637124569178, \"precision\": 1.0, \"recall\": 0.10733628754308222, \"specificity\": 1.0, \"npv\": 0.3870858688302907, \"accuracy\": 0.42915617128463474, \"f1\": 0.19386393952867942, \"f2\": 0.13066410932630065, \"f0_5\": 0.3754736479503961, \"p4\": 0.28777144299895174, \"phi\": 0.2038341485635611}, {\"truth_threshold\": 18.80000028014183, \"match_probability\": 0.9999978090369889, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 212.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1819.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.10438207779419005, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8956179222058099, \"precision\": 1.0, \"recall\": 0.10438207779419005, \"specificity\": 1.0, \"npv\": 0.38630229419703105, \"accuracy\": 0.4272670025188917, \"f1\": 0.18903254569772626, \"f2\": 0.12715930902111325, \"f0_5\": 0.3681833970128517, \"p4\": 0.2823097318295965, \"phi\": 0.20080596635794612}, {\"truth_threshold\": 18.900000281631947, \"match_probability\": 0.9999979557589296, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 209.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1822.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.10290497291974397, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.897095027080256, \"precision\": 1.0, \"recall\": 0.10290497291974397, \"specificity\": 1.0, \"npv\": 0.38591169531513314, \"accuracy\": 0.4263224181360202, \"f1\": 0.18660714285714286, \"f2\": 0.12540501620064803, \"f0_5\": 0.36449250087199164, \"p4\": 0.27954493418624105, \"phi\": 0.19927928280635765}, {\"truth_threshold\": 19.000000283122063, \"match_probability\": 0.9999980926553794, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 201.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1830.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.09896602658788774, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9010339734121122, \"precision\": 1.0, \"recall\": 0.09896602658788774, \"specificity\": 1.0, \"npv\": 0.38487394957983195, \"accuracy\": 0.4238035264483627, \"f1\": 0.18010752688172044, \"f2\": 0.12072072072072072, \"f0_5\": 0.3544973544973545, \"p4\": 0.2720583020072346, \"phi\": 0.19516517498545435}, {\"truth_threshold\": 19.10000028461218, \"match_probability\": 0.9999982203843173, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 194.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1837.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.09551944854751354, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9044805514524865, \"precision\": 1.0, \"recall\": 0.09551944854751354, \"specificity\": 1.0, \"npv\": 0.3839704896042924, \"accuracy\": 0.4215994962216625, \"f1\": 0.17438202247191012, \"f2\": 0.11661457081029093, \"f0_5\": 0.3455646597791236, \"p4\": 0.2653674457472427, \"phi\": 0.19151148640622262}, {\"truth_threshold\": 19.200000286102295, \"match_probability\": 0.9999983395596597, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 176.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1855.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.08665681930083703, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.913343180699163, \"precision\": 1.0, \"recall\": 0.08665681930083703, \"specificity\": 1.0, \"npv\": 0.38166666666666665, \"accuracy\": 0.41593198992443325, \"f1\": 0.15949252378794743, \"f2\": 0.10602409638554217, \"f0_5\": 0.3217550274223035, \"p4\": 0.24752689591851462, \"phi\": 0.18186263873178055}, {\"truth_threshold\": 19.30000028759241, \"match_probability\": 0.9999984507542113, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 165.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1866.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.08124076809453472, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9187592319054653, \"precision\": 1.0, \"recall\": 0.08124076809453472, \"specificity\": 1.0, \"npv\": 0.38027233477250083, \"accuracy\": 0.41246851385390426, \"f1\": 0.15027322404371585, \"f2\": 0.09952949692363373, \"f0_5\": 0.30657748049052397, \"p4\": 0.23614444277786112, \"phi\": 0.1757658003196868}, {\"truth_threshold\": 19.400000289082527, \"match_probability\": 0.9999985545024187, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 150.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1881.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.07385524372230429, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9261447562776958, \"precision\": 1.0, \"recall\": 0.07385524372230429, \"specificity\": 1.0, \"npv\": 0.37838730998017184, \"accuracy\": 0.4077455919395466, \"f1\": 0.1375515818431912, \"f2\": 0.09064539521392313, \"f0_5\": 0.28506271379703535, \"p4\": 0.21998818404076986, \"phi\": 0.16717023359441924}, {\"truth_threshold\": 19.500000290572643, \"match_probability\": 0.9999986513029383, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 148.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1883.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.07287050713934022, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9271294928606598, \"precision\": 1.0, \"recall\": 0.07287050713934022, \"specificity\": 1.0, \"npv\": 0.3781373844121532, \"accuracy\": 0.40711586901763225, \"f1\": 0.13584212941716384, \"f2\": 0.08945841392649903, \"f0_5\": 0.2821197102554327, \"p4\": 0.21777579155929253, \"phi\": 0.16599717759786536}, {\"truth_threshold\": 19.60000029206276, \"match_probability\": 0.9999987416210334, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 144.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1887.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.07090103397341212, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9290989660265879, \"precision\": 1.0, \"recall\": 0.07090103397341212, \"specificity\": 1.0, \"npv\": 0.3776385224274406, \"accuracy\": 0.40585642317380355, \"f1\": 0.13241379310344828, \"f2\": 0.08708272859216255, \"f0_5\": 0.2761795166858458, \"p4\": 0.21330828694544357, \"phi\": 0.16363056471300563}, {\"truth_threshold\": 19.80000029504299, \"match_probability\": 0.9999989045173057, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 143.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1888.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.07040866568193008, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.92959133431807, \"precision\": 1.0, \"recall\": 0.07040866568193008, \"specificity\": 1.0, \"npv\": 0.3775140125288493, \"accuracy\": 0.40554156171284633, \"f1\": 0.13155473781048757, \"f2\": 0.08648844804644974, \"f0_5\": 0.2746830580099885, \"p4\": 0.2121823937921731, \"phi\": 0.16303452977325913}, {\"truth_threshold\": 19.900000296533108, \"match_probability\": 0.9999989778784306, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 142.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1889.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.06991629739044805, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.930083702609552, \"precision\": 1.0, \"recall\": 0.06991629739044805, \"specificity\": 1.0, \"npv\": 0.3773895847066579, \"accuracy\": 0.40522670025188917, \"f1\": 0.13069489185457892, \"f2\": 0.08589402371158965, \"f0_5\": 0.27318199307425933, \"p4\": 0.21105285310447533, \"phi\": 0.16243670286117107}, {\"truth_threshold\": 20.000000298023224, \"match_probability\": 0.99999904632679, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 141.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1890.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.06942392909896603, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.930576070901034, \"precision\": 1.0, \"recall\": 0.06942392909896603, \"specificity\": 1.0, \"npv\": 0.3772652388797364, \"accuracy\": 0.404911838790932, \"f1\": 0.1298342541436464, \"f2\": 0.0852994555353902, \"f0_5\": 0.27167630057803466, \"p4\": 0.20991964425027304, \"phi\": 0.16183706372611714}, {\"truth_threshold\": 20.10000029951334, \"match_probability\": 0.9999991101913761, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 136.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1895.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.06696208764155588, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9330379123584441, \"precision\": 1.0, \"recall\": 0.06696208764155588, \"specificity\": 1.0, \"npv\": 0.37664473684210525, \"accuracy\": 0.4033375314861461, \"f1\": 0.12551915089986157, \"f2\": 0.08232445520581114, \"f0_5\": 0.26407766990291265, \"p4\": 0.2041978445273122, \"phi\": 0.158810950120424}, {\"truth_threshold\": 20.200000301003456, \"match_probability\": 0.9999991697791492, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 133.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1898.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.0654849827671098, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9345150172328902, \"precision\": 1.0, \"recall\": 0.0654849827671098, \"specificity\": 1.0, \"npv\": 0.37627341439369044, \"accuracy\": 0.40239294710327456, \"f1\": 0.12292051756007394, \"f2\": 0.08053772556618627, \"f0_5\": 0.259461568474444, \"p4\": 0.20071939222033594, \"phi\": 0.1569721569492258}, {\"truth_threshold\": 20.300000302493572, \"match_probability\": 0.9999992253765136, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 116.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1915.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.05711472181191531, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9428852781880847, \"precision\": 1.0, \"recall\": 0.05711472181191531, \"specificity\": 1.0, \"npv\": 0.3741830065359477, \"accuracy\": 0.39704030226700254, \"f1\": 0.10805775500698649, \"f2\": 0.0703883495145631, \"f0_5\": 0.23246492985971945, \"p4\": 0.18033362807377223, \"phi\": 0.1461894603760707}, {\"truth_threshold\": 20.40000030398369, \"match_probability\": 0.9999992772506945, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 102.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1929.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.050221565731166914, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9497784342688331, \"precision\": 1.0, \"recall\": 0.050221565731166914, \"specificity\": 1.0, \"npv\": 0.37247885491216653, \"accuracy\": 0.392632241813602, \"f1\": 0.09563994374120956, \"f2\": 0.06199854121079504, \"f0_5\": 0.2091020910209102, \"p4\": 0.16262489021357832, \"phi\": 0.13677160266459246}, {\"truth_threshold\": 20.500000305473804, \"match_probability\": 0.9999993256510213, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 88.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1943.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.043328409650418516, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9566715903495815, \"precision\": 1.0, \"recall\": 0.043328409650418516, \"specificity\": 1.0, \"npv\": 0.3707901554404145, \"accuracy\": 0.38822418136020154, \"f1\": 0.0830580462482303, \"f2\": 0.0535801266439357, \"f0_5\": 0.18464120856063784, \"p4\": 0.1440066829619842, \"phi\": 0.12675073076422336}, {\"truth_threshold\": 21.000000312924385, \"match_probability\": 0.9999995231631726, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 87.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1944.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.04283604135893648, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9571639586410635, \"precision\": 1.0, \"recall\": 0.04283604135893648, \"specificity\": 1.0, \"npv\": 0.370670119779864, \"accuracy\": 0.3879093198992443, \"f1\": 0.0821529745042493, \"f2\": 0.052977712824260136, \"f0_5\": 0.1828499369482976, \"p4\": 0.1426399013699101, \"phi\": 0.12600809728510384}, {\"truth_threshold\": 21.1000003144145, \"match_probability\": 0.9999995550954947, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 84.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1947.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.0413589364844904, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9586410635155096, \"precision\": 1.0, \"recall\": 0.0413589364844904, \"specificity\": 1.0, \"npv\": 0.3703104786545925, \"accuracy\": 0.3869647355163728, \"f1\": 0.07943262411347518, \"f2\": 0.05116959064327485, \"f0_5\": 0.17743979721166034, \"p4\": 0.13850891224492662, \"phi\": 0.12375640414223632}, {\"truth_threshold\": 21.200000315904617, \"match_probability\": 0.9999995848894065, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 82.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1949.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.04037419990152634, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9596258000984736, \"precision\": 1.0, \"recall\": 0.04037419990152634, \"specificity\": 1.0, \"npv\": 0.37007110536522303, \"accuracy\": 0.38633501259445846, \"f1\": 0.0776147657359205, \"f2\": 0.049963441384352915, \"f0_5\": 0.17380245866892752, \"p4\": 0.13572905941236357, \"phi\": 0.12223471186939629}, {\"truth_threshold\": 21.40000031888485, \"match_probability\": 0.9999996386252203, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 81.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1950.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.03988183161004431, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9601181683899557, \"precision\": 1.0, \"recall\": 0.03988183161004431, \"specificity\": 1.0, \"npv\": 0.36995153473344106, \"accuracy\": 0.38602015113350124, \"f1\": 0.07670454545454546, \"f2\": 0.04936014625228519, \"f0_5\": 0.17197452229299362, \"p4\": 0.1343312766142348, \"phi\": 0.12146746400627847}, {\"truth_threshold\": 21.500000320374966, \"match_probability\": 0.9999996628254004, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 78.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1953.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.03840472673559823, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9615952732644018, \"precision\": 1.0, \"recall\": 0.03840472673559823, \"specificity\": 1.0, \"npv\": 0.3695932859909619, \"accuracy\": 0.3850755667506297, \"f1\": 0.07396870554765292, \"f2\": 0.04754937820043892, \"f0_5\": 0.16645326504481434, \"p4\": 0.1301061018101006, \"phi\": 0.11913911679962505}, {\"truth_threshold\": 21.600000321865082, \"match_probability\": 0.999999685404968, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 72.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1959.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.03545051698670606, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9645494830132939, \"precision\": 1.0, \"recall\": 0.03545051698670606, \"specificity\": 1.0, \"npv\": 0.36887886597938147, \"accuracy\": 0.38318639798488663, \"f1\": 0.06847360912981455, \"f2\": 0.043923865300146414, \"f0_5\": 0.15523932729624837, \"p4\": 0.12150944981378942, \"phi\": 0.11435447741316879}, {\"truth_threshold\": 21.700000323355198, \"match_probability\": 0.9999997064724503, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 69.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1962.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.033973412112259974, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.96602658788774, \"precision\": 1.0, \"recall\": 0.033973412112259974, \"specificity\": 1.0, \"npv\": 0.3685226906984229, \"accuracy\": 0.3822418136020151, \"f1\": 0.06571428571428571, \"f2\": 0.04210911753936287, \"f0_5\": 0.14954486345903772, \"p4\": 0.11713607088211223, \"phi\": 0.11189268628385163}, {\"truth_threshold\": 21.800000324845314, \"match_probability\": 0.999999726129107, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 64.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1967.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.03151157065484983, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9684884293451502, \"precision\": 1.0, \"recall\": 0.03151157065484983, \"specificity\": 1.0, \"npv\": 0.3679305912596401, \"accuracy\": 0.38066750629722923, \"f1\": 0.06109785202863962, \"f2\": 0.039081582804103565, \"f0_5\": 0.139921294271972, \"p4\": 0.10973250829301784, \"phi\": 0.10767576710921925}, {\"truth_threshold\": 21.90000032633543, \"match_probability\": 0.9999997444694171, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 57.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1974.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.028064992614475627, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9719350073855244, \"precision\": 1.0, \"recall\": 0.028064992614475627, \"specificity\": 1.0, \"npv\": 0.367104841295287, \"accuracy\": 0.378463476070529, \"f1\": 0.05459770114942529, \"f2\": 0.034836817015034834, \"f0_5\": 0.12616201859229748, \"p4\": 0.09911884237575404, \"phi\": 0.10150268301720144}, {\"truth_threshold\": 22.20000033080578, \"match_probability\": 0.9999997924446623, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 56.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1975.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.027572624322993598, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9724273756770064, \"precision\": 1.0, \"recall\": 0.027572624322993598, \"specificity\": 1.0, \"npv\": 0.36698717948717946, \"accuracy\": 0.3781486146095718, \"f1\": 0.05366554863440345, \"f2\": 0.034229828850855744, \"f0_5\": 0.12416851441241686, \"p4\": 0.09757823512291441, \"phi\": 0.10059224438968953}, {\"truth_threshold\": 22.300000332295895, \"match_probability\": 0.9999998063440199, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 55.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1976.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.02708025603151157, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9729197439684885, \"precision\": 1.0, \"recall\": 0.02708025603151157, \"specificity\": 1.0, \"npv\": 0.3668695930791413, \"accuracy\": 0.3778337531486146, \"f1\": 0.052732502396931925, \"f2\": 0.03362269226066757, \"f0_5\": 0.12216792536650378, \"p4\": 0.0960314132133735, \"phi\": 0.09967408143925688}, {\"truth_threshold\": 22.500000335276127, \"match_probability\": 0.9999998314126736, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 54.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1977.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.026587887740029542, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9734121122599705, \"precision\": 1.0, \"recall\": 0.026587887740029542, \"specificity\": 1.0, \"npv\": 0.3667520819987188, \"accuracy\": 0.37751889168765745, \"f1\": 0.051798561151079135, \"f2\": 0.03301540719002201, \"f0_5\": 0.12016021361815754, \"p4\": 0.09447833496259946, \"phi\": 0.09874797812919536}, {\"truth_threshold\": 22.70000033825636, \"match_probability\": 0.9999998532362051, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 51.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1980.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.025110782865583457, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9748892171344166, \"precision\": 1.0, \"recall\": 0.025110782865583457, \"specificity\": 1.0, \"npv\": 0.3664, \"accuracy\": 0.3765743073047859, \"f1\": 0.04899135446685879, \"f2\": 0.031192660550458717, \"f0_5\": 0.11409395973154363, \"p4\": 0.08978113973386223, \"phi\": 0.09591971039337942}, {\"truth_threshold\": 22.800000339746475, \"match_probability\": 0.9999998630645361, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 50.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1981.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.024618414574101428, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9753815854258986, \"precision\": 1.0, \"recall\": 0.024618414574101428, \"specificity\": 1.0, \"npv\": 0.36628278950735765, \"accuracy\": 0.3762594458438287, \"f1\": 0.048053820278712155, \"f2\": 0.030584781012967948, \"f0_5\": 0.11205737337516809, \"p4\": 0.08820261179873627, \"phi\": 0.09495947326860263}, {\"truth_threshold\": 23.000000342726707, \"match_probability\": 0.999999880790753, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 49.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1982.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.0241260462826194, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9758739537173806, \"precision\": 1.0, \"recall\": 0.0241260462826194, \"specificity\": 1.0, \"npv\": 0.36616565398145184, \"accuracy\": 0.37594458438287154, \"f1\": 0.047115384615384615, \"f2\": 0.029976752722378562, \"f0_5\": 0.11001347103726987, \"p4\": 0.08661761345741757, \"phi\": 0.09399005008543249}, {\"truth_threshold\": 23.500000350177288, \"match_probability\": 0.9999999157063305, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 46.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1985.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.022648941408173313, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9773510585918267, \"precision\": 1.0, \"recall\": 0.022648941408173313, \"specificity\": 1.0, \"npv\": 0.365814696485623, \"accuracy\": 0.375, \"f1\": 0.04429465575349061, \"f2\": 0.028151774785801713, \"f0_5\": 0.1038374717832957, \"p4\": 0.08182335429924091, \"phi\": 0.0910237091474061}, {\"truth_threshold\": 23.70000035315752, \"match_probability\": 0.9999999266180979, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 44.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1987.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.021664204825209258, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9783357951747907, \"precision\": 1.0, \"recall\": 0.021664204825209258, \"specificity\": 1.0, \"npv\": 0.365581098339719, \"accuracy\": 0.3743702770780856, \"f1\": 0.042409638554216866, \"f2\": 0.02693437806072478, \"f0_5\": 0.09968282736746716, \"p4\": 0.07859401270561604, \"phi\": 0.08899451553133284}, {\"truth_threshold\": 23.900000356137753, \"match_probability\": 0.9999999361173434, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 42.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1989.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.0206794682422452, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9793205317577548, \"precision\": 1.0, \"recall\": 0.0206794682422452, \"specificity\": 1.0, \"npv\": 0.3653477983407786, \"accuracy\": 0.3737405541561713, \"f1\": 0.04052098408104197, \"f2\": 0.025716385011021307, \"f0_5\": 0.09549795361527967, \"p4\": 0.0753377096255321, \"phi\": 0.08692064307839845}, {\"truth_threshold\": 24.00000035762787, \"match_probability\": 0.9999999403953735, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 37.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1994.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.018217626784835055, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9817823732151649, \"precision\": 1.0, \"recall\": 0.018217626784835055, \"specificity\": 1.0, \"npv\": 0.3647658489964957, \"accuracy\": 0.3721662468513854, \"f1\": 0.035783365570599614, \"f2\": 0.022668790589388556, \"f0_5\": 0.08490133088572739, \"p4\": 0.06707653041837135, \"phi\": 0.08151790049352142}, {\"truth_threshold\": 24.100000359117985, \"match_probability\": 0.9999999443869169, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 32.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1999.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.015755785327424915, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9842442146725751, \"precision\": 1.0, \"recall\": 0.015755785327424915, \"specificity\": 1.0, \"npv\": 0.3641857506361323, \"accuracy\": 0.37059193954659947, \"f1\": 0.031022782355792537, \"f2\": 0.0196174595389897, \"f0_5\": 0.07410838351088467, \"p4\": 0.0586384719748834, \"phi\": 0.07574980202172149}, {\"truth_threshold\": 24.2000003606081, \"match_probability\": 0.9999999481111586, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 30.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 2001.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.014771048744460856, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9852289512555391, \"precision\": 1.0, \"recall\": 0.014771048744460856, \"specificity\": 1.0, \"npv\": 0.3639542275905912, \"accuracy\": 0.36996221662468515, \"f1\": 0.02911208151382824, \"f2\": 0.01839587932303164, \"f0_5\": 0.0697350069735007, \"p4\": 0.05521232030378831, \"phi\": 0.07332111317003598}, {\"truth_threshold\": 24.300000362098217, \"match_probability\": 0.999999951585999, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 27.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 2004.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.013293943870014771, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9867060561299852, \"precision\": 1.0, \"recall\": 0.013293943870014771, \"specificity\": 1.0, \"npv\": 0.3636074944426802, \"accuracy\": 0.3690176322418136, \"f1\": 0.026239067055393587, \"f2\": 0.016562384983437616, \"f0_5\": 0.06311360448807854, \"p4\": 0.050017230584043997, \"phi\": 0.06952537394245138}, {\"truth_threshold\": 24.50000036507845, \"match_probability\": 0.999999957853164, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 26.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 2005.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.012801575578532743, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9871984244214672, \"precision\": 1.0, \"recall\": 0.012801575578532743, \"specificity\": 1.0, \"npv\": 0.3634920634920635, \"accuracy\": 0.36870277078085645, \"f1\": 0.025279533300923675, \"f2\": 0.015950920245398775, \"f0_5\": 0.06088992974238876, \"p4\": 0.048270424636238894, \"phi\": 0.0682148893057115}, {\"truth_threshold\": 24.70000036805868, \"match_probability\": 0.999999963309048, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 25.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 2006.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.012309207287050714, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9876907927129492, \"precision\": 1.0, \"recall\": 0.012309207287050714, \"specificity\": 1.0, \"npv\": 0.3633767058076801, \"accuracy\": 0.36838790931989923, \"f1\": 0.024319066147859923, \"f2\": 0.015339305436249846, \"f0_5\": 0.05865790708587518, \"p4\": 0.04651597386980402, \"phi\": 0.06687958728246145}, {\"truth_threshold\": 24.800000369548798, \"match_probability\": 0.9999999657661313, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 23.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 2008.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.011324470704086657, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9886755292959133, \"precision\": 1.0, \"recall\": 0.011324470704086657, \"specificity\": 1.0, \"npv\": 0.36314620995876945, \"accuracy\": 0.3677581863979849, \"f1\": 0.022395326192794548, \"f2\": 0.014115625383576776, \"f0_5\": 0.054168629298162976, \"p4\": 0.042983917959230976, \"phi\": 0.06412829809045448}, {\"truth_threshold\": 25.00000037252903, \"match_probability\": 0.9999999701976862, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 22.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 2009.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.010832102412604629, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9891678975873953, \"precision\": 1.0, \"recall\": 0.010832102412604629, \"specificity\": 1.0, \"npv\": 0.3630310716550412, \"accuracy\": 0.3674433249370277, \"f1\": 0.02143205065757428, \"f2\": 0.013503560029462312, \"f0_5\": 0.05191127890514394, \"p4\": 0.04120620154151108, \"phi\": 0.06270876930003502}, {\"truth_threshold\": 25.100000374019146, \"match_probability\": 0.9999999721934579, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 18.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 2013.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.008862629246676515, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9911373707533235, \"precision\": 1.0, \"recall\": 0.008862629246676515, \"specificity\": 1.0, \"npv\": 0.36257124762507914, \"accuracy\": 0.366183879093199, \"f1\": 0.017569546120058566, \"f2\": 0.01105379513633014, \"f0_5\": 0.042796005706134094, \"p4\": 0.034016089560848325, \"phi\": 0.05668628179027108}, {\"truth_threshold\": 25.200000375509262, \"match_probability\": 0.9999999740555788, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 17.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 2014.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.008370260955194485, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9916297390448056, \"precision\": 1.0, \"recall\": 0.008370260955194485, \"specificity\": 1.0, \"npv\": 0.3624564735675847, \"accuracy\": 0.36586901763224183, \"f1\": 0.0166015625, \"f2\": 0.010440977766859108, \"f0_5\": 0.040495474035254886, \"p4\": 0.03219846095822884, \"phi\": 0.05508044361350257}, {\"truth_threshold\": 25.400000378489494, \"match_probability\": 0.9999999774140695, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 15.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 2016.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.007385524372230428, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9926144756277696, \"precision\": 1.0, \"recall\": 0.007385524372230428, \"specificity\": 1.0, \"npv\": 0.3622271433090794, \"accuracy\": 0.36523929471032746, \"f1\": 0.01466275659824047, \"f2\": 0.009214891264283081, \"f0_5\": 0.035868005738880916, \"p4\": 0.028538670521671944, \"phi\": 0.05172269709897784}, {\"truth_threshold\": 25.50000037997961, \"match_probability\": 0.9999999789265818, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 12.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 2019.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.005908419497784343, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9940915805022157, \"precision\": 1.0, \"recall\": 0.005908419497784343, \"specificity\": 1.0, \"npv\": 0.36188369152970923, \"accuracy\": 0.3642947103274559, \"f1\": 0.011747430249632892, \"f2\": 0.007374631268436578, \"f0_5\": 0.02886002886002886, \"p4\": 0.022986746233599045, \"phi\": 0.046240249339339734}, {\"truth_threshold\": 25.600000381469727, \"match_probability\": 0.9999999803378055, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 9.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 2022.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.004431314623338257, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9955686853766618, \"precision\": 1.0, \"recall\": 0.004431314623338257, \"specificity\": 1.0, \"npv\": 0.361540890432586, \"accuracy\": 0.3633501259445844, \"f1\": 0.008823529411764706, \"f2\": 0.005533013648100332, \"f0_5\": 0.02177068214804064, \"p4\": 0.017358654565300884, \"phi\": 0.040026259314463214}, {\"truth_threshold\": 25.700000382959843, \"match_probability\": 0.9999999816545239, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 8.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 2023.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.003938946331856229, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9960610536681438, \"precision\": 1.0, \"recall\": 0.003938946331856229, \"specificity\": 1.0, \"npv\": 0.3614267676767677, \"accuracy\": 0.3630352644836272, \"f1\": 0.00784698381559588, \"f2\": 0.004918839153959666, \"f0_5\": 0.019389238972370333, \"p4\": 0.015465403546152875, \"phi\": 0.03773116272757914}, {\"truth_threshold\": 25.900000385940075, \"match_probability\": 0.9999999840293354, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 6.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 2025.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.0029542097488921715, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9970457902511078, \"precision\": 1.0, \"recall\": 0.0029542097488921715, \"specificity\": 1.0, \"npv\": 0.361198738170347, \"accuracy\": 0.36240554156171284, \"f1\": 0.005891016200294551, \"f2\": 0.0036900369003690036, \"f0_5\": 0.014598540145985401, \"p4\": 0.011652683870064942, \"phi\": 0.032665835877723835}, {\"truth_threshold\": 26.00000038743019, \"match_probability\": 0.999999985098843, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 5.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 2026.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.002461841457410143, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9975381585425899, \"precision\": 1.0, \"recall\": 0.002461841457410143, \"specificity\": 1.0, \"npv\": 0.36108483128350677, \"accuracy\": 0.3620906801007557, \"f1\": 0.004911591355599214, \"f2\": 0.0030754090294009104, \"f0_5\": 0.01218917601170161, \"p4\": 0.009733083985039102, \"phi\": 0.02981498964104606}, {\"truth_threshold\": 26.100000388920307, \"match_probability\": 0.9999999860967289, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 4.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 2027.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.0019694731659281144, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9980305268340719, \"precision\": 1.0, \"recall\": 0.0019694731659281144, \"specificity\": 1.0, \"npv\": 0.3609709962168979, \"accuracy\": 0.36177581863979846, \"f1\": 0.003931203931203931, \"f2\": 0.0024606299212598425, \"f0_5\": 0.009770395701025891, \"p4\": 0.007804568825263287, \"phi\": 0.026663133550419747}, {\"truth_threshold\": 26.30000039190054, \"match_probability\": 0.9999999878964996, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 3.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 2028.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.0014771048744460858, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9985228951255539, \"precision\": 1.0, \"recall\": 0.0014771048744460858, \"specificity\": 1.0, \"npv\": 0.36085723290261584, \"accuracy\": 0.3614609571788413, \"f1\": 0.0029498525073746312, \"f2\": 0.0018456995201181247, \"f0_5\": 0.007342143906020558, \"p4\": 0.00586707112734875, \"phi\": 0.023087312050119223}, {\"truth_threshold\": 26.50000039488077, \"match_probability\": 0.9999999894632908, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 2.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 2029.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.0009847365829640572, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.999015263417036, \"precision\": 1.0, \"recall\": 0.0009847365829640572, \"specificity\": 1.0, \"npv\": 0.36074354127284186, \"accuracy\": 0.36114609571788414, \"f1\": 0.001967535661583866, \"f2\": 0.0012306177701206006, \"f0_5\": 0.004904364884747425, \"p4\": 0.003920522953249476, \"phi\": 0.018847741566547744}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linker.accuracy_analysis_from_labels_table(\n",
    "    labels_table, output_type=\"precision_recall\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e6ba74",
   "metadata": {},
   "source": [
    "## Truth table\n",
    "\n",
    "Finally, Splink can also report the underlying table used to construct the ROC and precision recall curves.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7c283ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-20T20:18:25.558507Z",
     "iopub.status.busy": "2024-05-20T20:18:25.558255Z",
     "iopub.status.idle": "2024-05-20T20:18:25.784948Z",
     "shell.execute_reply": "2024-05-20T20:18:25.781483Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>truth_threshold</th>\n",
       "      <th>match_probability</th>\n",
       "      <th>total_clerical_labels</th>\n",
       "      <th>p</th>\n",
       "      <th>n</th>\n",
       "      <th>tp</th>\n",
       "      <th>tn</th>\n",
       "      <th>fp</th>\n",
       "      <th>fn</th>\n",
       "      <th>P_rate</th>\n",
       "      <th>N_rate</th>\n",
       "      <th>tp_rate</th>\n",
       "      <th>tn_rate</th>\n",
       "      <th>fp_rate</th>\n",
       "      <th>fn_rate</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>specificity</th>\n",
       "      <th>npv</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f0_5</th>\n",
       "      <th>p4</th>\n",
       "      <th>phi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-12.3</td>\n",
       "      <td>0.000198</td>\n",
       "      <td>3176.0</td>\n",
       "      <td>2031.0</td>\n",
       "      <td>1145.0</td>\n",
       "      <td>1027.0</td>\n",
       "      <td>1145.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1004.0</td>\n",
       "      <td>0.639484</td>\n",
       "      <td>0.360516</td>\n",
       "      <td>0.505662</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.494338</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.505662</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.532806</td>\n",
       "      <td>0.683879</td>\n",
       "      <td>0.671681</td>\n",
       "      <td>0.561141</td>\n",
       "      <td>0.836455</td>\n",
       "      <td>0.683240</td>\n",
       "      <td>0.519057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-5.8</td>\n",
       "      <td>0.017632</td>\n",
       "      <td>3176.0</td>\n",
       "      <td>2031.0</td>\n",
       "      <td>1145.0</td>\n",
       "      <td>1026.0</td>\n",
       "      <td>1145.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1005.0</td>\n",
       "      <td>0.639484</td>\n",
       "      <td>0.360516</td>\n",
       "      <td>0.505170</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.494830</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.505170</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.532558</td>\n",
       "      <td>0.683564</td>\n",
       "      <td>0.671246</td>\n",
       "      <td>0.560656</td>\n",
       "      <td>0.836186</td>\n",
       "      <td>0.682913</td>\n",
       "      <td>0.518683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-4.7</td>\n",
       "      <td>0.037048</td>\n",
       "      <td>3176.0</td>\n",
       "      <td>2031.0</td>\n",
       "      <td>1145.0</td>\n",
       "      <td>1025.0</td>\n",
       "      <td>1145.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1006.0</td>\n",
       "      <td>0.639484</td>\n",
       "      <td>0.360516</td>\n",
       "      <td>0.504677</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.495323</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.504677</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.532311</td>\n",
       "      <td>0.683249</td>\n",
       "      <td>0.670812</td>\n",
       "      <td>0.560171</td>\n",
       "      <td>0.835916</td>\n",
       "      <td>0.682586</td>\n",
       "      <td>0.518310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-3.7</td>\n",
       "      <td>0.071449</td>\n",
       "      <td>3176.0</td>\n",
       "      <td>2031.0</td>\n",
       "      <td>1145.0</td>\n",
       "      <td>1023.0</td>\n",
       "      <td>1145.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1008.0</td>\n",
       "      <td>0.639484</td>\n",
       "      <td>0.360516</td>\n",
       "      <td>0.503693</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.496307</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.503693</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.531816</td>\n",
       "      <td>0.682620</td>\n",
       "      <td>0.669941</td>\n",
       "      <td>0.559200</td>\n",
       "      <td>0.835375</td>\n",
       "      <td>0.681932</td>\n",
       "      <td>0.517563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-3.2</td>\n",
       "      <td>0.098139</td>\n",
       "      <td>3176.0</td>\n",
       "      <td>2031.0</td>\n",
       "      <td>1145.0</td>\n",
       "      <td>1017.0</td>\n",
       "      <td>1145.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1014.0</td>\n",
       "      <td>0.639484</td>\n",
       "      <td>0.360516</td>\n",
       "      <td>0.500739</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.499261</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500739</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.530338</td>\n",
       "      <td>0.680730</td>\n",
       "      <td>0.667323</td>\n",
       "      <td>0.556285</td>\n",
       "      <td>0.833743</td>\n",
       "      <td>0.679967</td>\n",
       "      <td>0.515326</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   truth_threshold  match_probability  total_clerical_labels       p       n  \\\n",
       "0            -12.3           0.000198                 3176.0  2031.0  1145.0   \n",
       "1             -5.8           0.017632                 3176.0  2031.0  1145.0   \n",
       "2             -4.7           0.037048                 3176.0  2031.0  1145.0   \n",
       "3             -3.7           0.071449                 3176.0  2031.0  1145.0   \n",
       "4             -3.2           0.098139                 3176.0  2031.0  1145.0   \n",
       "\n",
       "       tp      tn   fp      fn    P_rate    N_rate   tp_rate  tn_rate  \\\n",
       "0  1027.0  1145.0  0.0  1004.0  0.639484  0.360516  0.505662      1.0   \n",
       "1  1026.0  1145.0  0.0  1005.0  0.639484  0.360516  0.505170      1.0   \n",
       "2  1025.0  1145.0  0.0  1006.0  0.639484  0.360516  0.504677      1.0   \n",
       "3  1023.0  1145.0  0.0  1008.0  0.639484  0.360516  0.503693      1.0   \n",
       "4  1017.0  1145.0  0.0  1014.0  0.639484  0.360516  0.500739      1.0   \n",
       "\n",
       "   fp_rate   fn_rate  precision    recall  specificity       npv  accuracy  \\\n",
       "0      0.0  0.494338        1.0  0.505662          1.0  0.532806  0.683879   \n",
       "1      0.0  0.494830        1.0  0.505170          1.0  0.532558  0.683564   \n",
       "2      0.0  0.495323        1.0  0.504677          1.0  0.532311  0.683249   \n",
       "3      0.0  0.496307        1.0  0.503693          1.0  0.531816  0.682620   \n",
       "4      0.0  0.499261        1.0  0.500739          1.0  0.530338  0.680730   \n",
       "\n",
       "         f1        f2      f0_5        p4       phi  \n",
       "0  0.671681  0.561141  0.836455  0.683240  0.519057  \n",
       "1  0.671246  0.560656  0.836186  0.682913  0.518683  \n",
       "2  0.670812  0.560171  0.835916  0.682586  0.518310  \n",
       "3  0.669941  0.559200  0.835375  0.681932  0.517563  \n",
       "4  0.667323  0.556285  0.833743  0.679967  0.515326  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_table = linker.accuracy_analysis_from_labels_table(labels_table, output_type=\"table\")\n",
    "roc_table.as_pandas_dataframe(limit=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff86458e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f2d6d5f4",
   "metadata": {},
   "source": [
    "!!! note \"Further Reading\"\n",
    ":material-tools: For more on the quality assurance tools in Splink, please refer to the [Evaluation API documentation](../../linkereval.md).\n",
    "\n",
    "    :bar_chart: For more on the charts used in this tutorial, please refer to the [Charts Gallery](../../charts/index.md#model-evaluation).\n",
    "\n",
    "    :material-thumbs-up-down: For more on the Evaluation Metrics used in this tutorial, please refer to the [Edge Metrics guide.](../../topic_guides/evaluation/edge_metrics.md)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ee8f2d",
   "metadata": {},
   "source": [
    "## :material-flag-checkered: That's it!\n",
    "\n",
    "That wraps up the Splink tutorial! Don't worry, there are still plenty of resources to help on the next steps of your Splink journey:\n",
    "\n",
    ":octicons-link-16: For some end-to-end notebooks of Splink pipelines, check out our [Examples](../examples/examples_index.md)\n",
    "\n",
    ":simple-readme: For more deepdives into the different aspects of Splink, and record linkage more generally, check out our [Topic Guides](../../topic_guides/topic_guides_index.md)\n",
    "\n",
    ":material-tools: For a reference on all the functionality avalable in Splink, see our [Documentation](../../documentation_index.md)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
