{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57b58c35",
   "metadata": {},
   "source": [
    "## Evaluation of prediction results\n",
    "\n",
    " <a target=\"_blank\" href=\"https://colab.research.google.com/github/moj-analytical-services/splink/blob/splink4_dev/docs/demos/tutorials/07_Quality_assurance.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "In the previous tutorial, we looked at various ways to visualise the results of our model.\n",
    "These are useful for evaluating a linkage pipeline because they allow us to understand how our model works and verify that it is doing something sensible. They can also be useful to identify examples where the model is not performing as expected.\n",
    "\n",
    "In addition to these spot checks, Splink also has functions to perform more formal accuracy analysis. These functions allow you to understand the likely prevalence of false positives and false negatives in your linkage models.\n",
    "\n",
    "They rely on the existence of a sample of labelled (ground truth) matches, which may have been produced (for example) by human beings. For the accuracy analysis to be unbiased, the sample should be representative of the overall dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e08e61e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-07T09:03:02.636698Z",
     "iopub.status.busy": "2024-06-07T09:03:02.636146Z",
     "iopub.status.idle": "2024-06-07T09:03:02.641116Z",
     "shell.execute_reply": "2024-06-07T09:03:02.640495Z"
    }
   },
   "outputs": [],
   "source": [
    "# Uncomment and run this cell if you're running in Google Colab.\n",
    "# !pip install git+https://github.com/moj-analytical-services/splink.git@splink4_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb29d421",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-07T09:03:02.644758Z",
     "iopub.status.busy": "2024-06-07T09:03:02.644471Z",
     "iopub.status.idle": "2024-06-07T09:03:04.108590Z",
     "shell.execute_reply": "2024-06-07T09:03:04.107484Z"
    }
   },
   "outputs": [],
   "source": [
    "# Rerun our predictions to we're ready to view the charts\n",
    "import pandas as pd\n",
    "\n",
    "from splink import DuckDBAPI, Linker, splink_datasets\n",
    "\n",
    "pd.options.display.max_columns = 1000\n",
    "\n",
    "db_api = DuckDBAPI()\n",
    "df = splink_datasets.fake_1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f88cc1c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-07T09:03:04.112617Z",
     "iopub.status.busy": "2024-06-07T09:03:04.112305Z",
     "iopub.status.idle": "2024-06-07T09:03:04.739121Z",
     "shell.execute_reply": "2024-06-07T09:03:04.738560Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import urllib\n",
    "\n",
    "from splink import block_on\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/moj-analytical-services/splink/479cef8d0bce546fae85a9a146da8970877cc00e/docs/demos/demo_settings/saved_model_from_demo.json\"\n",
    "\n",
    "with urllib.request.urlopen(url) as u:\n",
    "    settings = json.loads(u.read().decode())\n",
    "\n",
    "# # TODO: Fix this, shouldn't be needed\n",
    "for r in settings[\"blocking_rules_to_generate_predictions\"]:\n",
    "    del r[\"sql_dialect\"]\n",
    "\n",
    "settings[\"blocking_rules_to_generate_predictions\"].append(block_on(\"city\", salting_partitions=10))\n",
    "\n",
    "\n",
    "linker = Linker(df, \"../demo_settings/saved_model_from_demo.json\", database_api=DuckDBAPI())\n",
    "df_predictions = linker.inference.predict(threshold_match_probability=0.1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0dedd9",
   "metadata": {},
   "source": [
    "## Load in labels\n",
    "\n",
    "The labels file contains a list of pairwise comparisons which represent matches and non-matches.\n",
    "\n",
    "The required format of the labels file is described [here](https://moj-analytical-services.github.io/splink/linkerqa.html#splink.linker.Linker.roc_chart_from_labels).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbfdc70c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-07T09:03:04.742455Z",
     "iopub.status.busy": "2024-06-07T09:03:04.742207Z",
     "iopub.status.idle": "2024-06-07T09:03:04.766096Z",
     "shell.execute_reply": "2024-06-07T09:03:04.765427Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unique_id_l</th>\n",
       "      <th>source_dataset_l</th>\n",
       "      <th>unique_id_r</th>\n",
       "      <th>source_dataset_r</th>\n",
       "      <th>clerical_match_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>fake_1000</td>\n",
       "      <td>1</td>\n",
       "      <td>fake_1000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>fake_1000</td>\n",
       "      <td>2</td>\n",
       "      <td>fake_1000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>fake_1000</td>\n",
       "      <td>3</td>\n",
       "      <td>fake_1000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>fake_1000</td>\n",
       "      <td>4</td>\n",
       "      <td>fake_1000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>fake_1000</td>\n",
       "      <td>5</td>\n",
       "      <td>fake_1000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   unique_id_l source_dataset_l  unique_id_r source_dataset_r  \\\n",
       "0            0        fake_1000            1        fake_1000   \n",
       "1            0        fake_1000            2        fake_1000   \n",
       "2            0        fake_1000            3        fake_1000   \n",
       "3            0        fake_1000            4        fake_1000   \n",
       "4            0        fake_1000            5        fake_1000   \n",
       "\n",
       "   clerical_match_score  \n",
       "0                   1.0  \n",
       "1                   1.0  \n",
       "2                   1.0  \n",
       "3                   0.0  \n",
       "4                   0.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from splink.datasets import splink_dataset_labels\n",
    "\n",
    "df_labels = splink_dataset_labels.fake_1000_labels\n",
    "labels_table = linker.table_management.register_labels_table(df_labels)\n",
    "df_labels.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030c4cde",
   "metadata": {},
   "source": [
    "### Threshold Selection chart\n",
    "\n",
    "Splink includes an interactive dashboard that shows key accuracy statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e83d9645",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-07T09:03:04.769834Z",
     "iopub.status.busy": "2024-06-07T09:03:04.769551Z",
     "iopub.status.idle": "2024-06-07T09:03:06.065158Z",
     "shell.execute_reply": "2024-06-07T09:03:06.064618Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-0e126e7078084902acafbac7689e9a48.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-0e126e7078084902acafbac7689e9a48.vega-embed details,\n",
       "  #altair-viz-0e126e7078084902acafbac7689e9a48.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-0e126e7078084902acafbac7689e9a48\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-0e126e7078084902acafbac7689e9a48\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-0e126e7078084902acafbac7689e9a48\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300, \"discreteHeight\": {\"step\": 150}, \"discreteWidth\": {\"step\": 150}}, \"axis\": {\"gridWidth\": 0.5, \"labelFontSize\": 12, \"titleFontSize\": 16}, \"axisX\": {\"format\": \"+.0f\", \"grid\": false, \"offset\": 20, \"values\": {\"expr\": \"[-25,-20,-15,-10,-5,0,5,10,15,20,25]\"}}, \"axisY\": {\"title\": \"Match probability threshold\", \"titleFontSize\": 16}, \"concat\": {\"spacing\": 40}}, \"hconcat\": [{\"vconcat\": [{\"layer\": [{\"layer\": [{\"mark\": {\"type\": \"rule\"}, \"encoding\": {\"opacity\": {\"condition\": {\"param\": \"threshold\", \"value\": 0.3, \"empty\": false}, \"value\": 0}, \"x\": {\"axis\": {\"orient\": \"bottom\"}, \"field\": \"truth_threshold\", \"scale\": {\"nice\": false}, \"title\": null, \"type\": \"quantitative\"}}, \"params\": [{\"name\": \"threshold\", \"select\": {\"type\": \"point\", \"encodings\": [\"x\"], \"fields\": [\"truth_threshold\"], \"nearest\": true, \"on\": \"mouseover\", \"toggle\": false}, \"value\": null}]}, {\"mark\": {\"type\": \"rule\"}, \"encoding\": {\"opacity\": {\"condition\": {\"param\": \"threshold\", \"value\": 0.3, \"empty\": false}, \"value\": 0}, \"y\": {\"axis\": {\"orient\": \"right\"}, \"field\": \"match_probability\", \"title\": \" \", \"type\": \"quantitative\"}}, \"params\": [{\"name\": \"prob\", \"select\": {\"type\": \"point\", \"encodings\": [\"y\"], \"fields\": [\"match_probability\"], \"nearest\": true, \"on\": \"mouseover\", \"toggle\": false}}]}]}, {\"layer\": [{\"mark\": {\"type\": \"text\", \"fontSize\": 14, \"fontWeight\": \"bold\", \"xOffset\": 25, \"yOffset\": 10}, \"encoding\": {\"text\": {\"aggregate\": \"min\", \"field\": \"truth_threshold\", \"format\": \"+.2f\"}, \"y\": {\"axis\": {\"orient\": \"left\"}, \"field\": \"match_probability\", \"title\": \"Match probability threshold\", \"type\": \"quantitative\"}}, \"transform\": [{\"filter\": {\"param\": \"threshold\", \"empty\": false}}]}, {\"mark\": {\"type\": \"text\", \"fontSize\": 14, \"xOffset\": -25, \"yOffset\": -10}, \"encoding\": {\"text\": {\"aggregate\": \"min\", \"field\": \"match_probability\", \"format\": \".3f\"}}, \"transform\": [{\"filter\": {\"param\": \"threshold\", \"empty\": false}}]}, {\"mark\": {\"type\": \"line\", \"color\": \"red\", \"opacity\": 0.5}}, {\"mark\": {\"type\": \"line\", \"color\": \"green\", \"opacity\": 0.5, \"strokeWidth\": 3}, \"transform\": [{\"filter\": \"datum.truth_threshold >= threshold.truth_threshold\"}]}, {\"mark\": {\"type\": \"point\", \"color\": \"green\", \"size\": 100}, \"encoding\": {\"opacity\": {\"condition\": {\"param\": \"threshold\", \"value\": 1, \"empty\": false}, \"value\": 0}}}], \"encoding\": {\"x\": {\"field\": \"truth_threshold\", \"type\": \"quantitative\", \"title\": \"Match weight threshold\", \"axis\": {\"orient\": \"top\"}}, \"y\": {\"field\": \"match_probability\", \"type\": \"quantitative\", \"title\": \"Match probability threshold\", \"axis\": {\"orient\": \"left\", \"titlePadding\": 10}}}}, {\"mark\": {\"type\": \"text\", \"align\": \"left\", \"color\": \"red\", \"fontSize\": 12, \"text\": \"Non-match\", \"x\": 0, \"y\": \"height\", \"yOffset\": 10}, \"data\": {\"values\": [{}]}}, {\"mark\": {\"type\": \"text\", \"align\": \"right\", \"color\": \"green\", \"fontSize\": 12, \"fontWeight\": \"bold\", \"text\": \"Match\", \"x\": \"width\", \"y\": 0, \"yOffset\": -10}, \"data\": {\"values\": [{}]}}], \"description\": \"Match weight vs probability\"}, {\"hconcat\": [{\"layer\": [{\"mark\": {\"type\": \"rect\", \"opacity\": 0.5}, \"encoding\": {\"color\": {\"field\": \"count\", \"legend\": null, \"scale\": {\"scheme\": \"reds\", \"zero\": true}, \"type\": \"quantitative\"}}, \"transform\": [{\"filter\": \"datum.predicted == 0\"}]}, {\"mark\": {\"type\": \"rect\", \"opacity\": 0.5}, \"encoding\": {\"color\": {\"field\": \"count\", \"legend\": null, \"scale\": {\"scheme\": \"greens\", \"zero\": true}, \"type\": \"quantitative\"}}, \"transform\": [{\"filter\": \"datum.predicted == 1\"}]}, {\"mark\": {\"type\": \"text\", \"fontSize\": 14, \"yOffset\": -40}, \"encoding\": {\"color\": {\"condition\": [{\"test\": \"datum.predicted==1 && datum.actual==1\", \"value\": \"darkgreen\"}, {\"test\": \"datum.predicted==0 && datum.actual==0\", \"value\": \"darkred\"}], \"value\": \"black\"}, \"opacity\": {\"condition\": {\"test\": \"datum.predicted != datum.actual\", \"value\": 1}, \"value\": 0.5}, \"text\": {\"field\": \"confusion_label\", \"type\": \"nominal\"}}}, {\"mark\": {\"type\": \"text\", \"fontSize\": 28, \"fontWeight\": \"bold\", \"yOffset\": 10}, \"encoding\": {\"color\": {\"condition\": [{\"test\": \"datum.predicted==1 && datum.actual==1\", \"value\": \"darkgreen\"}, {\"test\": \"datum.predicted==0 && datum.actual==0\", \"value\": \"darkred\"}], \"value\": \"black\"}, \"text\": {\"field\": \"count\", \"format\": \",\", \"type\": \"nominal\"}}}], \"description\": \"Confusion matrix\", \"encoding\": {\"x\": {\"field\": \"actual\", \"type\": \"nominal\", \"title\": \"Actual\", \"axis\": {\"domain\": false, \"labelAngle\": 0, \"labelExpr\": \"datum.label == 1 ? 'Match' : 'Non-match'\", \"labelFontSize\": 18, \"labelPadding\": 10, \"orient\": \"top\", \"ticks\": false, \"titleAngle\": 0, \"titleFontSize\": 20}, \"sort\": \"-x\"}, \"y\": {\"field\": \"predicted\", \"type\": \"nominal\", \"title\": \"Predicted\", \"axis\": {\"domain\": false, \"labelExpr\": \"datum.label == 1 ? 'Match' : 'Non-match'\", \"labelFontSize\": 18, \"labelPadding\": 10, \"ticks\": false, \"titleAngle\": 0, \"titleFontSize\": 20, \"titlePadding\": -30}, \"sort\": \"-y\"}}, \"resolve\": {\"scale\": {\"color\": \"independent\"}}, \"transform\": [{\"filter\": {\"or\": [{\"param\": \"threshold\", \"empty\": false}, {\"and\": [{\"param\": \"threshold\", \"empty\": true}, \"datum.truth_threshold == datum.median_threshold\"]}]}}]}], \"transform\": [{\"fold\": [\"tp\", \"tn\", \"fp\", \"fn\"], \"as\": [\"label\", \"count\"]}, {\"calculate\": \"datum.label === 'tp' ? 'True Positive (TP)' : datum.label === 'tn' ? 'True Negative (TN)' : datum.label === 'fp' ? 'False Positive (FP)' : 'False Negative (FN)'\", \"as\": \"confusion_label\"}, {\"calculate\": \"datum.label === 'tp' || datum.label === 'fp' ? 1 : 0\", \"as\": \"predicted\"}, {\"calculate\": \"datum.label === 'tp' || datum.label === 'fn' ? 1 : 0\", \"as\": \"actual\"}, {\"joinaggregate\": [{\"op\": \"median\", \"field\": \"truth_threshold\", \"as\": \"median_threshold\"}]}]}]}, {\"layer\": [{\"layer\": [{\"mark\": {\"type\": \"point\", \"size\": 100}, \"encoding\": {\"opacity\": {\"condition\": {\"param\": \"threshold\", \"value\": 1, \"empty\": false}, \"value\": 0}, \"tooltip\": [{\"field\": \"truth_threshold\", \"format\": \".3f\", \"title\": \"Match weight threshold\", \"type\": \"quantitative\"}, {\"field\": \"match_probability\", \"format\": \".3%\", \"title\": \"Match probability threshold\", \"type\": \"quantitative\"}, {\"field\": \"precision\", \"format\": \".4f\", \"title\": \"Precision\", \"type\": \"quantitative\"}, {\"field\": \"recall\", \"format\": \".4f\", \"title\": \"Recall (TPR)\", \"type\": \"quantitative\"}, {\"field\": \"fp_rate\", \"format\": \".4f\", \"title\": \"FPR\", \"type\": \"quantitative\"}], \"x\": {\"axis\": {\"orient\": \"top\"}, \"field\": \"truth_threshold\", \"title\": \"Match weight threshold\"}}, \"params\": [{\"name\": \"metric\", \"select\": {\"type\": \"point\", \"fields\": [\"metric\"]}, \"bind\": \"legend\", \"value\": [{\"metric\": \"precision\"}, {\"metric\": \"recall\"}]}, {\"name\": \"threshold\", \"select\": {\"type\": \"point\", \"encodings\": [\"x\"], \"fields\": [\"truth_threshold\"], \"nearest\": true, \"on\": \"mouseover\", \"toggle\": false}, \"value\": null}], \"transform\": [{\"filter\": {\"param\": \"metric\", \"empty\": true}}]}, {\"mark\": {\"type\": \"line\"}, \"encoding\": {\"opacity\": {\"condition\": {\"param\": \"metric\", \"value\": 1}, \"value\": 0.1}, \"x\": {\"axis\": {\"orient\": \"bottom\"}, \"field\": \"truth_threshold\", \"title\": null}}}], \"encoding\": {\"color\": {\"field\": \"metric\", \"type\": \"nominal\", \"sort\": [\"precision\", \"recall\", \"f1\"], \"title\": [\"Performance\", \"Metric\"], \"legend\": {\"fillColor\": \"whitesmoke\", \"labelExpr\": \"{'precision': 'Precision (PPV)', 'recall': 'Recall (TPR)', 'specificity': 'Specificity (TNR)', 'accuracy': 'Accuracy', 'npv': 'NPV', 'f1': 'F1', 'f2': 'F2', 'f0_5': 'F0.5', 'p4': 'P4', 'phi': '\\u03c6 (MCC)'}[datum.value]\", \"labelFontSize\": 14, \"legendX\": 800, \"legendY\": 160, \"orient\": \"none\", \"padding\": 10, \"titleFontSize\": 16, \"titlePadding\": 15}}, \"x\": {\"type\": \"quantitative\", \"field\": \"truth_threshold\"}, \"y\": {\"field\": \"value\", \"type\": \"quantitative\", \"axis\": {\"labelFontSize\": 12, \"title\": \"Performance metric score\", \"titleFontSize\": 18, \"titlePadding\": 10, \"values\": {\"expr\": \"[0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85,0.9,0.95,1.0]\"}}, \"scale\": {\"domain\": [0.5, 1]}}}}, {\"layer\": [{\"mark\": {\"type\": \"rule\", \"color\": \"gray\"}, \"encoding\": {\"x\": {\"field\": \"truth_threshold\", \"title\": null, \"type\": \"quantitative\"}}}, {\"layer\": [{\"mark\": {\"type\": \"rect\", \"fill\": \"whitesmoke\", \"x\": 200, \"x2\": 10, \"y2Offset\": 20, \"yOffset\": -20}, \"encoding\": {\"y2\": {\"field\": \"score_index\"}}}, {\"mark\": {\"type\": \"text\", \"align\": \"right\", \"baseline\": \"middle\", \"fontSize\": 16, \"x\": 200, \"xOffset\": -10}}], \"encoding\": {\"color\": {\"field\": \"metric\", \"sort\": [\"precision\", \"recall\", \"f1\"]}, \"text\": {\"field\": \"y_text\"}, \"y\": {\"field\": \"score_index\", \"type\": \"quantitative\"}}, \"transform\": [{\"filter\": {\"param\": \"metric\", \"empty\": true}}]}, {\"mark\": {\"type\": \"text\", \"fontSize\": 14, \"fontWeight\": \"bold\", \"xOffset\": 20, \"y\": 0, \"yOffset\": -10}, \"encoding\": {\"text\": {\"condition\": {\"param\": \"threshold\", \"aggregate\": \"min\", \"empty\": false, \"field\": \"truth_threshold\", \"format\": \"+.2f\", \"type\": \"nominal\"}, \"value\": \"\"}, \"x\": {\"field\": \"truth_threshold\", \"type\": \"quantitative\"}}}], \"transform\": [{\"filter\": {\"param\": \"threshold\", \"empty\": false}}]}], \"description\": \"Accuracy chart\", \"height\": 700, \"transform\": [{\"fold\": [\"precision\", \"recall\", \"f1\"], \"as\": [\"metric\", \"value\"]}, {\"calculate\": \"0.6375 - 0.025*indexof(['precision', 'recall', 'f1'], datum.metric)\", \"as\": \"score_index\"}, {\"calculate\": \"{'precision': 'Precision (PPV)', 'recall': 'Recall (TPR)', 'specificity': 'Specificity (TNR)', 'accuracy': 'Accuracy', 'npv': 'NPV', 'f1': 'F1', 'f2': 'F2', 'f0_5': 'F0.5', 'p4': 'P4', 'phi': '\\u03c6 (MCC)'}[datum.metric]\", \"as\": \"metric_text\"}, {\"calculate\": \"datum.metric_text + ' = ' + format(datum.value, ',.3g')\", \"as\": \"y_text\"}], \"width\": 500}], \"data\": {\"name\": \"data-e6e4f74b041d643e45e25962c535b12c\"}, \"title\": {\"text\": \"Match Threshold Selection Tool\", \"anchor\": \"middle\", \"baseline\": \"line-bottom\", \"fontSize\": 28, \"subtitle\": [\"Hover over either line graph to show Confusion Matrix (bottom left) and selected performance metrics (right).\", \"\", \"Click a legend value to show a specific evaluation metric. Shift + Click to show multiple metrics\"], \"subtitleFontSize\": 14, \"subtitleFontStyle\": \"italic\"}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.14.1.json\", \"datasets\": {\"data-e6e4f74b041d643e45e25962c535b12c\": [{\"truth_threshold\": -12.400000184774399, \"match_probability\": 0.00018498974370122882, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 1027.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1004.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.5056622353520434, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.49433776464795665, \"precision\": 1.0, \"recall\": 0.5056622353520434, \"specificity\": 1.0, \"npv\": 0.532805956258725, \"accuracy\": 0.6838790931989924, \"f1\": 0.671680837148463, \"f2\": 0.5611408589225221, \"f0_5\": 0.836455448770158, \"p4\": 0.6832397200141539, \"phi\": 0.5190566932914649}, {\"truth_threshold\": -6.100000090897083, \"match_probability\": 0.014369156816028038, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 1026.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1005.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.5051698670605613, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.4948301329394387, \"precision\": 1.0, \"recall\": 0.5051698670605613, \"specificity\": 1.0, \"npv\": 0.5325581395348837, \"accuracy\": 0.6835642317380353, \"f1\": 0.6712463199214916, \"f2\": 0.5606557377049181, \"f0_5\": 0.8361858190709046, \"p4\": 0.6829130052819856, \"phi\": 0.5186832603341437}, {\"truth_threshold\": -5.000000074505806, \"match_probability\": 0.030303028785498974, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 1024.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1007.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.5041851304775973, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.4958148695224028, \"precision\": 1.0, \"recall\": 0.5041851304775973, \"specificity\": 1.0, \"npv\": 0.5320631970260223, \"accuracy\": 0.6829345088161209, \"f1\": 0.6703764320785598, \"f2\": 0.559685177087888, \"f0_5\": 0.8356455035090583, \"p4\": 0.6822591980364565, \"phi\": 0.5179366297288623}, {\"truth_threshold\": -4.90000007301569, \"match_probability\": 0.032407497325934585, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 1019.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1012.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.5017232890201871, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.4982767109798129, \"precision\": 1.0, \"recall\": 0.5017232890201871, \"specificity\": 1.0, \"npv\": 0.530829856281873, \"accuracy\": 0.681360201511335, \"f1\": 0.6681967213114755, \"f2\": 0.5572569178606585, \"f0_5\": 0.834288521368921, \"p4\": 0.6806224540570874, \"phi\": 0.51607141114758}, {\"truth_threshold\": -4.6000000685453415, \"match_probability\": 0.039601660807737325, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 1018.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1013.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.5012309207287051, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.49876907927129494, \"precision\": 1.0, \"recall\": 0.5012309207287051, \"specificity\": 1.0, \"npv\": 0.530583873957368, \"accuracy\": 0.6810453400503779, \"f1\": 0.6677599212856674, \"f2\": 0.5567709472763072, \"f0_5\": 0.8340160576765525, \"p4\": 0.680294719867444, \"phi\": 0.515698597697778}, {\"truth_threshold\": -4.100000061094761, \"match_probability\": 0.0551013486283602, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 1017.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1014.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.5007385524372231, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.49926144756277696, \"precision\": 1.0, \"recall\": 0.5007385524372231, \"specificity\": 1.0, \"npv\": 0.5303381194997684, \"accuracy\": 0.6807304785894207, \"f1\": 0.6673228346456693, \"f2\": 0.5562848703642927, \"f0_5\": 0.8337432365961633, \"p4\": 0.6799668560937839, \"phi\": 0.5153258602676495}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.HConcatChart(...)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linker.evaluation.accuracy_analysis_from_labels_table(\n",
    "    labels_table, output_type=\"threshold_selection\", add_metrics=[\"f1\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e4396d",
   "metadata": {},
   "source": [
    "## Receiver operating characteristic curve\n",
    "\n",
    "A [ROC chart](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) shows how the number of false positives and false negatives varies depending on the match threshold chosen. The match threshold is the match weight chosen as a cutoff for which pairwise comparisons to accept as matches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01dd7eec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-07T09:03:06.068545Z",
     "iopub.status.busy": "2024-06-07T09:03:06.068277Z",
     "iopub.status.idle": "2024-06-07T09:03:06.496528Z",
     "shell.execute_reply": "2024-06-07T09:03:06.495858Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-78bc3af449494f198392218729449265.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-78bc3af449494f198392218729449265.vega-embed details,\n",
       "  #altair-viz-78bc3af449494f198392218729449265.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-78bc3af449494f198392218729449265\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-78bc3af449494f198392218729449265\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-78bc3af449494f198392218729449265\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-566a43f4ff082e8139ab51d5a4757be2\"}, \"mark\": {\"type\": \"line\", \"clip\": true, \"point\": true}, \"encoding\": {\"tooltip\": [{\"field\": \"truth_threshold\", \"format\": \".4f\", \"type\": \"quantitative\"}, {\"field\": \"match_probability\", \"format\": \".4%\", \"type\": \"quantitative\"}, {\"field\": \"fp_rate\", \"format\": \".4f\", \"title\": \"FP_rate\", \"type\": \"quantitative\"}, {\"field\": \"tp_rate\", \"format\": \".4f\", \"title\": \"TP_rate\", \"type\": \"quantitative\"}, {\"field\": \"tp\", \"format\": \",.0f\", \"title\": \"TP\", \"type\": \"quantitative\"}, {\"field\": \"tn\", \"format\": \",.0f\", \"title\": \"TN\", \"type\": \"quantitative\"}, {\"field\": \"fp\", \"format\": \",.0f\", \"title\": \"FP\", \"type\": \"quantitative\"}, {\"field\": \"fn\", \"format\": \",.0f\", \"title\": \"FN\", \"type\": \"quantitative\"}, {\"field\": \"precision\", \"format\": \".4f\", \"type\": \"quantitative\"}, {\"field\": \"recall\", \"format\": \".4f\", \"type\": \"quantitative\"}, {\"field\": \"f1\", \"format\": \".4f\", \"title\": \"F1\", \"type\": \"quantitative\"}], \"x\": {\"field\": \"fp_rate\", \"sort\": [\"truth_threshold\"], \"title\": \"False Positive Rate amongst clerically reviewed records\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"tp_rate\", \"sort\": [\"truth_threshold\"], \"title\": \"True Positive Rate amongst clerically reviewed records\", \"type\": \"quantitative\"}}, \"height\": 400, \"params\": [{\"name\": \"mouse_zoom\", \"select\": {\"type\": \"interval\", \"encodings\": [\"x\"]}, \"bind\": \"scales\"}], \"title\": \"Receiver operating characteristic curve\", \"width\": 400, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.9.3.json\", \"datasets\": {\"data-566a43f4ff082e8139ab51d5a4757be2\": [{\"truth_threshold\": -12.400000184774399, \"match_probability\": 0.00018498974370122882, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 1027.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1004.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.5056622353520434, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.49433776464795665, \"precision\": 1.0, \"recall\": 0.5056622353520434, \"specificity\": 1.0, \"npv\": 0.532805956258725, \"accuracy\": 0.6838790931989924, \"f1\": 0.671680837148463, \"f2\": 0.5611408589225221, \"f0_5\": 0.836455448770158, \"p4\": 0.6832397200141539, \"phi\": 0.5190566932914649}, {\"truth_threshold\": -6.100000090897083, \"match_probability\": 0.014369156816028038, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 1026.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1005.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.5051698670605613, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.4948301329394387, \"precision\": 1.0, \"recall\": 0.5051698670605613, \"specificity\": 1.0, \"npv\": 0.5325581395348837, \"accuracy\": 0.6835642317380353, \"f1\": 0.6712463199214916, \"f2\": 0.5606557377049181, \"f0_5\": 0.8361858190709046, \"p4\": 0.6829130052819856, \"phi\": 0.5186832603341437}, {\"truth_threshold\": -5.000000074505806, \"match_probability\": 0.030303028785498974, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 1024.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1007.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.5041851304775973, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.4958148695224028, \"precision\": 1.0, \"recall\": 0.5041851304775973, \"specificity\": 1.0, \"npv\": 0.5320631970260223, \"accuracy\": 0.6829345088161209, \"f1\": 0.6703764320785598, \"f2\": 0.559685177087888, \"f0_5\": 0.8356455035090583, \"p4\": 0.6822591980364565, \"phi\": 0.5179366297288623}, {\"truth_threshold\": -4.90000007301569, \"match_probability\": 0.032407497325934585, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 1019.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1012.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.5017232890201871, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.4982767109798129, \"precision\": 1.0, \"recall\": 0.5017232890201871, \"specificity\": 1.0, \"npv\": 0.530829856281873, \"accuracy\": 0.681360201511335, \"f1\": 0.6681967213114755, \"f2\": 0.5572569178606585, \"f0_5\": 0.834288521368921, \"p4\": 0.6806224540570874, \"phi\": 0.51607141114758}, {\"truth_threshold\": -4.6000000685453415, \"match_probability\": 0.039601660807737325, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 1018.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1013.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.5012309207287051, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.49876907927129494, \"precision\": 1.0, \"recall\": 0.5012309207287051, \"specificity\": 1.0, \"npv\": 0.530583873957368, \"accuracy\": 0.6810453400503779, \"f1\": 0.6677599212856674, \"f2\": 0.5567709472763072, \"f0_5\": 0.8340160576765525, \"p4\": 0.680294719867444, \"phi\": 0.515698597697778}, {\"truth_threshold\": -4.100000061094761, \"match_probability\": 0.0551013486283602, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 1017.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1014.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.5007385524372231, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.49926144756277696, \"precision\": 1.0, \"recall\": 0.5007385524372231, \"specificity\": 1.0, \"npv\": 0.5303381194997684, \"accuracy\": 0.6807304785894207, \"f1\": 0.6673228346456693, \"f2\": 0.5562848703642927, \"f0_5\": 0.8337432365961633, \"p4\": 0.6799668560937839, \"phi\": 0.5153258602676495}, {\"truth_threshold\": -4.000000059604645, \"match_probability\": 0.05882352712444066, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 1006.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1025.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.49532250123092075, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5046774987690793, \"precision\": 1.0, \"recall\": 0.49532250123092075, \"specificity\": 1.0, \"npv\": 0.5276497695852534, \"accuracy\": 0.6772670025188917, \"f1\": 0.6624958840961476, \"f2\": 0.5509309967141293, \"f0_5\": 0.8307184145334434, \"p4\": 0.6763516632891752, \"phi\": 0.5112306755711034}, {\"truth_threshold\": -3.9000000581145287, \"match_probability\": 0.06278043839004852, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 997.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1034.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.49089118660758246, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5091088133924175, \"precision\": 1.0, \"recall\": 0.49089118660758246, \"specificity\": 1.0, \"npv\": 0.5254703992657183, \"accuracy\": 0.674433249370277, \"f1\": 0.6585204755614267, \"f2\": 0.5465409494572964, \"f0_5\": 0.8282106662236252, \"p4\": 0.6733816166373302, \"phi\": 0.5078865895283203}, {\"truth_threshold\": -3.500000052154064, \"match_probability\": 0.08121030044424019, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 996.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1035.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.49039881831610044, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5096011816838996, \"precision\": 1.0, \"recall\": 0.49039881831610044, \"specificity\": 1.0, \"npv\": 0.5252293577981652, \"accuracy\": 0.6741183879093199, \"f1\": 0.6580773042616452, \"f2\": 0.5460526315789473, \"f0_5\": 0.827930174563591, \"p4\": 0.6730509183540228, \"phi\": 0.5075153755396427}, {\"truth_threshold\": -2.9000000432133675, \"match_probability\": 0.11814376082605058, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 994.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1037.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.4894140817331364, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5105859182668636, \"precision\": 1.0, \"recall\": 0.4894140817331364, \"specificity\": 1.0, \"npv\": 0.5247479376718607, \"accuracy\": 0.6734886649874056, \"f1\": 0.6571900826446281, \"f2\": 0.5450756744900197, \"f0_5\": 0.8273680705843183, \"p4\": 0.6723890998562475, \"phi\": 0.5067731544360166}, {\"truth_threshold\": -1.5000000223517418, \"match_probability\": 0.2612038719739489, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 966.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1065.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.4756277695716396, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5243722304283605, \"precision\": 1.0, \"recall\": 0.4756277695716396, \"specificity\": 1.0, \"npv\": 0.5180995475113123, \"accuracy\": 0.6646725440806045, \"f1\": 0.6446446446446447, \"f2\": 0.5313531353135313, \"f0_5\": 0.8193384223918575, \"p4\": 0.6630623177686907, \"phi\": 0.4964096415249014}, {\"truth_threshold\": -0.5000000074505806, \"match_probability\": 0.41421356112001384, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 960.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1071.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.4726735598227474, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5273264401772526, \"precision\": 1.0, \"recall\": 0.4726735598227474, \"specificity\": 1.0, \"npv\": 0.5166967509025271, \"accuracy\": 0.6627833753148614, \"f1\": 0.641925777331996, \"f2\": 0.5284015852047557, \"f0_5\": 0.8175779253960143, \"p4\": 0.6610481781257823, \"phi\": 0.49419519685843255}, {\"truth_threshold\": -0.10000000149011612, \"match_probability\": 0.48267825490990723, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 959.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1072.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.47218119153126537, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5278188084687346, \"precision\": 1.0, \"recall\": 0.47218119153126537, \"specificity\": 1.0, \"npv\": 0.5164636896707262, \"accuracy\": 0.6624685138539043, \"f1\": 0.6414715719063545, \"f2\": 0.5279092810745348, \"f0_5\": 0.8172831089142663, \"p4\": 0.6607119325939106, \"phi\": 0.49382632612220784}, {\"truth_threshold\": 0.20000000298023224, \"match_probability\": 0.5346019618947252, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 957.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1074.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.4711964549483013, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5288035450516987, \"precision\": 1.0, \"recall\": 0.4711964549483013, \"specificity\": 1.0, \"npv\": 0.51599819738621, \"accuracy\": 0.6618387909319899, \"f1\": 0.6405622489959839, \"f2\": 0.5269243475388173, \"f0_5\": 0.8166922683051715, \"p4\": 0.6600389602879736, \"phi\": 0.49308875607551217}, {\"truth_threshold\": 0.6000000089406967, \"match_probability\": 0.6024989422185573, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 955.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1076.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.47021171836533726, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5297882816346627, \"precision\": 1.0, \"recall\": 0.47021171836533726, \"specificity\": 1.0, \"npv\": 0.5155335434488969, \"accuracy\": 0.6612090680100756, \"f1\": 0.6396517079705292, \"f2\": 0.5259389800638837, \"f0_5\": 0.8160998119979491, \"p4\": 0.6593653425793322, \"phi\": 0.49235141244854475}, {\"truth_threshold\": 1.0000000149011612, \"match_probability\": 0.6666666689619328, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 952.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1079.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.4687346134908912, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5312653865091088, \"precision\": 1.0, \"recall\": 0.4687346134908912, \"specificity\": 1.0, \"npv\": 0.5148381294964028, \"accuracy\": 0.660264483627204, \"f1\": 0.6382836071069393, \"f2\": 0.5244601145879242, \"f0_5\": 0.8152080835759548, \"p4\": 0.6583536959994251, \"phi\": 0.491245815900624}, {\"truth_threshold\": 1.3000000193715096, \"match_probability\": 0.7111737233641148, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 949.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1082.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.4672575086164451, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5327424913835549, \"precision\": 1.0, \"recall\": 0.4672575086164451, \"specificity\": 1.0, \"npv\": 0.5141445891333633, \"accuracy\": 0.6593198992443325, \"f1\": 0.6369127516778523, \"f2\": 0.5229802711341343, \"f0_5\": 0.8143126823408272, \"p4\": 0.6573405717493672, \"phi\": 0.4901407142720151}, {\"truth_threshold\": 1.600000023841858, \"match_probability\": 0.7519492561137834, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 947.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1084.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.46627277203348105, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.533727227966519, \"precision\": 1.0, \"recall\": 0.46627277203348105, \"specificity\": 1.0, \"npv\": 0.5136832660385823, \"accuracy\": 0.6586901763224181, \"f1\": 0.6359973136333109, \"f2\": 0.5219931650314188, \"f0_5\": 0.8137136965114281, \"p4\": 0.6566643270207304, \"phi\": 0.48940425049545894}, {\"truth_threshold\": 1.9000000283122063, \"match_probability\": 0.7886787621992872, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 943.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1088.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.46430329886755295, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.535696701132447, \"precision\": 1.0, \"recall\": 0.46430329886755295, \"specificity\": 1.0, \"npv\": 0.5127630989699955, \"accuracy\": 0.6574307304785895, \"f1\": 0.6341627437794216, \"f2\": 0.5200176464100584, \"f0_5\": 0.8125107702912286, \"p4\": 0.6553098282363409, \"phi\": 0.4879319608196603}, {\"truth_threshold\": 2.1000000312924385, \"match_probability\": 0.8108601793810092, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 939.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1092.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.4623338257016248, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5376661742983752, \"precision\": 1.0, \"recall\": 0.4623338257016248, \"specificity\": 1.0, \"npv\": 0.5118462226195798, \"accuracy\": 0.6561712846347607, \"f1\": 0.6323232323232323, \"f2\": 0.5180403839788149, \"f0_5\": 0.8113011923276309, \"p4\": 0.6539526192956943, \"phi\": 0.48646050433168353}, {\"truth_threshold\": 2.2000000327825546, \"match_probability\": 0.8212623941099038, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 935.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1096.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.4603643525356967, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5396356474643033, \"precision\": 1.0, \"recall\": 0.4603643525356967, \"specificity\": 1.0, \"npv\": 0.5109326193663543, \"accuracy\": 0.654911838790932, \"f1\": 0.6304787592717465, \"f2\": 0.5160613754277514, \"f0_5\": 0.8100849072950962, \"p4\": 0.6525926625805016, \"phi\": 0.48498986020736484}, {\"truth_threshold\": 2.3000000342726707, \"match_probability\": 0.8312116004280432, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 926.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1105.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.45593303791235845, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5440669620876416, \"precision\": 1.0, \"recall\": 0.45593303791235845, \"specificity\": 1.0, \"npv\": 0.5088888888888888, \"accuracy\": 0.6520780856423174, \"f1\": 0.6263104497801826, \"f2\": 0.5116022099447514, \"f0_5\": 0.8073234524847428, \"p4\": 0.6495225157687339, \"phi\": 0.48168377289561637}, {\"truth_threshold\": 2.400000035762787, \"match_probability\": 0.8407144092272857, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 923.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1108.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.4544559330379124, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5455440669620877, \"precision\": 1.0, \"recall\": 0.4544559330379124, \"specificity\": 1.0, \"npv\": 0.5082112738570794, \"accuracy\": 0.6511335012594458, \"f1\": 0.6249153689911984, \"f2\": 0.5101138498949929, \"f0_5\": 0.8063952472479469, \"p4\": 0.6484959234103079, \"phi\": 0.4805825929443398}, {\"truth_threshold\": 2.7000000402331352, \"match_probability\": 0.8666314458464526, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 903.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1128.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.4446085672082718, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5553914327917282, \"precision\": 1.0, \"recall\": 0.4446085672082718, \"specificity\": 1.0, \"npv\": 0.5037395512538495, \"accuracy\": 0.6448362720403022, \"f1\": 0.6155419222903885, \"f2\": 0.5001661681621802, \"f0_5\": 0.8001063264221159, \"p4\": 0.6416094363472876, \"phi\": 0.47325143436561484}, {\"truth_threshold\": 2.8000000417232513, \"match_probability\": 0.8744413378412453, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 901.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1130.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.44362383062530775, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5563761693746923, \"precision\": 1.0, \"recall\": 0.44362383062530775, \"specificity\": 1.0, \"npv\": 0.5032967032967033, \"accuracy\": 0.6442065491183879, \"f1\": 0.6145975443383356, \"f2\": 0.49916897506925206, \"f0_5\": 0.7994676131322094, \"p4\": 0.6409166024701175, \"phi\": 0.47251921808279124}, {\"truth_threshold\": 2.9000000432133675, \"match_probability\": 0.8818562391739494, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 898.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1133.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.44214672575086167, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5578532742491383, \"precision\": 1.0, \"recall\": 0.44214672575086167, \"specificity\": 1.0, \"npv\": 0.5026338893766462, \"accuracy\": 0.6432619647355163, \"f1\": 0.6131785592352339, \"f2\": 0.4976723564619818, \"f0_5\": 0.7985061355148497, \"p4\": 0.6398758844531804, \"phi\": 0.4714211794555957}, {\"truth_threshold\": 3.0000000447034836, \"match_probability\": 0.8888888919492438, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 891.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1140.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.43870014771048743, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5612998522895125, \"precision\": 1.0, \"recall\": 0.43870014771048743, \"specificity\": 1.0, \"npv\": 0.5010940919037199, \"accuracy\": 0.6410579345088161, \"f1\": 0.6098562628336756, \"f2\": 0.49417637271214643, \"f0_5\": 0.7962466487935657, \"p4\": 0.6374406028285706, \"phi\": 0.46886037594897534}, {\"truth_threshold\": 3.1000000461935997, \"match_probability\": 0.8955524998434058, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 888.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1143.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.43722304283604135, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5627769571639586, \"precision\": 1.0, \"recall\": 0.43722304283604135, \"specificity\": 1.0, \"npv\": 0.5004370629370629, \"accuracy\": 0.6401133501259446, \"f1\": 0.6084275436793423, \"f2\": 0.49267643142476697, \"f0_5\": 0.7952713594841483, \"p4\": 0.6363938931197374, \"phi\": 0.4677634181990659}, {\"truth_threshold\": 3.300000049173832, \"match_probability\": 0.9078269283845571, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 886.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1145.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.4362383062530773, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5637616937469226, \"precision\": 1.0, \"recall\": 0.4362383062530773, \"specificity\": 1.0, \"npv\": 0.5, \"accuracy\": 0.6394836272040302, \"f1\": 0.6074734316078162, \"f2\": 0.4916759156492786, \"f0_5\": 0.7946188340807175, \"p4\": 0.635695067264574, \"phi\": 0.4670322827455707}, {\"truth_threshold\": 3.500000052154064, \"match_probability\": 0.9187896995557598, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 874.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1157.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.43032988675529293, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5696701132447071, \"precision\": 1.0, \"recall\": 0.43032988675529293, \"specificity\": 1.0, \"npv\": 0.49739357080799307, \"accuracy\": 0.635705289672544, \"f1\": 0.6017211703958691, \"f2\": 0.48566348077350524, \"f0_5\": 0.7906640130269585, \"p4\": 0.6314846590963767, \"phi\": 0.46264815897030703}, {\"truth_threshold\": 3.6000000536441803, \"match_probability\": 0.9238137785296746, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 872.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1159.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.4293451501723289, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5706548498276711, \"precision\": 1.0, \"recall\": 0.4293451501723289, \"specificity\": 1.0, \"npv\": 0.4969618055555556, \"accuracy\": 0.6350755667506297, \"f1\": 0.6007578367206339, \"f2\": 0.48465984882169855, \"f0_5\": 0.7899981880775503, \"p4\": 0.6307799634934296, \"phi\": 0.4619178942584512}, {\"truth_threshold\": 3.7000000551342964, \"match_probability\": 0.9285512128432143, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 867.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1164.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.42688330871491875, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5731166912850812, \"precision\": 1.0, \"recall\": 0.42688330871491875, \"specificity\": 1.0, \"npv\": 0.4958856647899524, \"accuracy\": 0.6335012594458438, \"f1\": 0.598343685300207, \"f2\": 0.48214881548214883, \"f0_5\": 0.7883251500272777, \"p4\": 0.629014444873201, \"phi\": 0.4600927225351777}, {\"truth_threshold\": 3.8000000566244125, \"match_probability\": 0.9330154225613858, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 856.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1175.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.42146725750861647, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5785327424913835, \"precision\": 1.0, \"recall\": 0.42146725750861647, \"specificity\": 1.0, \"npv\": 0.49353448275862066, \"accuracy\": 0.6300377833753149, \"f1\": 0.5930031174229303, \"f2\": 0.4766146993318486, \"f0_5\": 0.7846012832263978, \"p4\": 0.6251109156992851, \"phi\": 0.45607962565127746}, {\"truth_threshold\": 3.9000000581145287, \"match_probability\": 0.9372195616099515, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 852.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1179.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.4194977843426883, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5805022156573116, \"precision\": 1.0, \"recall\": 0.4194977843426883, \"specificity\": 1.0, \"npv\": 0.4926850258175559, \"accuracy\": 0.6287783375314862, \"f1\": 0.5910509885535901, \"f2\": 0.47459893048128343, \"f0_5\": 0.7832322118036403, \"p4\": 0.6236846938036701, \"phi\": 0.45462102537089605}, {\"truth_threshold\": 4.000000059604645, \"match_probability\": 0.9411764728755594, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 832.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1199.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.4096504185130478, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5903495814869523, \"precision\": 1.0, \"recall\": 0.4096504185130478, \"specificity\": 1.0, \"npv\": 0.488481228668942, \"accuracy\": 0.6224811083123426, \"f1\": 0.5812085225288159, \"f2\": 0.4644930772666369, \"f0_5\": 0.7762642284008211, \"p4\": 0.6164974082601424, \"phi\": 0.44733269471390064}, {\"truth_threshold\": 4.300000064074993, \"match_probability\": 0.9516868803254299, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 825.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1206.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.40620384047267355, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5937961595273265, \"precision\": 1.0, \"recall\": 0.40620384047267355, \"specificity\": 1.0, \"npv\": 0.48702679710761376, \"accuracy\": 0.6202770780856424, \"f1\": 0.5777310924369747, \"f2\": 0.46094535702313105, \"f0_5\": 0.7737760270118177, \"p4\": 0.6139589751589127, \"phi\": 0.44478326789372186}, {\"truth_threshold\": 4.400000065565109, \"match_probability\": 0.9547759482410569, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 822.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1209.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.40472673559822747, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5952732644017725, \"precision\": 1.0, \"recall\": 0.40472673559822747, \"specificity\": 1.0, \"npv\": 0.48640611724723876, \"accuracy\": 0.6193324937027708, \"f1\": 0.576235541535226, \"f2\": 0.45942320590207913, \"f0_5\": 0.7727016356457981, \"p4\": 0.6128673226148784, \"phi\": 0.4436908383193005}, {\"truth_threshold\": 4.700000070035458, \"match_probability\": 0.9629520927573305, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 821.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1210.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.40423436730674545, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5957656326932546, \"precision\": 1.0, \"recall\": 0.40423436730674545, \"specificity\": 1.0, \"npv\": 0.4861995753715499, \"accuracy\": 0.6190176322418136, \"f1\": 0.5757363253856943, \"f2\": 0.4589155953046395, \"f0_5\": 0.7723424270931326, \"p4\": 0.6125029320545232, \"phi\": 0.44332671669450147}, {\"truth_threshold\": 4.800000071525574, \"match_probability\": 0.9653471069144568, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 817.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1214.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.40226489414081734, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5977351058591827, \"precision\": 1.0, \"recall\": 0.40226489414081734, \"specificity\": 1.0, \"npv\": 0.4853751589656634, \"accuracy\": 0.6177581863979849, \"f1\": 0.5737359550561798, \"f2\": 0.4568840174477128, \"f0_5\": 0.7709001698433666, \"p4\": 0.611042815748838, \"phi\": 0.4418703281958464}, {\"truth_threshold\": 4.90000007301569, \"match_probability\": 0.9675925026740654, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 811.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1220.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.3993106843919252, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6006893156080748, \"precision\": 1.0, \"recall\": 0.3993106843919252, \"specificity\": 1.0, \"npv\": 0.48414376321353064, \"accuracy\": 0.6158690176322418, \"f1\": 0.5707248416608023, \"f2\": 0.45383324006715164, \"f0_5\": 0.7687203791469195, \"p4\": 0.6088448866523514, \"phi\": 0.43968599867732555}, {\"truth_threshold\": 5.000000074505806, \"match_probability\": 0.969696971214501, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 771.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1260.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.37961595273264404, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.620384047267356, \"precision\": 1.0, \"recall\": 0.37961595273264404, \"specificity\": 1.0, \"npv\": 0.4760914760914761, \"accuracy\": 0.6032745591939547, \"f1\": 0.550321199143469, \"f2\": 0.433389544688027, \"f0_5\": 0.7536656891495601, \"p4\": 0.5939408006943253, \"phi\": 0.42512576878419933}, {\"truth_threshold\": 5.100000075995922, \"match_probability\": 0.9716687817966767, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 756.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1275.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.3722304283604136, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6277695716395865, \"precision\": 1.0, \"recall\": 0.3722304283604136, \"specificity\": 1.0, \"npv\": 0.4731404958677686, \"accuracy\": 0.5985516372795969, \"f1\": 0.542518837459634, \"f2\": 0.42567567567567566, \"f0_5\": 0.7477744807121661, \"p4\": 0.5882313967029971, \"phi\": 0.41966330486655373}, {\"truth_threshold\": 6.000000089406967, \"match_probability\": 0.9846153855541349, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 750.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1281.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.36927621861152143, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6307237813884786, \"precision\": 1.0, \"recall\": 0.36927621861152143, \"specificity\": 1.0, \"npv\": 0.47197032151690027, \"accuracy\": 0.5966624685138538, \"f1\": 0.5393743257820928, \"f2\": 0.42258282623394183, \"f0_5\": 0.7453786523553966, \"p4\": 0.5859280050558678, \"phi\": 0.41747744325487207}, {\"truth_threshold\": 6.200000092387199, \"match_probability\": 0.9865801893041345, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 728.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1303.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.3584441161989168, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6415558838010832, \"precision\": 1.0, \"recall\": 0.3584441161989168, \"specificity\": 1.0, \"npv\": 0.46772875816993464, \"accuracy\": 0.589735516372796, \"f1\": 0.5277274374773469, \"f2\": 0.41120650700406686, \"f0_5\": 0.7363949018814485, \"p4\": 0.5773816708195095, \"phi\": 0.4094564950553833}, {\"truth_threshold\": 6.3000000938773155, \"match_probability\": 0.987467611228855, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 726.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1305.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.35745937961595275, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6425406203840472, \"precision\": 1.0, \"recall\": 0.35745937961595275, \"specificity\": 1.0, \"npv\": 0.4673469387755102, \"accuracy\": 0.5891057934508817, \"f1\": 0.5266594124047879, \"f2\": 0.4101694915254237, \"f0_5\": 0.7355623100303952, \"p4\": 0.5765966357046058, \"phi\": 0.40872673854313535}, {\"truth_threshold\": 6.400000095367432, \"match_probability\": 0.9882970460445225, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 723.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1308.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.3559822747415066, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6440177252584933, \"precision\": 1.0, \"recall\": 0.3559822747415066, \"specificity\": 1.0, \"npv\": 0.4667753770892784, \"accuracy\": 0.5881612090680101, \"f1\": 0.5250544662309368, \"f2\": 0.40861308918277384, \"f0_5\": 0.7343083485679464, \"p4\": 0.5754164781246025, \"phi\": 0.40763189341557404}, {\"truth_threshold\": 6.500000096857548, \"match_probability\": 0.9890721936212699, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 711.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1320.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.3500738552437223, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6499261447562777, \"precision\": 1.0, \"recall\": 0.3500738552437223, \"specificity\": 1.0, \"npv\": 0.4645030425963489, \"accuracy\": 0.5843828715365239, \"f1\": 0.5185995623632386, \"f2\": 0.40237691001697795, \"f0_5\": 0.7292307692307692, \"p4\": 0.5706640029441143, \"phi\": 0.40324976242292176}, {\"truth_threshold\": 6.800000101327896, \"match_probability\": 0.9911056147706719, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 702.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1329.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.345642540620384, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.654357459379616, \"precision\": 1.0, \"recall\": 0.345642540620384, \"specificity\": 1.0, \"npv\": 0.46281325788197253, \"accuracy\": 0.5815491183879093, \"f1\": 0.5137211855104281, \"f2\": 0.39768864717878993, \"f0_5\": 0.7253564786112833, \"p4\": 0.5670653210867429, \"phi\": 0.3999599358524826}, {\"truth_threshold\": 7.000000104308128, \"match_probability\": 0.9922480625716311, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 700.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1331.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.34465780403741997, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.65534219596258, \"precision\": 1.0, \"recall\": 0.34465780403741997, \"specificity\": 1.0, \"npv\": 0.4624394184168013, \"accuracy\": 0.5809193954659949, \"f1\": 0.5126327352618089, \"f2\": 0.39664551223934724, \"f0_5\": 0.724487683709377, \"p4\": 0.5662615170898467, \"phi\": 0.39922844895106907}, {\"truth_threshold\": 7.1000001057982445, \"match_probability\": 0.9927634299608046, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 697.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1334.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.3431806991629739, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6568193008370261, \"precision\": 1.0, \"recall\": 0.3431806991629739, \"specificity\": 1.0, \"npv\": 0.4618797902379992, \"accuracy\": 0.5799748110831234, \"f1\": 0.5109970674486803, \"f2\": 0.39507992291123456, \"f0_5\": 0.7231790827972608, \"p4\": 0.5650529747120447, \"phi\": 0.39813091985316124}, {\"truth_threshold\": 7.400000110268593, \"match_probability\": 0.9941140817673122, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 696.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1335.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.34268833087149186, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6573116691285081, \"precision\": 1.0, \"recall\": 0.34268833087149186, \"specificity\": 1.0, \"npv\": 0.46169354838709675, \"accuracy\": 0.5796599496221663, \"f1\": 0.5104510451045104, \"f2\": 0.3945578231292517, \"f0_5\": 0.7227414330218068, \"p4\": 0.5646493659013554, \"phi\": 0.3977649952810209}, {\"truth_threshold\": 7.500000111758709, \"match_probability\": 0.9945060786121668, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 690.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1341.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.3397341211225997, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6602658788774003, \"precision\": 1.0, \"recall\": 0.3397341211225997, \"specificity\": 1.0, \"npv\": 0.46057924376508447, \"accuracy\": 0.5777707808564232, \"f1\": 0.5071664829106945, \"f2\": 0.39142273655547993, \"f0_5\": 0.720100187852223, \"p4\": 0.5622196307198002, \"phi\": 0.3955685586441907}, {\"truth_threshold\": 7.600000113248825, \"match_probability\": 0.9948721034855129, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 688.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1343.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.33874938453963566, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6612506154603643, \"precision\": 1.0, \"recall\": 0.33874938453963566, \"specificity\": 1.0, \"npv\": 0.46020900321543406, \"accuracy\": 0.5771410579345088, \"f1\": 0.5060684075027584, \"f2\": 0.39037675896504764, \"f0_5\": 0.7192138825005226, \"p4\": 0.5614066094767297, \"phi\": 0.394836062941099}, {\"truth_threshold\": 7.700000114738941, \"match_probability\": 0.9952138598197071, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 687.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1344.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.33825701624815363, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6617429837518464, \"precision\": 1.0, \"recall\": 0.33825701624815363, \"specificity\": 1.0, \"npv\": 0.4600241060666935, \"accuracy\": 0.5768261964735516, \"f1\": 0.5055187637969095, \"f2\": 0.38985359210078313, \"f0_5\": 0.7187696170747018, \"p4\": 0.560999510044096, \"phi\": 0.39446974728151696}, {\"truth_threshold\": 7.800000116229057, \"match_probability\": 0.9955329415617687, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 686.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1345.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.3377646479566716, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6622353520433284, \"precision\": 1.0, \"recall\": 0.3377646479566716, \"specificity\": 1.0, \"npv\": 0.4598393574297189, \"accuracy\": 0.5765113350125944, \"f1\": 0.5049687154950313, \"f2\": 0.38933030646992056, \"f0_5\": 0.7183246073298429, \"p4\": 0.5605920163437905, \"phi\": 0.3941033857744324}, {\"truth_threshold\": 7.900000117719173, \"match_probability\": 0.99583083992065, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 684.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1347.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.33677991137370755, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6632200886262924, \"precision\": 1.0, \"recall\": 0.33677991137370755, \"specificity\": 1.0, \"npv\": 0.45947030497592295, \"accuracy\": 0.5758816120906801, \"f1\": 0.5038674033149171, \"f2\": 0.388283378746594, \"f0_5\": 0.7174323473882945, \"p4\": 0.5597758409315445, \"phi\": 0.3933705232838903}, {\"truth_threshold\": 8.00000011920929, \"match_probability\": 0.9961089497366072, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 683.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1348.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.3362875430822255, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6637124569177745, \"precision\": 1.0, \"recall\": 0.3362875430822255, \"specificity\": 1.0, \"npv\": 0.4592860008022463, \"accuracy\": 0.575566750629723, \"f1\": 0.5033161385408991, \"f2\": 0.3877597365731804, \"f0_5\": 0.7169850934285115, \"p4\": 0.5593671566036346, \"phi\": 0.3930040213303783}, {\"truth_threshold\": 8.200000122189522, \"match_probability\": 0.9966109369567457, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 677.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1354.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.3333333333333333, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6666666666666666, \"precision\": 1.0, \"recall\": 0.3333333333333333, \"specificity\": 1.0, \"npv\": 0.45818327330932374, \"accuracy\": 0.5736775818639799, \"f1\": 0.5, \"f2\": 0.38461538461538464, \"f0_5\": 0.7142857142857143, \"p4\": 0.5569066147859922, \"phi\": 0.39080398893790036}, {\"truth_threshold\": 8.300000123679638, \"match_probability\": 0.9968371745531442, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 675.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1356.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.33234859675036926, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6676514032496307, \"precision\": 1.0, \"recall\": 0.33234859675036926, \"specificity\": 1.0, \"npv\": 0.4578168732506997, \"accuracy\": 0.5730478589420654, \"f1\": 0.49889135254988914, \"f2\": 0.38356631435390387, \"f0_5\": 0.7133798351299937, \"p4\": 0.5560831885340557, \"phi\": 0.3900702441785476}, {\"truth_threshold\": 8.400000125169754, \"match_probability\": 0.9970483543414643, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 674.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1357.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.33185622845888724, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6681437715411127, \"precision\": 1.0, \"recall\": 0.33185622845888724, \"specificity\": 1.0, \"npv\": 0.45763389288569145, \"accuracy\": 0.5727329974811083, \"f1\": 0.4983364140480592, \"f2\": 0.38304160036371904, \"f0_5\": 0.712925745716099, \"p4\": 0.5556708607973319, \"phi\": 0.38970329445361884}, {\"truth_threshold\": 8.50000012665987, \"match_probability\": 0.997245472756309, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 673.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1358.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.3313638601674052, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6686361398325947, \"precision\": 1.0, \"recall\": 0.3313638601674052, \"specificity\": 1.0, \"npv\": 0.4574510587295246, \"accuracy\": 0.5724181360201511, \"f1\": 0.4977810650887574, \"f2\": 0.38251676707968624, \"f0_5\": 0.7124708871479991, \"p4\": 0.5552581215106543, \"phi\": 0.3893362925008169}, {\"truth_threshold\": 8.600000128149986, \"match_probability\": 0.9974294610402847, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 672.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1359.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.3308714918759232, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6691285081240768, \"precision\": 1.0, \"recall\": 0.3308714918759232, \"specificity\": 1.0, \"npv\": 0.45726837060702874, \"accuracy\": 0.572103274559194, \"f1\": 0.4972253052164262, \"f2\": 0.3819918144611187, \"f0_5\": 0.7120152574698029, \"p4\": 0.5548449693107866, \"phi\": 0.38896923782019077}, {\"truth_threshold\": 8.700000129640102, \"match_probability\": 0.997601189412643, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 668.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1363.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.3289020187099951, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.671097981290005, \"precision\": 1.0, \"recall\": 0.3289020187099951, \"specificity\": 1.0, \"npv\": 0.4565390749601276, \"accuracy\": 0.5708438287153652, \"f1\": 0.494998147462023, \"f2\": 0.37989080982711554, \"f0_5\": 0.710184988305337, \"p4\": 0.5531882039410483, \"phi\": 0.3875004817730937}, {\"truth_threshold\": 8.800000131130219, \"match_probability\": 0.997761470983937, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 664.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1367.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.326932545544067, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6730674544559331, \"precision\": 1.0, \"recall\": 0.326932545544067, \"specificity\": 1.0, \"npv\": 0.45581210191082805, \"accuracy\": 0.5695843828715366, \"f1\": 0.49276437847866417, \"f2\": 0.377787892580792, \"f0_5\": 0.7083422231704715, \"p4\": 0.5515247216052599, \"phi\": 0.3860308417309408}, {\"truth_threshold\": 8.900000132620335, \"match_probability\": 0.9979110654305032, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 660.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1371.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.3249630723781389, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6750369276218612, \"precision\": 1.0, \"recall\": 0.3249630723781389, \"specificity\": 1.0, \"npv\": 0.455087440381558, \"accuracy\": 0.5683249370277078, \"f1\": 0.49052396878483834, \"f2\": 0.3756830601092896, \"f0_5\": 0.7064868336544637, \"p4\": 0.5498544326420134, \"phi\": 0.384560285036162}, {\"truth_threshold\": 9.00000013411045, \"match_probability\": 0.9980506824420605, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 659.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1372.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.32447070408665685, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6755292959133432, \"precision\": 1.0, \"recall\": 0.32447070408665685, \"specificity\": 1.0, \"npv\": 0.454906634882797, \"accuracy\": 0.5680100755667506, \"f1\": 0.4899628252788104, \"f2\": 0.3751565524308323, \"f0_5\": 0.7060209985001071, \"p4\": 0.5494357862329947, \"phi\": 0.3841924987738736}, {\"truth_threshold\": 9.100000135600567, \"match_probability\": 0.9981809849551747, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 653.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1378.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.32151649433776464, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6784835056622354, \"precision\": 1.0, \"recall\": 0.32151649433776464, \"specificity\": 1.0, \"npv\": 0.453824811732065, \"accuracy\": 0.5661209068010076, \"f1\": 0.48658718330849476, \"f2\": 0.3719949868975732, \"f0_5\": 0.7032091320267069, \"p4\": 0.5469147740904808, \"phi\": 0.38198450559098546}, {\"truth_threshold\": 9.300000138580799, \"match_probability\": 0.9984160824655384, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 651.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1380.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.3205317577548006, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6794682422451994, \"precision\": 1.0, \"recall\": 0.3205317577548006, \"specificity\": 1.0, \"npv\": 0.4534653465346535, \"accuracy\": 0.5654911838790933, \"f1\": 0.4854586129753915, \"f2\": 0.37094017094017095, \"f0_5\": 0.7022653721682848, \"p4\": 0.5460709222975572, \"phi\": 0.3812480093136779}, {\"truth_threshold\": 9.400000140070915, \"match_probability\": 0.9985219959137808, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 649.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1382.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.31954702117183653, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6804529788281635, \"precision\": 1.0, \"recall\": 0.31954702117183653, \"specificity\": 1.0, \"npv\": 0.45310645033636726, \"accuracy\": 0.5648614609571788, \"f1\": 0.4843283582089552, \"f2\": 0.36988487404536646, \"f0_5\": 0.701318348822131, \"p4\": 0.5452252937050418, \"phi\": 0.3805112567043593}, {\"truth_threshold\": 9.500000141561031, \"match_probability\": 0.9986208369212233, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 648.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1383.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.3190546528803545, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6809453471196455, \"precision\": 1.0, \"recall\": 0.3190546528803545, \"specificity\": 1.0, \"npv\": 0.45292721518987344, \"accuracy\": 0.5645465994962217, \"f1\": 0.48376259798432253, \"f2\": 0.3693570451436389, \"f0_5\": 0.7008436080467229, \"p4\": 0.5448018093975668, \"phi\": 0.38014278294145043}, {\"truth_threshold\": 9.600000143051147, \"match_probability\": 0.9987130764898899, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 640.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1391.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.3151157065484983, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6848842934515017, \"precision\": 1.0, \"recall\": 0.3151157065484983, \"specificity\": 1.0, \"npv\": 0.4514984227129338, \"accuracy\": 0.5620277078085643, \"f1\": 0.47922126544365407, \"f2\": 0.3651300775901415, \"f0_5\": 0.6970159006752341, \"p4\": 0.5413976563199846, \"phi\": 0.3771925827461599}, {\"truth_threshold\": 9.700000144541264, \"match_probability\": 0.9987991544181472, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 637.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1394.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.31363860167405216, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6863613983259478, \"precision\": 1.0, \"recall\": 0.31363860167405216, \"specificity\": 1.0, \"npv\": 0.4509649468294604, \"accuracy\": 0.5610831234256927, \"f1\": 0.4775112443778111, \"f2\": 0.36354297454628465, \"f0_5\": 0.6955667176239354, \"p4\": 0.5401135374024728, \"phi\": 0.3760851171312224}, {\"truth_threshold\": 9.80000014603138, \"match_probability\": 0.9988794813467569, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 631.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1400.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.31068439192516, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6893156080748399, \"precision\": 1.0, \"recall\": 0.31068439192516, \"specificity\": 1.0, \"npv\": 0.449901768172888, \"accuracy\": 0.5591939546599496, \"f1\": 0.4740796393688956, \"f2\": 0.36036550542547113, \"f0_5\": 0.6926454445664105, \"p4\": 0.5375326892816356, \"phi\": 0.37386823517229706}, {\"truth_threshold\": 9.900000147521496, \"match_probability\": 0.9989544406735176, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 619.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1412.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.3047759724273757, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6952240275726244, \"precision\": 1.0, \"recall\": 0.3047759724273757, \"specificity\": 1.0, \"npv\": 0.44779037935080174, \"accuracy\": 0.5554156171284634, \"f1\": 0.4671698113207547, \"f2\": 0.35399748370124673, \"f0_5\": 0.6867095629021522, \"p4\": 0.532319348798713, \"phi\": 0.36942624204333946}, {\"truth_threshold\": 10.000000149011612, \"match_probability\": 0.9990243903445719, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 613.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1418.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.3018217626784835, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6981782373215165, \"precision\": 1.0, \"recall\": 0.3018217626784835, \"specificity\": 1.0, \"npv\": 0.44674209910261414, \"accuracy\": 0.5535264483627204, \"f1\": 0.4636913767019667, \"f2\": 0.350806913128076, \"f0_5\": 0.6836939549408878, \"p4\": 0.5296861510411321, \"phi\": 0.3672008820983914}, {\"truth_threshold\": 10.100000150501728, \"match_probability\": 0.9990896645300149, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 611.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1420.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.30083702609551943, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6991629739044806, \"precision\": 1.0, \"recall\": 0.30083702609551943, \"specificity\": 1.0, \"npv\": 0.44639376218323584, \"accuracy\": 0.552896725440806, \"f1\": 0.46252838758516274, \"f2\": 0.34974241556954777, \"f0_5\": 0.6826815642458101, \"p4\": 0.5288043991760993, \"phi\": 0.3664584176721763}, {\"truth_threshold\": 10.200000151991844, \"match_probability\": 0.9991505751910027, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 593.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1438.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.2919743968488429, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.708025603151157, \"precision\": 1.0, \"recall\": 0.2919743968488429, \"specificity\": 1.0, \"npv\": 0.443283004258614, \"accuracy\": 0.5472292191435768, \"f1\": 0.45198170731707316, \"f2\": 0.34013995640702077, \"f0_5\": 0.6734044969339087, \"p4\": 0.5207754894170561, \"phi\": 0.35976004197485845}, {\"truth_threshold\": 10.30000015348196, \"match_probability\": 0.9992074135451509, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 592.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1439.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.2914820285573609, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.7085179714426391, \"precision\": 1.0, \"recall\": 0.2914820285573609, \"specificity\": 1.0, \"npv\": 0.44311145510835914, \"accuracy\": 0.5469143576826196, \"f1\": 0.45139153640869234, \"f2\": 0.33960532354290957, \"f0_5\": 0.6728802000454649, \"p4\": 0.5203244044266416, \"phi\": 0.3593870139723867}, {\"truth_threshold\": 10.400000154972076, \"match_probability\": 0.9992604514366183, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 579.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1452.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.28508124076809455, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.7149187592319055, \"precision\": 1.0, \"recall\": 0.28508124076809455, \"specificity\": 1.0, \"npv\": 0.4408933384674625, \"accuracy\": 0.5428211586901763, \"f1\": 0.4436781609195402, \"f2\": 0.33264391589107206, \"f0_5\": 0.665976535541753, \"p4\": 0.5144102851795553, \"phi\": 0.3545284473447676}, {\"truth_threshold\": 10.600000157952309, \"match_probability\": 0.9993561239419685, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 570.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1461.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.28064992614475626, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.7193500738552437, \"precision\": 1.0, \"recall\": 0.28064992614475626, \"specificity\": 1.0, \"npv\": 0.43937068303914045, \"accuracy\": 0.5399874055415617, \"f1\": 0.43829296424452135, \"f2\": 0.3278122843340235, \"f0_5\": 0.6610995128740431, \"p4\": 0.5102600262107828, \"phi\": 0.35115431044642736}, {\"truth_threshold\": 10.700000159442425, \"match_probability\": 0.9993992164911604, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 564.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1467.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.2776957163958641, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.7223042836041359, \"precision\": 1.0, \"recall\": 0.2776957163958641, \"specificity\": 1.0, \"npv\": 0.4383614088820827, \"accuracy\": 0.5380982367758187, \"f1\": 0.4346820809248555, \"f2\": 0.324585635359116, \"f0_5\": 0.6578026592022393, \"p4\": 0.5074669616635656, \"phi\": 0.34889982155313615}, {\"truth_threshold\": 10.800000160932541, \"match_probability\": 0.9994394266126935, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 559.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1472.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.27523387493845397, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.724766125061546, \"precision\": 1.0, \"recall\": 0.27523387493845397, \"specificity\": 1.0, \"npv\": 0.4375238823079862, \"accuracy\": 0.5365239294710328, \"f1\": 0.43166023166023165, \"f2\": 0.3218933548312795, \"f0_5\": 0.6550269510194516, \"p4\": 0.5051230066125968, \"phi\": 0.34701785761793746}, {\"truth_threshold\": 10.900000162422657, \"match_probability\": 0.9994769469006325, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 546.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1485.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.2688330871491876, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.7311669128508124, \"precision\": 1.0, \"recall\": 0.2688330871491876, \"specificity\": 1.0, \"npv\": 0.435361216730038, \"accuracy\": 0.5324307304785895, \"f1\": 0.42374854481955765, \"f2\": 0.314878892733564, \"f0_5\": 0.6476868327402135, \"p4\": 0.4989569646924318, \"phi\": 0.34211036219115415}, {\"truth_threshold\": 11.000000163912773, \"match_probability\": 0.9995119571076428, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 532.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1499.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.2619399310684392, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.7380600689315608, \"precision\": 1.0, \"recall\": 0.2619399310684392, \"specificity\": 1.0, \"npv\": 0.43305597579425115, \"accuracy\": 0.5280226700251889, \"f1\": 0.4151385095591104, \"f2\": 0.30730129390018485, \"f0_5\": 0.6395768213512864, \"p4\": 0.49219626037015723, \"phi\": 0.33680061230395913}, {\"truth_threshold\": 11.10000016540289, \"match_probability\": 0.9995446249976983, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 527.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1504.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.25947808961102903, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.740521910388971, \"precision\": 1.0, \"recall\": 0.25947808961102903, \"specificity\": 1.0, \"npv\": 0.4322385805964515, \"accuracy\": 0.5264483627204031, \"f1\": 0.41204065676309615, \"f2\": 0.3045890648479945, \"f0_5\": 0.6366272046388016, \"p4\": 0.489750317956443, \"phi\": 0.3348976577244906}, {\"truth_threshold\": 11.200000166893005, \"match_probability\": 0.9995751071426191, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 519.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1512.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.2555391432791728, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.7444608567208272, \"precision\": 1.0, \"recall\": 0.2555391432791728, \"specificity\": 1.0, \"npv\": 0.4309371471584494, \"accuracy\": 0.5239294710327456, \"f1\": 0.40705882352941175, \"f2\": 0.3002429711905588, \"f0_5\": 0.6318480642804967, \"p4\": 0.48580131404368865, \"phi\": 0.3318453094320319}, {\"truth_threshold\": 11.300000168383121, \"match_probability\": 0.9996035496660847, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 499.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1532.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.24569177744953224, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.7543082225504677, \"precision\": 1.0, \"recall\": 0.24569177744953224, \"specificity\": 1.0, \"npv\": 0.42771759432200224, \"accuracy\": 0.517632241813602, \"f1\": 0.39446640316205533, \"f2\": 0.2893424562217326, \"f0_5\": 0.6195679165631984, \"p4\": 0.47572994995033335, \"phi\": 0.32417078214331824}, {\"truth_threshold\": 11.400000169873238, \"match_probability\": 0.99963008893853, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 473.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1558.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.2328902018709995, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.7671097981290005, \"precision\": 1.0, \"recall\": 0.2328902018709995, \"specificity\": 1.0, \"npv\": 0.42360340362560117, \"accuracy\": 0.5094458438287154, \"f1\": 0.3777955271565495, \"f2\": 0.27509596370827033, \"f0_5\": 0.6028549579403518, \"p4\": 0.46218369067653414, \"phi\": 0.3140908820446858}, {\"truth_threshold\": 11.500000171363354, \"match_probability\": 0.999654852226126, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 470.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1561.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.23141309699655344, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.7685869030034466, \"precision\": 1.0, \"recall\": 0.23141309699655344, \"specificity\": 1.0, \"npv\": 0.4231337767923134, \"accuracy\": 0.5085012594458438, \"f1\": 0.3758496601359456, \"f2\": 0.2734465906446358, \"f0_5\": 0.600869342879059, \"p4\": 0.46058564943838975, \"phi\": 0.3129196346210279}, {\"truth_threshold\": 11.60000017285347, \"match_probability\": 0.9996779582968373, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 451.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1580.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.2220580994583949, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.7779419005416052, \"precision\": 1.0, \"recall\": 0.2220580994583949, \"specificity\": 1.0, \"npv\": 0.42018348623853213, \"accuracy\": 0.5025188916876574, \"f1\": 0.36341659951651895, \"f2\": 0.2629737609329446, \"f0_5\": 0.5880052151238592, \"p4\": 0.4502862275083601, \"phi\": 0.30545891111233126}, {\"truth_threshold\": 11.700000174343586, \"match_probability\": 0.9996995179863626, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 443.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1588.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.21811915312653865, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.7818808468734614, \"precision\": 1.0, \"recall\": 0.21811915312653865, \"specificity\": 1.0, \"npv\": 0.4189535309184047, \"accuracy\": 0.5, \"f1\": 0.3581244947453517, \"f2\": 0.25855025096299755, \"f0_5\": 0.5824349198001578, \"p4\": 0.4458538540579834, \"phi\": 0.30229420994007733}, {\"truth_threshold\": 11.900000177323818, \"match_probability\": 0.9997384050389891, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 440.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1591.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.21664204825209257, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.7833579517479075, \"precision\": 1.0, \"recall\": 0.21664204825209257, \"specificity\": 1.0, \"npv\": 0.41849415204678364, \"accuracy\": 0.49905541561712846, \"f1\": 0.35613112100364225, \"f2\": 0.25688930406352173, \"f0_5\": 0.5803218148245846, \"p4\": 0.4441765200515326, \"phi\": 0.3011036869268423}, {\"truth_threshold\": 12.000000178813934, \"match_probability\": 0.9997559189953416, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 436.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1595.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.21467257508616444, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.7853274249138356, \"precision\": 1.0, \"recall\": 0.21467257508616444, \"specificity\": 1.0, \"npv\": 0.41788321167883213, \"accuracy\": 0.49779596977329976, \"f1\": 0.3534657478719092, \"f2\": 0.2546728971962617, \"f0_5\": 0.5774834437086093, \"p4\": 0.4419269349297068, \"phi\": 0.2995130466880727}, {\"truth_threshold\": 12.200000181794167, \"match_probability\": 0.9997875084304283, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 434.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1597.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.2136878385032004, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.7863121614967996, \"precision\": 1.0, \"recall\": 0.2136878385032004, \"specificity\": 1.0, \"npv\": 0.4175784099197666, \"accuracy\": 0.4971662468513854, \"f1\": 0.35212981744421906, \"f2\": 0.25356391680299134, \"f0_5\": 0.5760552163525352, \"p4\": 0.4407964459882871, \"phi\": 0.29871629989231974}, {\"truth_threshold\": 12.300000183284283, \"match_probability\": 0.9998017355340825, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 429.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1602.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.21122599704579026, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.7887740029542097, \"precision\": 1.0, \"recall\": 0.21122599704579026, \"specificity\": 1.0, \"npv\": 0.4168183472879505, \"accuracy\": 0.49559193954659947, \"f1\": 0.348780487804878, \"f2\": 0.2507891967730621, \"f0_5\": 0.5724579663730984, \"p4\": 0.43795337341921126, \"phi\": 0.2967201897291045}, {\"truth_threshold\": 12.400000184774399, \"match_probability\": 0.9998150102562988, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 425.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1606.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.20925652387986213, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.7907434761201378, \"precision\": 1.0, \"recall\": 0.20925652387986213, \"specificity\": 1.0, \"npv\": 0.4162122864412941, \"accuracy\": 0.49433249370277077, \"f1\": 0.34609120521172637, \"f2\": 0.24856708386945842, \"f0_5\": 0.5695523988206915, \"p4\": 0.43566133681892244, \"phi\": 0.2951188510698608}, {\"truth_threshold\": 12.500000186264515, \"match_probability\": 0.9998273963279586, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 412.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1619.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.20285573609059576, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.7971442639094042, \"precision\": 1.0, \"recall\": 0.20285573609059576, \"specificity\": 1.0, \"npv\": 0.4142547033285094, \"accuracy\": 0.49023929471032746, \"f1\": 0.3372902169463774, \"f2\": 0.24133083411433928, \"f0_5\": 0.5599347648817613, \"p4\": 0.42810118466525837, \"phi\": 0.28988608585562736}, {\"truth_threshold\": 12.600000187754631, \"match_probability\": 0.9998389532181915, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 410.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1621.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.2018709995076317, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.7981290004923683, \"precision\": 1.0, \"recall\": 0.2018709995076317, \"specificity\": 1.0, \"npv\": 0.4139551699204628, \"accuracy\": 0.4896095717884131, \"f1\": 0.33592789840229414, \"f2\": 0.24021560815561285, \"f0_5\": 0.5584309452465268, \"p4\": 0.4269226353344527, \"phi\": 0.2890770553039368}, {\"truth_threshold\": 12.700000189244747, \"match_probability\": 0.9998497364189812, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 402.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1629.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.19793205317577547, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8020679468242246, \"precision\": 1.0, \"recall\": 0.19793205317577547, \"specificity\": 1.0, \"npv\": 0.41276135544340303, \"accuracy\": 0.4870906801007557, \"f1\": 0.33045622688039455, \"f2\": 0.23574947220267417, \"f0_5\": 0.5523495465787304, \"p4\": 0.4221659841746226, \"phi\": 0.2858298489565581}, {\"truth_threshold\": 12.800000190734863, \"match_probability\": 0.9998597977108138, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 381.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1650.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.18759231905465287, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8124076809453471, \"precision\": 1.0, \"recall\": 0.18759231905465287, \"specificity\": 1.0, \"npv\": 0.40966010733452596, \"accuracy\": 0.4804785894206549, \"f1\": 0.31592039800995025, \"f2\": 0.2239858906525573, \"f0_5\": 0.5358649789029536, \"p4\": 0.4093429793942124, \"phi\": 0.27721668340679234}, {\"truth_threshold\": 12.90000019222498, \"match_probability\": 0.9998691854106266, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 361.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1670.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.1777449532250123, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8222550467749877, \"precision\": 1.0, \"recall\": 0.1777449532250123, \"specificity\": 1.0, \"npv\": 0.4067495559502664, \"accuracy\": 0.47418136020151136, \"f1\": 0.30183946488294316, \"f2\": 0.21272834413671185, \"f0_5\": 0.5194244604316547, \"p4\": 0.396646195182804, \"phi\": 0.26888228055540336}, {\"truth_threshold\": 13.300000198185444, \"match_probability\": 0.9999008579398913, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 354.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1677.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.17429837518463812, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8257016248153619, \"precision\": 1.0, \"recall\": 0.17429837518463812, \"specificity\": 1.0, \"npv\": 0.4057406094968108, \"accuracy\": 0.47197732997481107, \"f1\": 0.2968553459119497, \"f2\": 0.20877565463552725, \"f0_5\": 0.5134899912967799, \"p4\": 0.3920831758418028, \"phi\": 0.2659321886904984}, {\"truth_threshold\": 13.40000019967556, \"match_probability\": 0.999907496573012, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 352.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1679.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.17331363860167406, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.826686361398326, \"precision\": 1.0, \"recall\": 0.17331363860167406, \"specificity\": 1.0, \"npv\": 0.4054532577903683, \"accuracy\": 0.47134760705289674, \"f1\": 0.2954259336970206, \"f2\": 0.20764511562057575, \"f0_5\": 0.5117766792672288, \"p4\": 0.3907676489102373, \"phi\": 0.26508598490027957}, {\"truth_threshold\": 13.600000202655792, \"match_probability\": 0.9999194701253888, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 351.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1680.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.172821270310192, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.827178729689808, \"precision\": 1.0, \"recall\": 0.172821270310192, \"specificity\": 1.0, \"npv\": 0.40530973451327434, \"accuracy\": 0.47103274559193953, \"f1\": 0.2947103274559194, \"f2\": 0.20707964601769913, \"f0_5\": 0.5109170305676856, \"p4\": 0.3901078901006101, \"phi\": 0.26466231916854116}, {\"truth_threshold\": 13.700000204145908, \"match_probability\": 0.9999248625650565, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 347.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1684.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.1708517971442639, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8291482028557361, \"precision\": 1.0, \"recall\": 0.1708517971442639, \"specificity\": 1.0, \"npv\": 0.4047366560622128, \"accuracy\": 0.46977329974811083, \"f1\": 0.29184188393608074, \"f2\": 0.20481643253452958, \"f0_5\": 0.5074583211465341, \"p4\": 0.38745542187943405, \"phi\": 0.2629638474360856}, {\"truth_threshold\": 13.800000205636024, \"match_probability\": 0.999929893941616, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 344.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1687.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.16937469226981783, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8306253077301822, \"precision\": 1.0, \"recall\": 0.16937469226981783, \"specificity\": 1.0, \"npv\": 0.4043079096045198, \"accuracy\": 0.4688287153652393, \"f1\": 0.28968421052631577, \"f2\": 0.2031176192725555, \"f0_5\": 0.5048429703551511, \"p4\": 0.38545180714785676, \"phi\": 0.26168593346131325}, {\"truth_threshold\": 13.90000020712614, \"match_probability\": 0.9999345884275949, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 339.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1692.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.16691285081240767, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8330871491875923, \"precision\": 1.0, \"recall\": 0.16691285081240767, \"specificity\": 1.0, \"npv\": 0.4035953471977441, \"accuracy\": 0.4672544080604534, \"f1\": 0.28607594936708863, \"f2\": 0.20028358738036156, \"f0_5\": 0.5004428697962799, \"p4\": 0.3820848184886705, \"phi\": 0.2595481650395528}, {\"truth_threshold\": 14.000000208616257, \"match_probability\": 0.9999389685776376, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 331.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1700.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.16297390448055146, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8370260955194485, \"precision\": 1.0, \"recall\": 0.16297390448055146, \"specificity\": 1.0, \"npv\": 0.4024604569420035, \"accuracy\": 0.464735516372796, \"f1\": 0.2802709568162574, \"f2\": 0.19574216439976344, \"f0_5\": 0.4932935916542474, \"p4\": 0.3766241509696461, \"phi\": 0.25610652484242796}, {\"truth_threshold\": 14.200000211596489, \"match_probability\": 0.9999468686412301, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 317.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1714.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.15608074839980304, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.843919251600197, \"precision\": 1.0, \"recall\": 0.15608074839980304, \"specificity\": 1.0, \"npv\": 0.40048968170689053, \"accuracy\": 0.46032745591939545, \"f1\": 0.27001703577512776, \"f2\": 0.18777396043122851, \"f0_5\": 0.48044862079418005, \"p4\": 0.36684178397302697, \"phi\": 0.2500174578848653}, {\"truth_threshold\": 14.400000214576721, \"match_probability\": 0.9999537461476637, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 316.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1715.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.15558838010832102, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.844411619891679, \"precision\": 1.0, \"recall\": 0.15558838010832102, \"specificity\": 1.0, \"npv\": 0.40034965034965037, \"accuracy\": 0.4600125944584383, \"f1\": 0.26927993182786536, \"f2\": 0.1872037914691943, \"f0_5\": 0.4795144157814871, \"p4\": 0.366131657936778, \"phi\": 0.2495791529251488}, {\"truth_threshold\": 14.70000021904707, \"match_probability\": 0.9999624298714548, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 315.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1716.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.155096011816839, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.844903988183161, \"precision\": 1.0, \"recall\": 0.155096011816839, \"specificity\": 1.0, \"npv\": 0.400209716882209, \"accuracy\": 0.4596977329974811, \"f1\": 0.26854219948849106, \"f2\": 0.18663348738002133, \"f0_5\": 0.4785779398359161, \"p4\": 0.36541997841978086, \"phi\": 0.2491403840784887}, {\"truth_threshold\": 15.000000223517418, \"match_probability\": 0.9999694833578969, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 314.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1717.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.15460364352535697, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8453963564746431, \"precision\": 1.0, \"recall\": 0.15460364352535697, \"specificity\": 1.0, \"npv\": 0.4000698812019567, \"accuracy\": 0.4593828715365239, \"f1\": 0.2678038379530917, \"f2\": 0.1860630481156672, \"f0_5\": 0.4776391846668695, \"p4\": 0.36470673862472397, \"phi\": 0.2487011485670688}, {\"truth_threshold\": 15.100000225007534, \"match_probability\": 0.9999715269079685, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 307.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1724.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.15115706548498276, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8488429345150172, \"precision\": 1.0, \"recall\": 0.15115706548498276, \"specificity\": 1.0, \"npv\": 0.399093760892297, \"accuracy\": 0.45717884130982367, \"f1\": 0.262617621899059, \"f2\": 0.18206618431977226, \"f0_5\": 0.4710033752684873, \"p4\": 0.35966979322171594, \"phi\": 0.24561319539032303}, {\"truth_threshold\": 15.20000022649765, \"match_probability\": 0.9999734336151354, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 305.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1726.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.1501723289020187, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8498276710979813, \"precision\": 1.0, \"recall\": 0.1501723289020187, \"specificity\": 1.0, \"npv\": 0.39881574364332983, \"accuracy\": 0.45654911838790935, \"f1\": 0.2611301369863014, \"f2\": 0.18092300391505517, \"f0_5\": 0.46908643494309443, \"p4\": 0.3582162273053647, \"phi\": 0.24472655970635737}, {\"truth_threshold\": 15.300000227987766, \"match_probability\": 0.9999752126423825, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 302.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1729.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.14869522402757263, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8513047759724274, \"precision\": 1.0, \"recall\": 0.14869522402757263, \"specificity\": 1.0, \"npv\": 0.39839944328462074, \"accuracy\": 0.4556045340050378, \"f1\": 0.25889412773253323, \"f2\": 0.17920721576074056, \"f0_5\": 0.46619326952763196, \"p4\": 0.3560236322925244, \"phi\": 0.243392880897669}, {\"truth_threshold\": 15.400000229477882, \"match_probability\": 0.9999768725392036, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 297.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1734.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.14623338257016247, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8537666174298375, \"precision\": 1.0, \"recall\": 0.14623338257016247, \"specificity\": 1.0, \"npv\": 0.39770753733935393, \"accuracy\": 0.4540302267002519, \"f1\": 0.2551546391752577, \"f2\": 0.17634485215532597, \"f0_5\": 0.4613233923578751, \"p4\": 0.3523361639168977, \"phi\": 0.24115994372777358}, {\"truth_threshold\": 15.70000023394823, \"match_probability\": 0.9999812145830361, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 287.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1744.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.1413096996553422, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8586903003446578, \"precision\": 1.0, \"recall\": 0.1413096996553422, \"specificity\": 1.0, \"npv\": 0.39633091034960194, \"accuracy\": 0.4508816120906801, \"f1\": 0.24762726488352027, \"f2\": 0.17060991558673166, \"f0_5\": 0.45139981126140294, \"p4\": 0.34483355667090254, \"phi\": 0.23665460465756968}, {\"truth_threshold\": 15.900000236928463, \"match_probability\": 0.9999836463049459, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 279.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1752.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.13737075332348597, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8626292466765141, \"precision\": 1.0, \"recall\": 0.13737075332348597, \"specificity\": 1.0, \"npv\": 0.39523645150155334, \"accuracy\": 0.44836272040302266, \"f1\": 0.24155844155844156, \"f2\": 0.16601213852195645, \"f0_5\": 0.4432793136320305, \"p4\": 0.3387045984433298, \"phi\": 0.23301057719268842}, {\"truth_threshold\": 16.00000023841858, \"match_probability\": 0.9999847414462861, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 276.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1755.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.1358936484490399, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8641063515509602, \"precision\": 1.0, \"recall\": 0.1358936484490399, \"specificity\": 1.0, \"npv\": 0.39482758620689656, \"accuracy\": 0.4474181360201511, \"f1\": 0.23927178153446033, \"f2\": 0.16428571428571428, \"f0_5\": 0.44019138755980863, \"p4\": 0.33637622790175986, \"phi\": 0.23163454232472105}, {\"truth_threshold\": 16.100000239908695, \"match_probability\": 0.9999857632514492, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 274.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1757.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.13490891186607581, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8650910881339242, \"precision\": 1.0, \"recall\": 0.13490891186607581, \"specificity\": 1.0, \"npv\": 0.3945554789800138, \"accuracy\": 0.4467884130982368, \"f1\": 0.23774403470715835, \"f2\": 0.16313407954274828, \"f0_5\": 0.4381196034537896, \"p4\": 0.33481470493206833, \"phi\": 0.23071421789736327}, {\"truth_threshold\": 16.20000024139881, \"match_probability\": 0.9999867166312594, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 271.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1760.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.13343180699162974, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8665681930083703, \"precision\": 1.0, \"recall\": 0.13343180699162974, \"specificity\": 1.0, \"npv\": 0.39414802065404475, \"accuracy\": 0.44584382871536526, \"f1\": 0.23544743701129453, \"f2\": 0.16140559857057774, \"f0_5\": 0.434991974317817, \"p4\": 0.33245833489583054, \"phi\": 0.22932920140715485}, {\"truth_threshold\": 16.300000242888927, \"match_probability\": 0.9999876061677141, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 268.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1763.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.13195470211718366, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8680452978828164, \"precision\": 1.0, \"recall\": 0.13195470211718366, \"specificity\": 1.0, \"npv\": 0.3937414030261348, \"accuracy\": 0.4448992443324937, \"f1\": 0.23314484558503698, \"f2\": 0.15967588179218303, \"f0_5\": 0.43184015468901066, \"p4\": 0.3300848527615133, \"phi\": 0.22793865303523134}, {\"truth_threshold\": 16.400000244379044, \"match_probability\": 0.9999884361359999, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 264.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1767.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.12998522895125553, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8700147710487445, \"precision\": 1.0, \"recall\": 0.12998522895125553, \"specificity\": 1.0, \"npv\": 0.39320054945054944, \"accuracy\": 0.44363979848866497, \"f1\": 0.23006535947712417, \"f2\": 0.15736766809728184, \"f0_5\": 0.42759961127308066, \"p4\": 0.32689317655913785, \"phi\": 0.22607579137114425}, {\"truth_threshold\": 16.50000024586916, \"match_probability\": 0.9999892105250341, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 262.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1769.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.12900049236829148, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8709995076317085, \"precision\": 1.0, \"recall\": 0.12900049236829148, \"specificity\": 1.0, \"npv\": 0.39293067947838023, \"accuracy\": 0.44301007556675065, \"f1\": 0.2285215874400349, \"f2\": 0.1562127355115669, \"f0_5\": 0.425462812601494, \"p4\": 0.32528558993728013, \"phi\": 0.22514051416686065}, {\"truth_threshold\": 16.600000247359276, \"match_probability\": 0.9999899330566321, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 259.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1772.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.1275233874938454, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8724766125061546, \"precision\": 1.0, \"recall\": 0.1275233874938454, \"specificity\": 1.0, \"npv\": 0.39252656839218375, \"accuracy\": 0.4420654911838791, \"f1\": 0.2262008733624454, \"f2\": 0.15447930335202195, \"f0_5\": 0.42223671340071733, \"p4\": 0.3228593171458215, \"phi\": 0.2237326924763251}, {\"truth_threshold\": 16.700000248849392, \"match_probability\": 0.9999906072033913, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 249.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1782.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.12259970457902511, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8774002954209749, \"precision\": 1.0, \"recall\": 0.12259970457902511, \"specificity\": 1.0, \"npv\": 0.3911855141783396, \"accuracy\": 0.4389168765743073, \"f1\": 0.21842105263157896, \"f2\": 0.14869222500895737, \"f0_5\": 0.41129831516352827, \"p4\": 0.3146395889340626, \"phi\": 0.21899595538241903}, {\"truth_threshold\": 16.800000250339508, \"match_probability\": 0.9999912362053778, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 241.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1790.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.11866075824716889, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8813392417528311, \"precision\": 1.0, \"recall\": 0.11866075824716889, \"specificity\": 1.0, \"npv\": 0.3901192504258944, \"accuracy\": 0.4363979848866499, \"f1\": 0.21214788732394366, \"f2\": 0.14405260011954574, \"f0_5\": 0.4023372287145242, \"p4\": 0.3079124729407039, \"phi\": 0.2151553997982709}, {\"truth_threshold\": 16.900000251829624, \"match_probability\": 0.999991823085696, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 240.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1791.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.11816838995568685, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8818316100443131, \"precision\": 1.0, \"recall\": 0.11816838995568685, \"specificity\": 1.0, \"npv\": 0.38998637602179836, \"accuracy\": 0.4360831234256927, \"f1\": 0.21136063408190225, \"f2\": 0.14347202295552366, \"f0_5\": 0.4012036108324975, \"p4\": 0.30706183558280153, \"phi\": 0.214671987364791}, {\"truth_threshold\": 17.00000025331974, \"match_probability\": 0.9999923706650156, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 236.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1795.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.11619891678975874, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8838010832102413, \"precision\": 1.0, \"recall\": 0.11619891678975874, \"specificity\": 1.0, \"npv\": 0.38945578231292516, \"accuracy\": 0.43482367758186397, \"f1\": 0.20820467578297308, \"f2\": 0.14114832535885166, \"f0_5\": 0.39663865546218485, \"p4\": 0.30363716807944324, \"phi\": 0.21273067489732173}, {\"truth_threshold\": 17.100000254809856, \"match_probability\": 0.9999928815751264, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 235.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1796.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.11570654849827672, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8842934515017233, \"precision\": 1.0, \"recall\": 0.11570654849827672, \"specificity\": 1.0, \"npv\": 0.3893233594015641, \"accuracy\": 0.4345088161209068, \"f1\": 0.20741394527802295, \"f2\": 0.14056705347529608, \"f0_5\": 0.39548973409626387, \"p4\": 0.3027754178880268, \"phi\": 0.21224340311564244}, {\"truth_threshold\": 17.200000256299973, \"match_probability\": 0.999993358271586, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 234.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1797.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.11521418020679468, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8847858197932054, \"precision\": 1.0, \"recall\": 0.11521418020679468, \"specificity\": 1.0, \"npv\": 0.3891910265125765, \"accuracy\": 0.43419395465994964, \"f1\": 0.20662251655629138, \"f2\": 0.1399856424982053, \"f0_5\": 0.3943377148634985, \"p4\": 0.30191141261310905, \"phi\": 0.21175534246740363}, {\"truth_threshold\": 17.30000025779009, \"match_probability\": 0.999993803045519, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 230.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1801.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.11324470704086657, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8867552929591335, \"precision\": 1.0, \"recall\": 0.11324470704086657, \"specificity\": 1.0, \"npv\": 0.38866259334691106, \"accuracy\": 0.4329345088161209, \"f1\": 0.2034498009730208, \"f2\": 0.13765860665549437, \"f0_5\": 0.3896984073195527, \"p4\": 0.2984326196428951, \"phi\": 0.2097950941307122}, {\"truth_threshold\": 17.400000259280205, \"match_probability\": 0.9999942180346287, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 228.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1803.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.11225997045790251, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8877400295420975, \"precision\": 1.0, \"recall\": 0.11225997045790251, \"specificity\": 1.0, \"npv\": 0.3883989145183175, \"accuracy\": 0.43230478589420657, \"f1\": 0.20185922974767595, \"f2\": 0.13649425287356323, \"f0_5\": 0.3873598369011213, \"p4\": 0.2966794033341487, \"phi\": 0.2088100827778863}, {\"truth_threshold\": 17.50000026077032, \"match_probability\": 0.9999946052334694, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 224.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1807.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.11029049729197439, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8897095027080256, \"precision\": 1.0, \"recall\": 0.11029049729197439, \"specificity\": 1.0, \"npv\": 0.38787262872628725, \"accuracy\": 0.4310453400503778, \"f1\": 0.19866962305986696, \"f2\": 0.13416387158600862, \"f0_5\": 0.38264434574649814, \"p4\": 0.29314487543657275, \"phi\": 0.20683003918233825}, {\"truth_threshold\": 17.600000262260437, \"match_probability\": 0.999994966503032, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 218.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1813.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.10733628754308222, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8926637124569178, \"precision\": 1.0, \"recall\": 0.10733628754308222, \"specificity\": 1.0, \"npv\": 0.3870858688302907, \"accuracy\": 0.42915617128463474, \"f1\": 0.19386393952867942, \"f2\": 0.13066410932630065, \"f0_5\": 0.3754736479503961, \"p4\": 0.28777144299895174, \"phi\": 0.2038341485635611}, {\"truth_threshold\": 17.700000263750553, \"match_probability\": 0.9999953035796879, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 214.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1817.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.10536681437715412, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8946331856228459, \"precision\": 1.0, \"recall\": 0.10536681437715412, \"specificity\": 1.0, \"npv\": 0.3865631330182309, \"accuracy\": 0.42789672544080604, \"f1\": 0.19064587973273942, \"f2\": 0.12832813624370354, \"f0_5\": 0.37062694838933147, \"p4\": 0.2841402750546976, \"phi\": 0.20181904241617807}, {\"truth_threshold\": 17.80000026524067, \"match_probability\": 0.9999956180835331, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 212.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1819.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.10438207779419005, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8956179222058099, \"precision\": 1.0, \"recall\": 0.10438207779419005, \"specificity\": 1.0, \"npv\": 0.38630229419703105, \"accuracy\": 0.4272670025188917, \"f1\": 0.18903254569772626, \"f2\": 0.12715930902111325, \"f0_5\": 0.3681833970128517, \"p4\": 0.2823097318295965, \"phi\": 0.20080596635794612}, {\"truth_threshold\": 17.900000266730785, \"match_probability\": 0.9999959115261747, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 207.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1824.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.1019202363367799, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8980797636632201, \"precision\": 1.0, \"recall\": 0.1019202363367799, \"specificity\": 1.0, \"npv\": 0.3856517345907713, \"accuracy\": 0.4256926952141058, \"f1\": 0.18498659517426275, \"f2\": 0.12423478574000721, \"f0_5\": 0.3620146904512067, \"p4\": 0.2776889307543874, \"phi\": 0.19825669202622276}, {\"truth_threshold\": 18.0000002682209, \"match_probability\": 0.9999961853179954, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 205.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1826.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.10093549975381585, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8990645002461841, \"precision\": 1.0, \"recall\": 0.10093549975381585, \"specificity\": 1.0, \"npv\": 0.38539212386401883, \"accuracy\": 0.4250629722921914, \"f1\": 0.18336314847942756, \"f2\": 0.12306399327650379, \"f0_5\": 0.3595229743949491, \"p4\": 0.27582256169212693, \"phi\": 0.1972301868969333}, {\"truth_threshold\": 18.200000271201134, \"match_probability\": 0.9999966791247992, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 198.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1833.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.09748892171344166, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9025110782865583, \"precision\": 1.0, \"recall\": 0.09748892171344166, \"specificity\": 1.0, \"npv\": 0.3844862323707186, \"accuracy\": 0.4228589420654912, \"f1\": 0.17765814266487215, \"f2\": 0.11896178803172314, \"f0_5\": 0.35069075451647186, \"p4\": 0.26920710012797755, \"phi\": 0.1936056512798248}, {\"truth_threshold\": 18.30000027269125, \"match_probability\": 0.999996901513191, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 196.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1835.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.0965041851304776, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9034958148695224, \"precision\": 1.0, \"recall\": 0.0965041851304776, \"specificity\": 1.0, \"npv\": 0.38422818791946306, \"accuracy\": 0.4222292191435768, \"f1\": 0.1760215536596318, \"f2\": 0.11778846153846154, \"f0_5\": 0.3481349911190053, \"p4\": 0.2672927556600361, \"phi\": 0.19256071296951466}, {\"truth_threshold\": 18.400000274181366, \"match_probability\": 0.9999971090089864, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 193.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1838.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.09502708025603152, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9049729197439685, \"precision\": 1.0, \"recall\": 0.09502708025603152, \"specificity\": 1.0, \"npv\": 0.38384177003017095, \"accuracy\": 0.42128463476070527, \"f1\": 0.1735611510791367, \"f2\": 0.11602741373091259, \"f0_5\": 0.3442739921512665, \"p4\": 0.26440064321915874, \"phi\": 0.19098524206407744}, {\"truth_threshold\": 18.500000275671482, \"match_probability\": 0.9999973026094866, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 190.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1841.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.09354997538158542, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9064500246184146, \"precision\": 1.0, \"recall\": 0.09354997538158542, \"specificity\": 1.0, \"npv\": 0.38345612860013395, \"accuracy\": 0.42034005037783373, \"f1\": 0.1710941017559658, \"f2\": 0.11426509502044743, \"f0_5\": 0.3403797921891795, \"p4\": 0.2614834724836873, \"phi\": 0.18939987167487887}, {\"truth_threshold\": 18.600000277161598, \"match_probability\": 0.999997483245208, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 187.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1844.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.09207287050713935, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9079271294928607, \"precision\": 1.0, \"recall\": 0.09207287050713935, \"specificity\": 1.0, \"npv\": 0.3830712612914018, \"accuracy\": 0.4193954659949622, \"f1\": 0.16862037871956717, \"f2\": 0.11250150403080256, \"f0_5\": 0.3364519611370997, \"p4\": 0.2585408498527471, \"phi\": 0.18780434136592739}, {\"truth_threshold\": 18.700000278651714, \"match_probability\": 0.9999976517843541, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 186.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1845.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.0915805022156573, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9084194977843427, \"precision\": 1.0, \"recall\": 0.0915805022156573, \"specificity\": 1.0, \"npv\": 0.38294314381270905, \"accuracy\": 0.41908060453400503, \"f1\": 0.16779431664411368, \"f2\": 0.11191335740072202, \"f0_5\": 0.33513513513513515, \"p4\": 0.2575542504705109, \"phi\": 0.1872701936518745}, {\"truth_threshold\": 18.80000028014183, \"match_probability\": 0.9999978090369889, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 177.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1854.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.08714918759231906, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.912850812407681, \"precision\": 1.0, \"recall\": 0.08714918759231906, \"specificity\": 1.0, \"npv\": 0.3817939313104368, \"accuracy\": 0.4162468513853904, \"f1\": 0.16032608695652173, \"f2\": 0.10661366100469823, \"f0_5\": 0.3231106243154436, \"p4\": 0.24854306779885504, \"phi\": 0.18240896617595923}, {\"truth_threshold\": 18.900000281631947, \"match_probability\": 0.9999979557589296, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 173.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1858.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.08517971442639094, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9148202855736091, \"precision\": 1.0, \"recall\": 0.08517971442639094, \"specificity\": 1.0, \"npv\": 0.3812853812853813, \"accuracy\": 0.4149874055415617, \"f1\": 0.1569872958257713, \"f2\": 0.10425454983729059, \"f0_5\": 0.3176643408005876, \"p4\": 0.24446004916721792, \"phi\": 0.18021592574699485}, {\"truth_threshold\": 19.000000283122063, \"match_probability\": 0.9999980926553794, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 167.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1864.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.08222550467749877, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9177744953225012, \"precision\": 1.0, \"recall\": 0.08222550467749877, \"specificity\": 1.0, \"npv\": 0.3805250913924892, \"accuracy\": 0.41309823677581864, \"f1\": 0.15195632393084624, \"f2\": 0.10071161500422145, \"f0_5\": 0.30937384216376435, \"p4\": 0.2382423776518271, \"phi\": 0.17688659554132066}, {\"truth_threshold\": 19.10000028461218, \"match_probability\": 0.9999982203843173, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 164.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1867.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.08074839980305268, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9192516001969473, \"precision\": 1.0, \"recall\": 0.08074839980305268, \"specificity\": 1.0, \"npv\": 0.3801460823373174, \"accuracy\": 0.4121536523929471, \"f1\": 0.14943052391799544, \"f2\": 0.09893822393822393, \"f0_5\": 0.30517305545217716, \"p4\": 0.23509063941010752, \"phi\": 0.17520327576885625}, {\"truth_threshold\": 19.200000286102295, \"match_probability\": 0.9999983395596597, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 155.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1876.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.07631708517971443, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9236829148202855, \"precision\": 1.0, \"recall\": 0.07631708517971443, \"specificity\": 1.0, \"npv\": 0.3790135716650116, \"accuracy\": 0.4093198992443325, \"f1\": 0.14181152790484905, \"f2\": 0.09361033941297259, \"f0_5\": 0.29234251225952473, \"p4\": 0.22545812557563438, \"phi\": 0.1700741339329014}, {\"truth_threshold\": 19.30000028759241, \"match_probability\": 0.9999984507542113, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 139.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1892.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.06843919251600197, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.931560807483998, \"precision\": 1.0, \"recall\": 0.06843919251600197, \"specificity\": 1.0, \"npv\": 0.37701679288771817, \"accuracy\": 0.40428211586901763, \"f1\": 0.12811059907834102, \"f2\": 0.08410988745007866, \"f0_5\": 0.2686509470429068, \"p4\": 0.20764213874468843, \"phi\": 0.16063226596860353}, {\"truth_threshold\": 19.400000289082527, \"match_probability\": 0.9999985545024187, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 126.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1905.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.0620384047267356, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9379615952732644, \"precision\": 1.0, \"recall\": 0.0620384047267356, \"specificity\": 1.0, \"npv\": 0.37540983606557377, \"accuracy\": 0.4001889168765743, \"f1\": 0.1168289290681502, \"f2\": 0.07636363636363637, \"f0_5\": 0.2485207100591716, \"p4\": 0.19246681908459196, \"phi\": 0.1526100499581647}, {\"truth_threshold\": 19.500000290572643, \"match_probability\": 0.9999986513029383, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 125.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1906.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.06154603643525357, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9384539635647464, \"precision\": 1.0, \"recall\": 0.06154603643525357, \"specificity\": 1.0, \"npv\": 0.37528679121599473, \"accuracy\": 0.3998740554156171, \"f1\": 0.11595547309833024, \"f2\": 0.07576675960722512, \"f0_5\": 0.24693796918214145, \"p4\": 0.19127198374939863, \"phi\": 0.1519783357121962}, {\"truth_threshold\": 19.60000029206276, \"match_probability\": 0.9999987416210334, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 124.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1907.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.06105366814377154, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9389463318562284, \"precision\": 1.0, \"recall\": 0.06105366814377154, \"specificity\": 1.0, \"npv\": 0.3751638269986894, \"accuracy\": 0.39955919395465994, \"f1\": 0.11508120649651972, \"f2\": 0.07516973811833172, \"f0_5\": 0.24535021764938664, \"p4\": 0.19007310478285272, \"phi\": 0.15134440126124685}, {\"truth_threshold\": 19.700000293552876, \"match_probability\": 0.9999988258908107, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 123.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1908.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.060561299852289516, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9394387001477105, \"precision\": 1.0, \"recall\": 0.060561299852289516, \"specificity\": 1.0, \"npv\": 0.37504094333442517, \"accuracy\": 0.3992443324937028, \"f1\": 0.11420612813370473, \"f2\": 0.07457257184430702, \"f0_5\": 0.2437574316290131, \"p4\": 0.1888701585551805, \"phi\": 0.1507082181772502}, {\"truth_threshold\": 19.80000029504299, \"match_probability\": 0.9999989045173057, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 121.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1910.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.05957656326932546, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9404234367306745, \"precision\": 1.0, \"recall\": 0.05957656326932546, \"specificity\": 1.0, \"npv\": 0.37479541734860883, \"accuracy\": 0.3986146095717884, \"f1\": 0.11245353159851301, \"f2\": 0.07337780473013948, \"f0_5\": 0.24055666003976142, \"p4\": 0.18645196888541976, \"phi\": 0.1494289894723331}, {\"truth_threshold\": 19.900000296533108, \"match_probability\": 0.9999989778784306, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 113.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1918.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.05563761693746923, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9443623830625307, \"precision\": 1.0, \"recall\": 0.05563761693746923, \"specificity\": 1.0, \"npv\": 0.37381651975187724, \"accuracy\": 0.396095717884131, \"f1\": 0.10541044776119403, \"f2\": 0.06859293432074784, \"f0_5\": 0.22754732178815948, \"p4\": 0.1766116659113618, \"phi\": 0.14421601967483658}, {\"truth_threshold\": 20.10000029951334, \"match_probability\": 0.9999991101913761, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 111.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1920.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.05465288035450517, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9453471196454948, \"precision\": 1.0, \"recall\": 0.05465288035450517, \"specificity\": 1.0, \"npv\": 0.3735725938009788, \"accuracy\": 0.3954659949622166, \"f1\": 0.10364145658263306, \"f2\": 0.06739526411657559, \"f0_5\": 0.22424242424242424, \"p4\": 0.1741087023528203, \"phi\": 0.142887432172067}, {\"truth_threshold\": 20.200000301003456, \"match_probability\": 0.9999991697791492, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 109.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1922.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.05366814377154111, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9463318562284588, \"precision\": 1.0, \"recall\": 0.05366814377154111, \"specificity\": 1.0, \"npv\": 0.3733289859797848, \"accuracy\": 0.39483627204030225, \"f1\": 0.10186915887850467, \"f2\": 0.06619701202477833, \"f0_5\": 0.22091609241994326, \"p4\": 0.17158817188926406, \"phi\": 0.14154813207402897}, {\"truth_threshold\": 20.300000302493572, \"match_probability\": 0.9999992253765136, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 91.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1940.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.0448055145248646, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9551944854751354, \"precision\": 1.0, \"recall\": 0.0448055145248646, \"specificity\": 1.0, \"npv\": 0.3711507293354943, \"accuracy\": 0.3891687657430731, \"f1\": 0.08576814326107446, \"f2\": 0.05538648813146683, \"f0_5\": 0.18997912317327767, \"p4\": 0.1480768274225295, \"phi\": 0.1289558040343884}, {\"truth_threshold\": 20.40000030398369, \"match_probability\": 0.9999992772506945, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 86.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1945.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.04234367306745446, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9576563269325455, \"precision\": 1.0, \"recall\": 0.04234367306745446, \"specificity\": 1.0, \"npv\": 0.3705501618122977, \"accuracy\": 0.38759445843828716, \"f1\": 0.0812470477090222, \"f2\": 0.05237515225334957, \"f0_5\": 0.18105263157894738, \"p4\": 0.14126803374967498, \"phi\": 0.12526154600224398}, {\"truth_threshold\": 20.60000030696392, \"match_probability\": 0.9999993708101274, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 72.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1959.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.03545051698670606, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9645494830132939, \"precision\": 1.0, \"recall\": 0.03545051698670606, \"specificity\": 1.0, \"npv\": 0.36887886597938147, \"accuracy\": 0.38318639798488663, \"f1\": 0.06847360912981455, \"f2\": 0.043923865300146414, \"f0_5\": 0.15523932729624837, \"p4\": 0.12150944981378942, \"phi\": 0.11435447741316879}, {\"truth_threshold\": 21.000000312924385, \"match_probability\": 0.9999995231631726, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 71.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1960.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.034958148695224026, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9650418513047759, \"precision\": 1.0, \"recall\": 0.034958148695224026, \"specificity\": 1.0, \"npv\": 0.3687600644122383, \"accuracy\": 0.38287153652392947, \"f1\": 0.0675547098001903, \"f2\": 0.043319097010372176, \"f0_5\": 0.15334773218142547, \"p4\": 0.1200573002429353, \"phi\": 0.11353928467532028}, {\"truth_threshold\": 21.1000003144145, \"match_probability\": 0.9999995550954947, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 70.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1961.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.034465780403742, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.965534219596258, \"precision\": 1.0, \"recall\": 0.034465780403742, \"specificity\": 1.0, \"npv\": 0.3686413393432067, \"accuracy\": 0.3825566750629723, \"f1\": 0.06663493574488338, \"f2\": 0.0427141811081279, \"f0_5\": 0.15144958892254434, \"p4\": 0.11859951946108616, \"phi\": 0.11271872714657621}, {\"truth_threshold\": 21.200000315904617, \"match_probability\": 0.9999995848894065, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 69.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1962.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.033973412112259974, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.96602658788774, \"precision\": 1.0, \"recall\": 0.033973412112259974, \"specificity\": 1.0, \"npv\": 0.3685226906984229, \"accuracy\": 0.3822418136020151, \"f1\": 0.06571428571428571, \"f2\": 0.04210911753936287, \"f0_5\": 0.14954486345903772, \"p4\": 0.11713607088211223, \"phi\": 0.11189268628385163}, {\"truth_threshold\": 21.500000320374966, \"match_probability\": 0.9999996628254004, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 67.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1964.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.032988675529295915, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.967011324470704, \"precision\": 1.0, \"recall\": 0.032988675529295915, \"specificity\": 1.0, \"npv\": 0.3682856223866195, \"accuracy\": 0.38161209068010077, \"f1\": 0.06387035271687322, \"f2\": 0.04089854718593578, \"f0_5\": 0.14571552849064812, \"p4\": 0.11419202241119845, \"phi\": 0.11022365852672915}, {\"truth_threshold\": 21.600000321865082, \"match_probability\": 0.999999685404968, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 66.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1965.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.03249630723781388, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9675036927621861, \"precision\": 1.0, \"recall\": 0.03249630723781388, \"specificity\": 1.0, \"npv\": 0.36816720257234725, \"accuracy\": 0.38129722921914355, \"f1\": 0.06294706723891273, \"f2\": 0.040293040293040296, \"f0_5\": 0.1437908496732026, \"p4\": 0.11271134775969976, \"phi\": 0.10938041200177231}, {\"truth_threshold\": 21.700000323355198, \"match_probability\": 0.9999997064724503, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 62.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1969.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.03052683407188577, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9694731659281143, \"precision\": 1.0, \"recall\": 0.03052683407188577, \"specificity\": 1.0, \"npv\": 0.36769428387925496, \"accuracy\": 0.38003778337531485, \"f1\": 0.05924510272336359, \"f2\": 0.03786953334962131, \"f0_5\": 0.13602457218078104, \"p4\": 0.10673009231574156, \"phi\": 0.1059459408998895}, {\"truth_threshold\": 21.800000324845314, \"match_probability\": 0.999999726129107, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 55.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1976.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.02708025603151157, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9729197439684885, \"precision\": 1.0, \"recall\": 0.02708025603151157, \"specificity\": 1.0, \"npv\": 0.3668695930791413, \"accuracy\": 0.3778337531486146, \"f1\": 0.052732502396931925, \"f2\": 0.03362269226066757, \"f0_5\": 0.12216792536650378, \"p4\": 0.0960314132133735, \"phi\": 0.09967408143925688}, {\"truth_threshold\": 22.40000033378601, \"match_probability\": 0.9999998193125794, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 48.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1983.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.023633677991137372, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9763663220088626, \"precision\": 1.0, \"recall\": 0.023633677991137372, \"specificity\": 1.0, \"npv\": 0.36604859335038364, \"accuracy\": 0.3756297229219144, \"f1\": 0.046176046176046176, \"f2\": 0.02936857562408223, \"f0_5\": 0.10796221322537113, \"p4\": 0.08502610073875708, \"phi\": 0.09301115301054898}, {\"truth_threshold\": 22.800000339746475, \"match_probability\": 0.9999998630645361, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 47.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1984.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.023141309699655343, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9768586903003447, \"precision\": 1.0, \"recall\": 0.023141309699655343, \"specificity\": 1.0, \"npv\": 0.36593160754234577, \"accuracy\": 0.37531486146095716, \"f1\": 0.04523580365736285, \"f2\": 0.028760249663443888, \"f0_5\": 0.10590356016223525, \"p4\": 0.08342802927538613, \"phi\": 0.09202247909630645}, {\"truth_threshold\": 23.000000342726707, \"match_probability\": 0.999999880790753, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 46.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1985.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.022648941408173313, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9773510585918267, \"precision\": 1.0, \"recall\": 0.022648941408173313, \"specificity\": 1.0, \"npv\": 0.365814696485623, \"accuracy\": 0.375, \"f1\": 0.04429465575349061, \"f2\": 0.028151774785801713, \"f0_5\": 0.1038374717832957, \"p4\": 0.08182335429924091, \"phi\": 0.0910237091474061}, {\"truth_threshold\": 23.20000034570694, \"match_probability\": 0.9999998962223214, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 45.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1986.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.022156573116691284, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9778434268833087, \"precision\": 1.0, \"recall\": 0.022156573116691284, \"specificity\": 1.0, \"npv\": 0.3656978601085915, \"accuracy\": 0.37468513853904284, \"f1\": 0.04335260115606936, \"f2\": 0.027543150936467132, \"f0_5\": 0.10176390773405698, \"p4\": 0.08021203063702596, \"phi\": 0.09001450647597614}, {\"truth_threshold\": 23.400000348687172, \"match_probability\": 0.9999999096562825, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 44.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1987.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.021664204825209258, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9783357951747907, \"precision\": 1.0, \"recall\": 0.021664204825209258, \"specificity\": 1.0, \"npv\": 0.365581098339719, \"accuracy\": 0.3743702770780856, \"f1\": 0.042409638554216866, \"f2\": 0.02693437806072478, \"f0_5\": 0.09968282736746716, \"p4\": 0.07859401270561604, \"phi\": 0.08899451553133284}, {\"truth_threshold\": 23.600000351667404, \"match_probability\": 0.9999999213512251, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 43.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1988.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.02117183653372723, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9788281634662728, \"precision\": 1.0, \"recall\": 0.02117183653372723, \"specificity\": 1.0, \"npv\": 0.3654644111075646, \"accuracy\": 0.37405541561712846, \"f1\": 0.041465766634522665, \"f2\": 0.026325456103832495, \"f0_5\": 0.09759418974126191, \"p4\": 0.07696925450739521, \"phi\": 0.08796336038865413}, {\"truth_threshold\": 23.70000035315752, \"match_probability\": 0.9999999266180979, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 42.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1989.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.0206794682422452, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9793205317577548, \"precision\": 1.0, \"recall\": 0.0206794682422452, \"specificity\": 1.0, \"npv\": 0.3653477983407786, \"accuracy\": 0.3737405541561713, \"f1\": 0.04052098408104197, \"f2\": 0.025716385011021307, \"f0_5\": 0.09549795361527967, \"p4\": 0.0753377096255321, \"phi\": 0.08692064307839845}, {\"truth_threshold\": 23.900000356137753, \"match_probability\": 0.9999999361173434, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 41.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1990.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.02018709995076317, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9798129000492368, \"precision\": 1.0, \"recall\": 0.02018709995076317, \"specificity\": 1.0, \"npv\": 0.3652312599681021, \"accuracy\": 0.3734256926952141, \"f1\": 0.03957528957528957, \"f2\": 0.02510716472749541, \"f0_5\": 0.09339407744874716, \"p4\": 0.07369933121919056, \"phi\": 0.08586594173547067}, {\"truth_threshold\": 24.00000035762787, \"match_probability\": 0.9999999403953735, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 35.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1996.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.017232890201871, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.982767109798129, \"precision\": 1.0, \"recall\": 0.017232890201871, \"specificity\": 1.0, \"npv\": 0.36453358802929003, \"accuracy\": 0.371536523929471, \"f1\": 0.03388189738625363, \"f2\": 0.02144870694938105, \"f0_5\": 0.08060801473975127, \"p4\": 0.06372287901796007, \"phi\": 0.07925886257954269}, {\"truth_threshold\": 24.100000359117985, \"match_probability\": 0.9999999443869169, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 32.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1999.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.015755785327424915, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9842442146725751, \"precision\": 1.0, \"recall\": 0.015755785327424915, \"specificity\": 1.0, \"npv\": 0.3641857506361323, \"accuracy\": 0.37059193954659947, \"f1\": 0.031022782355792537, \"f2\": 0.0196174595389897, \"f0_5\": 0.07410838351088467, \"p4\": 0.0586384719748834, \"phi\": 0.07574980202172149}, {\"truth_threshold\": 24.2000003606081, \"match_probability\": 0.9999999481111586, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 31.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 2000.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.015263417035942885, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9847365829640571, \"precision\": 1.0, \"recall\": 0.015263417035942885, \"specificity\": 1.0, \"npv\": 0.3640699523052464, \"accuracy\": 0.3702770780856423, \"f1\": 0.030067895247332686, \"f2\": 0.01900674432863274, \"f0_5\": 0.07192575406032482, \"p4\": 0.0569290852372513, \"phi\": 0.0745449630242769}, {\"truth_threshold\": 24.300000362098217, \"match_probability\": 0.999999951585999, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 28.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 2003.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.013786312161496799, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9862136878385032, \"precision\": 1.0, \"recall\": 0.013786312161496799, \"specificity\": 1.0, \"npv\": 0.363722998729352, \"accuracy\": 0.36933249370277077, \"f1\": 0.027197668771248178, \"f2\": 0.01717369970559372, \"f0_5\": 0.06532897806812879, \"p4\": 0.051756446046609055, \"phi\": 0.07081241982024446}, {\"truth_threshold\": 24.50000036507845, \"match_probability\": 0.999999957853164, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 26.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 2005.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.012801575578532743, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9871984244214672, \"precision\": 1.0, \"recall\": 0.012801575578532743, \"specificity\": 1.0, \"npv\": 0.3634920634920635, \"accuracy\": 0.36870277078085645, \"f1\": 0.025279533300923675, \"f2\": 0.015950920245398775, \"f0_5\": 0.06088992974238876, \"p4\": 0.048270424636238894, \"phi\": 0.0682148893057115}, {\"truth_threshold\": 24.600000366568565, \"match_probability\": 0.9999999606756114, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 24.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 2007.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.011816838995568686, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9881831610044313, \"precision\": 1.0, \"recall\": 0.011816838995568686, \"specificity\": 1.0, \"npv\": 0.36326142131979694, \"accuracy\": 0.36807304785894207, \"f1\": 0.02335766423357664, \"f2\": 0.014727540500736377, \"f0_5\": 0.056417489421720736, \"p4\": 0.04475382343492924, \"phi\": 0.06551794967058633}, {\"truth_threshold\": 24.800000369548798, \"match_probability\": 0.9999999657661313, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 22.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 2009.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.010832102412604629, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9891678975873953, \"precision\": 1.0, \"recall\": 0.010832102412604629, \"specificity\": 1.0, \"npv\": 0.3630310716550412, \"accuracy\": 0.3674433249370277, \"f1\": 0.02143205065757428, \"f2\": 0.013503560029462312, \"f0_5\": 0.05191127890514394, \"p4\": 0.04120620154151108, \"phi\": 0.06270876930003502}, {\"truth_threshold\": 24.900000371038914, \"match_probability\": 0.999999968058671, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 21.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 2010.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.0103397341211226, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9896602658788775, \"precision\": 1.0, \"recall\": 0.0103397341211226, \"specificity\": 1.0, \"npv\": 0.3629160063391442, \"accuracy\": 0.36712846347607053, \"f1\": 0.02046783625730994, \"f2\": 0.01289134438305709, \"f0_5\": 0.04964539007092199, \"p4\": 0.03942061774542593, \"phi\": 0.06125728539403615}, {\"truth_threshold\": 25.00000037252903, \"match_probability\": 0.9999999701976862, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 19.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 2012.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.009354997538158542, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9906450024618415, \"precision\": 1.0, \"recall\": 0.009354997538158542, \"specificity\": 1.0, \"npv\": 0.3626860943934115, \"accuracy\": 0.36649874055415615, \"f1\": 0.018536585365853658, \"f2\": 0.011666461991894878, \"f0_5\": 0.045087802562885616, \"p4\": 0.035825619558433386, \"phi\": 0.05824884136336706}, {\"truth_threshold\": 25.100000374019146, \"match_probability\": 0.9999999721934579, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 17.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 2014.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.008370260955194485, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9916297390448056, \"precision\": 1.0, \"recall\": 0.008370260955194485, \"specificity\": 1.0, \"npv\": 0.3624564735675847, \"accuracy\": 0.36586901763224183, \"f1\": 0.0166015625, \"f2\": 0.010440977766859108, \"f0_5\": 0.040495474035254886, \"p4\": 0.03219846095822884, \"phi\": 0.05508044361350257}, {\"truth_threshold\": 25.200000375509262, \"match_probability\": 0.9999999740555788, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 15.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 2016.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.007385524372230428, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9926144756277696, \"precision\": 1.0, \"recall\": 0.007385524372230428, \"specificity\": 1.0, \"npv\": 0.3622271433090794, \"accuracy\": 0.36523929471032746, \"f1\": 0.01466275659824047, \"f2\": 0.009214891264283081, \"f0_5\": 0.035868005738880916, \"p4\": 0.028538670521671944, \"phi\": 0.05172269709897784}, {\"truth_threshold\": 25.400000378489494, \"match_probability\": 0.9999999774140695, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 14.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 2017.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.006893156080748399, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9931068439192516, \"precision\": 1.0, \"recall\": 0.006893156080748399, \"specificity\": 1.0, \"npv\": 0.362112586970272, \"accuracy\": 0.3649244332493703, \"f1\": 0.013691931540342298, \"f2\": 0.00860162202015237, \"f0_5\": 0.033540967896502155, \"p4\": 0.026696388534875385, \"phi\": 0.04996097057493643}, {\"truth_threshold\": 25.50000037997961, \"match_probability\": 0.9999999789265818, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 12.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 2019.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.005908419497784343, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9940915805022157, \"precision\": 1.0, \"recall\": 0.005908419497784343, \"specificity\": 1.0, \"npv\": 0.36188369152970923, \"accuracy\": 0.3642947103274559, \"f1\": 0.011747430249632892, \"f2\": 0.007374631268436578, \"f0_5\": 0.02886002886002886, \"p4\": 0.022986746233599045, \"phi\": 0.046240249339339734}, {\"truth_threshold\": 25.600000381469727, \"match_probability\": 0.9999999803378055, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 9.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 2022.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.004431314623338257, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9955686853766618, \"precision\": 1.0, \"recall\": 0.004431314623338257, \"specificity\": 1.0, \"npv\": 0.361540890432586, \"accuracy\": 0.3633501259445844, \"f1\": 0.008823529411764706, \"f2\": 0.005533013648100332, \"f0_5\": 0.02177068214804064, \"p4\": 0.017358654565300884, \"phi\": 0.040026259314463214}, {\"truth_threshold\": 25.700000382959843, \"match_probability\": 0.9999999816545239, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 8.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 2023.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.003938946331856229, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9960610536681438, \"precision\": 1.0, \"recall\": 0.003938946331856229, \"specificity\": 1.0, \"npv\": 0.3614267676767677, \"accuracy\": 0.3630352644836272, \"f1\": 0.00784698381559588, \"f2\": 0.004918839153959666, \"f0_5\": 0.019389238972370333, \"p4\": 0.015465403546152875, \"phi\": 0.03773116272757914}, {\"truth_threshold\": 25.900000385940075, \"match_probability\": 0.9999999840293354, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 6.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 2025.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.0029542097488921715, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9970457902511078, \"precision\": 1.0, \"recall\": 0.0029542097488921715, \"specificity\": 1.0, \"npv\": 0.361198738170347, \"accuracy\": 0.36240554156171284, \"f1\": 0.005891016200294551, \"f2\": 0.0036900369003690036, \"f0_5\": 0.014598540145985401, \"p4\": 0.011652683870064942, \"phi\": 0.032665835877723835}, {\"truth_threshold\": 26.00000038743019, \"match_probability\": 0.999999985098843, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 5.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 2026.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.002461841457410143, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9975381585425899, \"precision\": 1.0, \"recall\": 0.002461841457410143, \"specificity\": 1.0, \"npv\": 0.36108483128350677, \"accuracy\": 0.3620906801007557, \"f1\": 0.004911591355599214, \"f2\": 0.0030754090294009104, \"f0_5\": 0.01218917601170161, \"p4\": 0.009733083985039102, \"phi\": 0.02981498964104606}, {\"truth_threshold\": 26.100000388920307, \"match_probability\": 0.9999999860967289, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 4.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 2027.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.0019694731659281144, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9980305268340719, \"precision\": 1.0, \"recall\": 0.0019694731659281144, \"specificity\": 1.0, \"npv\": 0.3609709962168979, \"accuracy\": 0.36177581863979846, \"f1\": 0.003931203931203931, \"f2\": 0.0024606299212598425, \"f0_5\": 0.009770395701025891, \"p4\": 0.007804568825263287, \"phi\": 0.026663133550419747}, {\"truth_threshold\": 26.200000390410423, \"match_probability\": 0.9999999870277894, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 3.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 2028.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.0014771048744460858, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9985228951255539, \"precision\": 1.0, \"recall\": 0.0014771048744460858, \"specificity\": 1.0, \"npv\": 0.36085723290261584, \"accuracy\": 0.3614609571788413, \"f1\": 0.0029498525073746312, \"f2\": 0.0018456995201181247, \"f0_5\": 0.007342143906020558, \"p4\": 0.00586707112734875, \"phi\": 0.023087312050119223}, {\"truth_threshold\": 26.400000393390656, \"match_probability\": 0.9999999887070348, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 2.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 2029.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.0009847365829640572, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.999015263417036, \"precision\": 1.0, \"recall\": 0.0009847365829640572, \"specificity\": 1.0, \"npv\": 0.36074354127284186, \"accuracy\": 0.36114609571788414, \"f1\": 0.001967535661583866, \"f2\": 0.0012306177701206006, \"f0_5\": 0.004904364884747425, \"p4\": 0.003920522953249476, \"phi\": 0.018847741566547744}, {\"truth_threshold\": 27.10000040382147, \"match_probability\": 0.9999999930483645, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 1.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 2030.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.0004923682914820286, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.999507631708518, \"precision\": 1.0, \"recall\": 0.0004923682914820286, \"specificity\": 1.0, \"npv\": 0.3606299212598425, \"accuracy\": 0.3608312342569269, \"f1\": 0.000984251968503937, \"f2\": 0.0006153846153846154, \"f0_5\": 0.002457002457002457, \"p4\": 0.001964855681779181, \"phi\": 0.013325266908696693}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linker.evaluation.accuracy_analysis_from_labels_table(\n",
    "    labels_table, output_type=\"roc\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f749c3c",
   "metadata": {},
   "source": [
    "### Precision-recall chart\n",
    "\n",
    "An alternative representation of truth space is called a [precision recall curve](https://stats.stackexchange.com/questions/7207/roc-vs-precision-and-recall-curves).\n",
    "\n",
    "This can be plotted as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18d25327",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-07T09:03:06.500029Z",
     "iopub.status.busy": "2024-06-07T09:03:06.499762Z",
     "iopub.status.idle": "2024-06-07T09:03:06.898469Z",
     "shell.execute_reply": "2024-06-07T09:03:06.897942Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-32fd474493264463bcc2515158566809.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-32fd474493264463bcc2515158566809.vega-embed details,\n",
       "  #altair-viz-32fd474493264463bcc2515158566809.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-32fd474493264463bcc2515158566809\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-32fd474493264463bcc2515158566809\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-32fd474493264463bcc2515158566809\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-566a43f4ff082e8139ab51d5a4757be2\"}, \"mark\": {\"type\": \"line\", \"clip\": true, \"point\": true}, \"encoding\": {\"tooltip\": [{\"field\": \"truth_threshold\", \"format\": \".4f\", \"type\": \"quantitative\"}, {\"field\": \"match_probability\", \"format\": \".4%\", \"type\": \"quantitative\"}, {\"field\": \"fp_rate\", \"format\": \".4f\", \"title\": \"FP_rate\", \"type\": \"quantitative\"}, {\"field\": \"tp_rate\", \"format\": \".4f\", \"title\": \"TP_rate\", \"type\": \"quantitative\"}, {\"field\": \"tp\", \"format\": \",.0f\", \"title\": \"TP\", \"type\": \"quantitative\"}, {\"field\": \"tn\", \"format\": \",.0f\", \"title\": \"TN\", \"type\": \"quantitative\"}, {\"field\": \"fp\", \"format\": \",.0f\", \"title\": \"FP\", \"type\": \"quantitative\"}, {\"field\": \"fn\", \"format\": \",.0f\", \"title\": \"FN\", \"type\": \"quantitative\"}, {\"field\": \"precision\", \"format\": \".4f\", \"type\": \"quantitative\"}, {\"field\": \"recall\", \"format\": \".4f\", \"type\": \"quantitative\"}, {\"field\": \"f1\", \"format\": \".4f\", \"title\": \"F1\", \"type\": \"quantitative\"}], \"x\": {\"field\": \"recall\", \"sort\": [\"-recall\"], \"title\": \"Recall\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"precision\", \"sort\": [\"-precision\"], \"title\": \"Precision\", \"type\": \"quantitative\"}}, \"height\": 400, \"params\": [{\"name\": \"mouse_zoom\", \"select\": {\"type\": \"interval\", \"encodings\": [\"x\", \"y\"]}, \"bind\": \"scales\"}], \"title\": \"Precision-recall curve\", \"width\": 400, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.9.3.json\", \"datasets\": {\"data-566a43f4ff082e8139ab51d5a4757be2\": [{\"truth_threshold\": -12.400000184774399, \"match_probability\": 0.00018498974370122882, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 1027.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1004.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.5056622353520434, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.49433776464795665, \"precision\": 1.0, \"recall\": 0.5056622353520434, \"specificity\": 1.0, \"npv\": 0.532805956258725, \"accuracy\": 0.6838790931989924, \"f1\": 0.671680837148463, \"f2\": 0.5611408589225221, \"f0_5\": 0.836455448770158, \"p4\": 0.6832397200141539, \"phi\": 0.5190566932914649}, {\"truth_threshold\": -6.100000090897083, \"match_probability\": 0.014369156816028038, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 1026.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1005.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.5051698670605613, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.4948301329394387, \"precision\": 1.0, \"recall\": 0.5051698670605613, \"specificity\": 1.0, \"npv\": 0.5325581395348837, \"accuracy\": 0.6835642317380353, \"f1\": 0.6712463199214916, \"f2\": 0.5606557377049181, \"f0_5\": 0.8361858190709046, \"p4\": 0.6829130052819856, \"phi\": 0.5186832603341437}, {\"truth_threshold\": -5.000000074505806, \"match_probability\": 0.030303028785498974, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 1024.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1007.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.5041851304775973, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.4958148695224028, \"precision\": 1.0, \"recall\": 0.5041851304775973, \"specificity\": 1.0, \"npv\": 0.5320631970260223, \"accuracy\": 0.6829345088161209, \"f1\": 0.6703764320785598, \"f2\": 0.559685177087888, \"f0_5\": 0.8356455035090583, \"p4\": 0.6822591980364565, \"phi\": 0.5179366297288623}, {\"truth_threshold\": -4.90000007301569, \"match_probability\": 0.032407497325934585, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 1019.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1012.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.5017232890201871, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.4982767109798129, \"precision\": 1.0, \"recall\": 0.5017232890201871, \"specificity\": 1.0, \"npv\": 0.530829856281873, \"accuracy\": 0.681360201511335, \"f1\": 0.6681967213114755, \"f2\": 0.5572569178606585, \"f0_5\": 0.834288521368921, \"p4\": 0.6806224540570874, \"phi\": 0.51607141114758}, {\"truth_threshold\": -4.6000000685453415, \"match_probability\": 0.039601660807737325, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 1018.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1013.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.5012309207287051, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.49876907927129494, \"precision\": 1.0, \"recall\": 0.5012309207287051, \"specificity\": 1.0, \"npv\": 0.530583873957368, \"accuracy\": 0.6810453400503779, \"f1\": 0.6677599212856674, \"f2\": 0.5567709472763072, \"f0_5\": 0.8340160576765525, \"p4\": 0.680294719867444, \"phi\": 0.515698597697778}, {\"truth_threshold\": -4.100000061094761, \"match_probability\": 0.0551013486283602, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 1017.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1014.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.5007385524372231, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.49926144756277696, \"precision\": 1.0, \"recall\": 0.5007385524372231, \"specificity\": 1.0, \"npv\": 0.5303381194997684, \"accuracy\": 0.6807304785894207, \"f1\": 0.6673228346456693, \"f2\": 0.5562848703642927, \"f0_5\": 0.8337432365961633, \"p4\": 0.6799668560937839, \"phi\": 0.5153258602676495}, {\"truth_threshold\": -4.000000059604645, \"match_probability\": 0.05882352712444066, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 1006.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1025.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.49532250123092075, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5046774987690793, \"precision\": 1.0, \"recall\": 0.49532250123092075, \"specificity\": 1.0, \"npv\": 0.5276497695852534, \"accuracy\": 0.6772670025188917, \"f1\": 0.6624958840961476, \"f2\": 0.5509309967141293, \"f0_5\": 0.8307184145334434, \"p4\": 0.6763516632891752, \"phi\": 0.5112306755711034}, {\"truth_threshold\": -3.9000000581145287, \"match_probability\": 0.06278043839004852, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 997.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1034.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.49089118660758246, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5091088133924175, \"precision\": 1.0, \"recall\": 0.49089118660758246, \"specificity\": 1.0, \"npv\": 0.5254703992657183, \"accuracy\": 0.674433249370277, \"f1\": 0.6585204755614267, \"f2\": 0.5465409494572964, \"f0_5\": 0.8282106662236252, \"p4\": 0.6733816166373302, \"phi\": 0.5078865895283203}, {\"truth_threshold\": -3.500000052154064, \"match_probability\": 0.08121030044424019, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 996.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1035.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.49039881831610044, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5096011816838996, \"precision\": 1.0, \"recall\": 0.49039881831610044, \"specificity\": 1.0, \"npv\": 0.5252293577981652, \"accuracy\": 0.6741183879093199, \"f1\": 0.6580773042616452, \"f2\": 0.5460526315789473, \"f0_5\": 0.827930174563591, \"p4\": 0.6730509183540228, \"phi\": 0.5075153755396427}, {\"truth_threshold\": -2.9000000432133675, \"match_probability\": 0.11814376082605058, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 994.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1037.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.4894140817331364, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5105859182668636, \"precision\": 1.0, \"recall\": 0.4894140817331364, \"specificity\": 1.0, \"npv\": 0.5247479376718607, \"accuracy\": 0.6734886649874056, \"f1\": 0.6571900826446281, \"f2\": 0.5450756744900197, \"f0_5\": 0.8273680705843183, \"p4\": 0.6723890998562475, \"phi\": 0.5067731544360166}, {\"truth_threshold\": -1.5000000223517418, \"match_probability\": 0.2612038719739489, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 966.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1065.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.4756277695716396, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5243722304283605, \"precision\": 1.0, \"recall\": 0.4756277695716396, \"specificity\": 1.0, \"npv\": 0.5180995475113123, \"accuracy\": 0.6646725440806045, \"f1\": 0.6446446446446447, \"f2\": 0.5313531353135313, \"f0_5\": 0.8193384223918575, \"p4\": 0.6630623177686907, \"phi\": 0.4964096415249014}, {\"truth_threshold\": -0.5000000074505806, \"match_probability\": 0.41421356112001384, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 960.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1071.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.4726735598227474, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5273264401772526, \"precision\": 1.0, \"recall\": 0.4726735598227474, \"specificity\": 1.0, \"npv\": 0.5166967509025271, \"accuracy\": 0.6627833753148614, \"f1\": 0.641925777331996, \"f2\": 0.5284015852047557, \"f0_5\": 0.8175779253960143, \"p4\": 0.6610481781257823, \"phi\": 0.49419519685843255}, {\"truth_threshold\": -0.10000000149011612, \"match_probability\": 0.48267825490990723, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 959.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1072.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.47218119153126537, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5278188084687346, \"precision\": 1.0, \"recall\": 0.47218119153126537, \"specificity\": 1.0, \"npv\": 0.5164636896707262, \"accuracy\": 0.6624685138539043, \"f1\": 0.6414715719063545, \"f2\": 0.5279092810745348, \"f0_5\": 0.8172831089142663, \"p4\": 0.6607119325939106, \"phi\": 0.49382632612220784}, {\"truth_threshold\": 0.20000000298023224, \"match_probability\": 0.5346019618947252, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 957.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1074.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.4711964549483013, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5288035450516987, \"precision\": 1.0, \"recall\": 0.4711964549483013, \"specificity\": 1.0, \"npv\": 0.51599819738621, \"accuracy\": 0.6618387909319899, \"f1\": 0.6405622489959839, \"f2\": 0.5269243475388173, \"f0_5\": 0.8166922683051715, \"p4\": 0.6600389602879736, \"phi\": 0.49308875607551217}, {\"truth_threshold\": 0.6000000089406967, \"match_probability\": 0.6024989422185573, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 955.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1076.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.47021171836533726, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5297882816346627, \"precision\": 1.0, \"recall\": 0.47021171836533726, \"specificity\": 1.0, \"npv\": 0.5155335434488969, \"accuracy\": 0.6612090680100756, \"f1\": 0.6396517079705292, \"f2\": 0.5259389800638837, \"f0_5\": 0.8160998119979491, \"p4\": 0.6593653425793322, \"phi\": 0.49235141244854475}, {\"truth_threshold\": 1.0000000149011612, \"match_probability\": 0.6666666689619328, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 952.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1079.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.4687346134908912, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5312653865091088, \"precision\": 1.0, \"recall\": 0.4687346134908912, \"specificity\": 1.0, \"npv\": 0.5148381294964028, \"accuracy\": 0.660264483627204, \"f1\": 0.6382836071069393, \"f2\": 0.5244601145879242, \"f0_5\": 0.8152080835759548, \"p4\": 0.6583536959994251, \"phi\": 0.491245815900624}, {\"truth_threshold\": 1.3000000193715096, \"match_probability\": 0.7111737233641148, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 949.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1082.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.4672575086164451, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5327424913835549, \"precision\": 1.0, \"recall\": 0.4672575086164451, \"specificity\": 1.0, \"npv\": 0.5141445891333633, \"accuracy\": 0.6593198992443325, \"f1\": 0.6369127516778523, \"f2\": 0.5229802711341343, \"f0_5\": 0.8143126823408272, \"p4\": 0.6573405717493672, \"phi\": 0.4901407142720151}, {\"truth_threshold\": 1.600000023841858, \"match_probability\": 0.7519492561137834, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 947.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1084.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.46627277203348105, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.533727227966519, \"precision\": 1.0, \"recall\": 0.46627277203348105, \"specificity\": 1.0, \"npv\": 0.5136832660385823, \"accuracy\": 0.6586901763224181, \"f1\": 0.6359973136333109, \"f2\": 0.5219931650314188, \"f0_5\": 0.8137136965114281, \"p4\": 0.6566643270207304, \"phi\": 0.48940425049545894}, {\"truth_threshold\": 1.9000000283122063, \"match_probability\": 0.7886787621992872, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 943.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1088.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.46430329886755295, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.535696701132447, \"precision\": 1.0, \"recall\": 0.46430329886755295, \"specificity\": 1.0, \"npv\": 0.5127630989699955, \"accuracy\": 0.6574307304785895, \"f1\": 0.6341627437794216, \"f2\": 0.5200176464100584, \"f0_5\": 0.8125107702912286, \"p4\": 0.6553098282363409, \"phi\": 0.4879319608196603}, {\"truth_threshold\": 2.1000000312924385, \"match_probability\": 0.8108601793810092, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 939.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1092.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.4623338257016248, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5376661742983752, \"precision\": 1.0, \"recall\": 0.4623338257016248, \"specificity\": 1.0, \"npv\": 0.5118462226195798, \"accuracy\": 0.6561712846347607, \"f1\": 0.6323232323232323, \"f2\": 0.5180403839788149, \"f0_5\": 0.8113011923276309, \"p4\": 0.6539526192956943, \"phi\": 0.48646050433168353}, {\"truth_threshold\": 2.2000000327825546, \"match_probability\": 0.8212623941099038, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 935.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1096.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.4603643525356967, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5396356474643033, \"precision\": 1.0, \"recall\": 0.4603643525356967, \"specificity\": 1.0, \"npv\": 0.5109326193663543, \"accuracy\": 0.654911838790932, \"f1\": 0.6304787592717465, \"f2\": 0.5160613754277514, \"f0_5\": 0.8100849072950962, \"p4\": 0.6525926625805016, \"phi\": 0.48498986020736484}, {\"truth_threshold\": 2.3000000342726707, \"match_probability\": 0.8312116004280432, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 926.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1105.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.45593303791235845, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5440669620876416, \"precision\": 1.0, \"recall\": 0.45593303791235845, \"specificity\": 1.0, \"npv\": 0.5088888888888888, \"accuracy\": 0.6520780856423174, \"f1\": 0.6263104497801826, \"f2\": 0.5116022099447514, \"f0_5\": 0.8073234524847428, \"p4\": 0.6495225157687339, \"phi\": 0.48168377289561637}, {\"truth_threshold\": 2.400000035762787, \"match_probability\": 0.8407144092272857, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 923.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1108.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.4544559330379124, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5455440669620877, \"precision\": 1.0, \"recall\": 0.4544559330379124, \"specificity\": 1.0, \"npv\": 0.5082112738570794, \"accuracy\": 0.6511335012594458, \"f1\": 0.6249153689911984, \"f2\": 0.5101138498949929, \"f0_5\": 0.8063952472479469, \"p4\": 0.6484959234103079, \"phi\": 0.4805825929443398}, {\"truth_threshold\": 2.7000000402331352, \"match_probability\": 0.8666314458464526, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 903.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1128.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.4446085672082718, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5553914327917282, \"precision\": 1.0, \"recall\": 0.4446085672082718, \"specificity\": 1.0, \"npv\": 0.5037395512538495, \"accuracy\": 0.6448362720403022, \"f1\": 0.6155419222903885, \"f2\": 0.5001661681621802, \"f0_5\": 0.8001063264221159, \"p4\": 0.6416094363472876, \"phi\": 0.47325143436561484}, {\"truth_threshold\": 2.8000000417232513, \"match_probability\": 0.8744413378412453, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 901.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1130.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.44362383062530775, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5563761693746923, \"precision\": 1.0, \"recall\": 0.44362383062530775, \"specificity\": 1.0, \"npv\": 0.5032967032967033, \"accuracy\": 0.6442065491183879, \"f1\": 0.6145975443383356, \"f2\": 0.49916897506925206, \"f0_5\": 0.7994676131322094, \"p4\": 0.6409166024701175, \"phi\": 0.47251921808279124}, {\"truth_threshold\": 2.9000000432133675, \"match_probability\": 0.8818562391739494, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 898.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1133.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.44214672575086167, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5578532742491383, \"precision\": 1.0, \"recall\": 0.44214672575086167, \"specificity\": 1.0, \"npv\": 0.5026338893766462, \"accuracy\": 0.6432619647355163, \"f1\": 0.6131785592352339, \"f2\": 0.4976723564619818, \"f0_5\": 0.7985061355148497, \"p4\": 0.6398758844531804, \"phi\": 0.4714211794555957}, {\"truth_threshold\": 3.0000000447034836, \"match_probability\": 0.8888888919492438, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 891.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1140.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.43870014771048743, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5612998522895125, \"precision\": 1.0, \"recall\": 0.43870014771048743, \"specificity\": 1.0, \"npv\": 0.5010940919037199, \"accuracy\": 0.6410579345088161, \"f1\": 0.6098562628336756, \"f2\": 0.49417637271214643, \"f0_5\": 0.7962466487935657, \"p4\": 0.6374406028285706, \"phi\": 0.46886037594897534}, {\"truth_threshold\": 3.1000000461935997, \"match_probability\": 0.8955524998434058, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 888.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1143.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.43722304283604135, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5627769571639586, \"precision\": 1.0, \"recall\": 0.43722304283604135, \"specificity\": 1.0, \"npv\": 0.5004370629370629, \"accuracy\": 0.6401133501259446, \"f1\": 0.6084275436793423, \"f2\": 0.49267643142476697, \"f0_5\": 0.7952713594841483, \"p4\": 0.6363938931197374, \"phi\": 0.4677634181990659}, {\"truth_threshold\": 3.300000049173832, \"match_probability\": 0.9078269283845571, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 886.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1145.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.4362383062530773, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5637616937469226, \"precision\": 1.0, \"recall\": 0.4362383062530773, \"specificity\": 1.0, \"npv\": 0.5, \"accuracy\": 0.6394836272040302, \"f1\": 0.6074734316078162, \"f2\": 0.4916759156492786, \"f0_5\": 0.7946188340807175, \"p4\": 0.635695067264574, \"phi\": 0.4670322827455707}, {\"truth_threshold\": 3.500000052154064, \"match_probability\": 0.9187896995557598, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 874.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1157.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.43032988675529293, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5696701132447071, \"precision\": 1.0, \"recall\": 0.43032988675529293, \"specificity\": 1.0, \"npv\": 0.49739357080799307, \"accuracy\": 0.635705289672544, \"f1\": 0.6017211703958691, \"f2\": 0.48566348077350524, \"f0_5\": 0.7906640130269585, \"p4\": 0.6314846590963767, \"phi\": 0.46264815897030703}, {\"truth_threshold\": 3.6000000536441803, \"match_probability\": 0.9238137785296746, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 872.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1159.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.4293451501723289, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5706548498276711, \"precision\": 1.0, \"recall\": 0.4293451501723289, \"specificity\": 1.0, \"npv\": 0.4969618055555556, \"accuracy\": 0.6350755667506297, \"f1\": 0.6007578367206339, \"f2\": 0.48465984882169855, \"f0_5\": 0.7899981880775503, \"p4\": 0.6307799634934296, \"phi\": 0.4619178942584512}, {\"truth_threshold\": 3.7000000551342964, \"match_probability\": 0.9285512128432143, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 867.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1164.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.42688330871491875, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5731166912850812, \"precision\": 1.0, \"recall\": 0.42688330871491875, \"specificity\": 1.0, \"npv\": 0.4958856647899524, \"accuracy\": 0.6335012594458438, \"f1\": 0.598343685300207, \"f2\": 0.48214881548214883, \"f0_5\": 0.7883251500272777, \"p4\": 0.629014444873201, \"phi\": 0.4600927225351777}, {\"truth_threshold\": 3.8000000566244125, \"match_probability\": 0.9330154225613858, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 856.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1175.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.42146725750861647, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5785327424913835, \"precision\": 1.0, \"recall\": 0.42146725750861647, \"specificity\": 1.0, \"npv\": 0.49353448275862066, \"accuracy\": 0.6300377833753149, \"f1\": 0.5930031174229303, \"f2\": 0.4766146993318486, \"f0_5\": 0.7846012832263978, \"p4\": 0.6251109156992851, \"phi\": 0.45607962565127746}, {\"truth_threshold\": 3.9000000581145287, \"match_probability\": 0.9372195616099515, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 852.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1179.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.4194977843426883, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5805022156573116, \"precision\": 1.0, \"recall\": 0.4194977843426883, \"specificity\": 1.0, \"npv\": 0.4926850258175559, \"accuracy\": 0.6287783375314862, \"f1\": 0.5910509885535901, \"f2\": 0.47459893048128343, \"f0_5\": 0.7832322118036403, \"p4\": 0.6236846938036701, \"phi\": 0.45462102537089605}, {\"truth_threshold\": 4.000000059604645, \"match_probability\": 0.9411764728755594, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 832.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1199.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.4096504185130478, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5903495814869523, \"precision\": 1.0, \"recall\": 0.4096504185130478, \"specificity\": 1.0, \"npv\": 0.488481228668942, \"accuracy\": 0.6224811083123426, \"f1\": 0.5812085225288159, \"f2\": 0.4644930772666369, \"f0_5\": 0.7762642284008211, \"p4\": 0.6164974082601424, \"phi\": 0.44733269471390064}, {\"truth_threshold\": 4.300000064074993, \"match_probability\": 0.9516868803254299, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 825.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1206.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.40620384047267355, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5937961595273265, \"precision\": 1.0, \"recall\": 0.40620384047267355, \"specificity\": 1.0, \"npv\": 0.48702679710761376, \"accuracy\": 0.6202770780856424, \"f1\": 0.5777310924369747, \"f2\": 0.46094535702313105, \"f0_5\": 0.7737760270118177, \"p4\": 0.6139589751589127, \"phi\": 0.44478326789372186}, {\"truth_threshold\": 4.400000065565109, \"match_probability\": 0.9547759482410569, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 822.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1209.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.40472673559822747, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5952732644017725, \"precision\": 1.0, \"recall\": 0.40472673559822747, \"specificity\": 1.0, \"npv\": 0.48640611724723876, \"accuracy\": 0.6193324937027708, \"f1\": 0.576235541535226, \"f2\": 0.45942320590207913, \"f0_5\": 0.7727016356457981, \"p4\": 0.6128673226148784, \"phi\": 0.4436908383193005}, {\"truth_threshold\": 4.700000070035458, \"match_probability\": 0.9629520927573305, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 821.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1210.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.40423436730674545, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5957656326932546, \"precision\": 1.0, \"recall\": 0.40423436730674545, \"specificity\": 1.0, \"npv\": 0.4861995753715499, \"accuracy\": 0.6190176322418136, \"f1\": 0.5757363253856943, \"f2\": 0.4589155953046395, \"f0_5\": 0.7723424270931326, \"p4\": 0.6125029320545232, \"phi\": 0.44332671669450147}, {\"truth_threshold\": 4.800000071525574, \"match_probability\": 0.9653471069144568, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 817.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1214.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.40226489414081734, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.5977351058591827, \"precision\": 1.0, \"recall\": 0.40226489414081734, \"specificity\": 1.0, \"npv\": 0.4853751589656634, \"accuracy\": 0.6177581863979849, \"f1\": 0.5737359550561798, \"f2\": 0.4568840174477128, \"f0_5\": 0.7709001698433666, \"p4\": 0.611042815748838, \"phi\": 0.4418703281958464}, {\"truth_threshold\": 4.90000007301569, \"match_probability\": 0.9675925026740654, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 811.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1220.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.3993106843919252, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6006893156080748, \"precision\": 1.0, \"recall\": 0.3993106843919252, \"specificity\": 1.0, \"npv\": 0.48414376321353064, \"accuracy\": 0.6158690176322418, \"f1\": 0.5707248416608023, \"f2\": 0.45383324006715164, \"f0_5\": 0.7687203791469195, \"p4\": 0.6088448866523514, \"phi\": 0.43968599867732555}, {\"truth_threshold\": 5.000000074505806, \"match_probability\": 0.969696971214501, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 771.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1260.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.37961595273264404, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.620384047267356, \"precision\": 1.0, \"recall\": 0.37961595273264404, \"specificity\": 1.0, \"npv\": 0.4760914760914761, \"accuracy\": 0.6032745591939547, \"f1\": 0.550321199143469, \"f2\": 0.433389544688027, \"f0_5\": 0.7536656891495601, \"p4\": 0.5939408006943253, \"phi\": 0.42512576878419933}, {\"truth_threshold\": 5.100000075995922, \"match_probability\": 0.9716687817966767, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 756.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1275.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.3722304283604136, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6277695716395865, \"precision\": 1.0, \"recall\": 0.3722304283604136, \"specificity\": 1.0, \"npv\": 0.4731404958677686, \"accuracy\": 0.5985516372795969, \"f1\": 0.542518837459634, \"f2\": 0.42567567567567566, \"f0_5\": 0.7477744807121661, \"p4\": 0.5882313967029971, \"phi\": 0.41966330486655373}, {\"truth_threshold\": 6.000000089406967, \"match_probability\": 0.9846153855541349, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 750.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1281.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.36927621861152143, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6307237813884786, \"precision\": 1.0, \"recall\": 0.36927621861152143, \"specificity\": 1.0, \"npv\": 0.47197032151690027, \"accuracy\": 0.5966624685138538, \"f1\": 0.5393743257820928, \"f2\": 0.42258282623394183, \"f0_5\": 0.7453786523553966, \"p4\": 0.5859280050558678, \"phi\": 0.41747744325487207}, {\"truth_threshold\": 6.200000092387199, \"match_probability\": 0.9865801893041345, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 728.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1303.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.3584441161989168, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6415558838010832, \"precision\": 1.0, \"recall\": 0.3584441161989168, \"specificity\": 1.0, \"npv\": 0.46772875816993464, \"accuracy\": 0.589735516372796, \"f1\": 0.5277274374773469, \"f2\": 0.41120650700406686, \"f0_5\": 0.7363949018814485, \"p4\": 0.5773816708195095, \"phi\": 0.4094564950553833}, {\"truth_threshold\": 6.3000000938773155, \"match_probability\": 0.987467611228855, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 726.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1305.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.35745937961595275, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6425406203840472, \"precision\": 1.0, \"recall\": 0.35745937961595275, \"specificity\": 1.0, \"npv\": 0.4673469387755102, \"accuracy\": 0.5891057934508817, \"f1\": 0.5266594124047879, \"f2\": 0.4101694915254237, \"f0_5\": 0.7355623100303952, \"p4\": 0.5765966357046058, \"phi\": 0.40872673854313535}, {\"truth_threshold\": 6.400000095367432, \"match_probability\": 0.9882970460445225, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 723.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1308.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.3559822747415066, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6440177252584933, \"precision\": 1.0, \"recall\": 0.3559822747415066, \"specificity\": 1.0, \"npv\": 0.4667753770892784, \"accuracy\": 0.5881612090680101, \"f1\": 0.5250544662309368, \"f2\": 0.40861308918277384, \"f0_5\": 0.7343083485679464, \"p4\": 0.5754164781246025, \"phi\": 0.40763189341557404}, {\"truth_threshold\": 6.500000096857548, \"match_probability\": 0.9890721936212699, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 711.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1320.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.3500738552437223, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6499261447562777, \"precision\": 1.0, \"recall\": 0.3500738552437223, \"specificity\": 1.0, \"npv\": 0.4645030425963489, \"accuracy\": 0.5843828715365239, \"f1\": 0.5185995623632386, \"f2\": 0.40237691001697795, \"f0_5\": 0.7292307692307692, \"p4\": 0.5706640029441143, \"phi\": 0.40324976242292176}, {\"truth_threshold\": 6.800000101327896, \"match_probability\": 0.9911056147706719, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 702.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1329.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.345642540620384, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.654357459379616, \"precision\": 1.0, \"recall\": 0.345642540620384, \"specificity\": 1.0, \"npv\": 0.46281325788197253, \"accuracy\": 0.5815491183879093, \"f1\": 0.5137211855104281, \"f2\": 0.39768864717878993, \"f0_5\": 0.7253564786112833, \"p4\": 0.5670653210867429, \"phi\": 0.3999599358524826}, {\"truth_threshold\": 7.000000104308128, \"match_probability\": 0.9922480625716311, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 700.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1331.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.34465780403741997, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.65534219596258, \"precision\": 1.0, \"recall\": 0.34465780403741997, \"specificity\": 1.0, \"npv\": 0.4624394184168013, \"accuracy\": 0.5809193954659949, \"f1\": 0.5126327352618089, \"f2\": 0.39664551223934724, \"f0_5\": 0.724487683709377, \"p4\": 0.5662615170898467, \"phi\": 0.39922844895106907}, {\"truth_threshold\": 7.1000001057982445, \"match_probability\": 0.9927634299608046, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 697.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1334.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.3431806991629739, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6568193008370261, \"precision\": 1.0, \"recall\": 0.3431806991629739, \"specificity\": 1.0, \"npv\": 0.4618797902379992, \"accuracy\": 0.5799748110831234, \"f1\": 0.5109970674486803, \"f2\": 0.39507992291123456, \"f0_5\": 0.7231790827972608, \"p4\": 0.5650529747120447, \"phi\": 0.39813091985316124}, {\"truth_threshold\": 7.400000110268593, \"match_probability\": 0.9941140817673122, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 696.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1335.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.34268833087149186, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6573116691285081, \"precision\": 1.0, \"recall\": 0.34268833087149186, \"specificity\": 1.0, \"npv\": 0.46169354838709675, \"accuracy\": 0.5796599496221663, \"f1\": 0.5104510451045104, \"f2\": 0.3945578231292517, \"f0_5\": 0.7227414330218068, \"p4\": 0.5646493659013554, \"phi\": 0.3977649952810209}, {\"truth_threshold\": 7.500000111758709, \"match_probability\": 0.9945060786121668, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 690.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1341.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.3397341211225997, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6602658788774003, \"precision\": 1.0, \"recall\": 0.3397341211225997, \"specificity\": 1.0, \"npv\": 0.46057924376508447, \"accuracy\": 0.5777707808564232, \"f1\": 0.5071664829106945, \"f2\": 0.39142273655547993, \"f0_5\": 0.720100187852223, \"p4\": 0.5622196307198002, \"phi\": 0.3955685586441907}, {\"truth_threshold\": 7.600000113248825, \"match_probability\": 0.9948721034855129, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 688.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1343.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.33874938453963566, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6612506154603643, \"precision\": 1.0, \"recall\": 0.33874938453963566, \"specificity\": 1.0, \"npv\": 0.46020900321543406, \"accuracy\": 0.5771410579345088, \"f1\": 0.5060684075027584, \"f2\": 0.39037675896504764, \"f0_5\": 0.7192138825005226, \"p4\": 0.5614066094767297, \"phi\": 0.394836062941099}, {\"truth_threshold\": 7.700000114738941, \"match_probability\": 0.9952138598197071, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 687.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1344.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.33825701624815363, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6617429837518464, \"precision\": 1.0, \"recall\": 0.33825701624815363, \"specificity\": 1.0, \"npv\": 0.4600241060666935, \"accuracy\": 0.5768261964735516, \"f1\": 0.5055187637969095, \"f2\": 0.38985359210078313, \"f0_5\": 0.7187696170747018, \"p4\": 0.560999510044096, \"phi\": 0.39446974728151696}, {\"truth_threshold\": 7.800000116229057, \"match_probability\": 0.9955329415617687, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 686.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1345.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.3377646479566716, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6622353520433284, \"precision\": 1.0, \"recall\": 0.3377646479566716, \"specificity\": 1.0, \"npv\": 0.4598393574297189, \"accuracy\": 0.5765113350125944, \"f1\": 0.5049687154950313, \"f2\": 0.38933030646992056, \"f0_5\": 0.7183246073298429, \"p4\": 0.5605920163437905, \"phi\": 0.3941033857744324}, {\"truth_threshold\": 7.900000117719173, \"match_probability\": 0.99583083992065, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 684.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1347.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.33677991137370755, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6632200886262924, \"precision\": 1.0, \"recall\": 0.33677991137370755, \"specificity\": 1.0, \"npv\": 0.45947030497592295, \"accuracy\": 0.5758816120906801, \"f1\": 0.5038674033149171, \"f2\": 0.388283378746594, \"f0_5\": 0.7174323473882945, \"p4\": 0.5597758409315445, \"phi\": 0.3933705232838903}, {\"truth_threshold\": 8.00000011920929, \"match_probability\": 0.9961089497366072, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 683.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1348.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.3362875430822255, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6637124569177745, \"precision\": 1.0, \"recall\": 0.3362875430822255, \"specificity\": 1.0, \"npv\": 0.4592860008022463, \"accuracy\": 0.575566750629723, \"f1\": 0.5033161385408991, \"f2\": 0.3877597365731804, \"f0_5\": 0.7169850934285115, \"p4\": 0.5593671566036346, \"phi\": 0.3930040213303783}, {\"truth_threshold\": 8.200000122189522, \"match_probability\": 0.9966109369567457, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 677.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1354.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.3333333333333333, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6666666666666666, \"precision\": 1.0, \"recall\": 0.3333333333333333, \"specificity\": 1.0, \"npv\": 0.45818327330932374, \"accuracy\": 0.5736775818639799, \"f1\": 0.5, \"f2\": 0.38461538461538464, \"f0_5\": 0.7142857142857143, \"p4\": 0.5569066147859922, \"phi\": 0.39080398893790036}, {\"truth_threshold\": 8.300000123679638, \"match_probability\": 0.9968371745531442, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 675.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1356.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.33234859675036926, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6676514032496307, \"precision\": 1.0, \"recall\": 0.33234859675036926, \"specificity\": 1.0, \"npv\": 0.4578168732506997, \"accuracy\": 0.5730478589420654, \"f1\": 0.49889135254988914, \"f2\": 0.38356631435390387, \"f0_5\": 0.7133798351299937, \"p4\": 0.5560831885340557, \"phi\": 0.3900702441785476}, {\"truth_threshold\": 8.400000125169754, \"match_probability\": 0.9970483543414643, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 674.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1357.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.33185622845888724, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6681437715411127, \"precision\": 1.0, \"recall\": 0.33185622845888724, \"specificity\": 1.0, \"npv\": 0.45763389288569145, \"accuracy\": 0.5727329974811083, \"f1\": 0.4983364140480592, \"f2\": 0.38304160036371904, \"f0_5\": 0.712925745716099, \"p4\": 0.5556708607973319, \"phi\": 0.38970329445361884}, {\"truth_threshold\": 8.50000012665987, \"match_probability\": 0.997245472756309, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 673.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1358.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.3313638601674052, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6686361398325947, \"precision\": 1.0, \"recall\": 0.3313638601674052, \"specificity\": 1.0, \"npv\": 0.4574510587295246, \"accuracy\": 0.5724181360201511, \"f1\": 0.4977810650887574, \"f2\": 0.38251676707968624, \"f0_5\": 0.7124708871479991, \"p4\": 0.5552581215106543, \"phi\": 0.3893362925008169}, {\"truth_threshold\": 8.600000128149986, \"match_probability\": 0.9974294610402847, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 672.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1359.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.3308714918759232, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6691285081240768, \"precision\": 1.0, \"recall\": 0.3308714918759232, \"specificity\": 1.0, \"npv\": 0.45726837060702874, \"accuracy\": 0.572103274559194, \"f1\": 0.4972253052164262, \"f2\": 0.3819918144611187, \"f0_5\": 0.7120152574698029, \"p4\": 0.5548449693107866, \"phi\": 0.38896923782019077}, {\"truth_threshold\": 8.700000129640102, \"match_probability\": 0.997601189412643, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 668.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1363.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.3289020187099951, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.671097981290005, \"precision\": 1.0, \"recall\": 0.3289020187099951, \"specificity\": 1.0, \"npv\": 0.4565390749601276, \"accuracy\": 0.5708438287153652, \"f1\": 0.494998147462023, \"f2\": 0.37989080982711554, \"f0_5\": 0.710184988305337, \"p4\": 0.5531882039410483, \"phi\": 0.3875004817730937}, {\"truth_threshold\": 8.800000131130219, \"match_probability\": 0.997761470983937, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 664.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1367.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.326932545544067, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6730674544559331, \"precision\": 1.0, \"recall\": 0.326932545544067, \"specificity\": 1.0, \"npv\": 0.45581210191082805, \"accuracy\": 0.5695843828715366, \"f1\": 0.49276437847866417, \"f2\": 0.377787892580792, \"f0_5\": 0.7083422231704715, \"p4\": 0.5515247216052599, \"phi\": 0.3860308417309408}, {\"truth_threshold\": 8.900000132620335, \"match_probability\": 0.9979110654305032, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 660.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1371.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.3249630723781389, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6750369276218612, \"precision\": 1.0, \"recall\": 0.3249630723781389, \"specificity\": 1.0, \"npv\": 0.455087440381558, \"accuracy\": 0.5683249370277078, \"f1\": 0.49052396878483834, \"f2\": 0.3756830601092896, \"f0_5\": 0.7064868336544637, \"p4\": 0.5498544326420134, \"phi\": 0.384560285036162}, {\"truth_threshold\": 9.00000013411045, \"match_probability\": 0.9980506824420605, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 659.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1372.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.32447070408665685, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6755292959133432, \"precision\": 1.0, \"recall\": 0.32447070408665685, \"specificity\": 1.0, \"npv\": 0.454906634882797, \"accuracy\": 0.5680100755667506, \"f1\": 0.4899628252788104, \"f2\": 0.3751565524308323, \"f0_5\": 0.7060209985001071, \"p4\": 0.5494357862329947, \"phi\": 0.3841924987738736}, {\"truth_threshold\": 9.100000135600567, \"match_probability\": 0.9981809849551747, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 653.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1378.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.32151649433776464, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6784835056622354, \"precision\": 1.0, \"recall\": 0.32151649433776464, \"specificity\": 1.0, \"npv\": 0.453824811732065, \"accuracy\": 0.5661209068010076, \"f1\": 0.48658718330849476, \"f2\": 0.3719949868975732, \"f0_5\": 0.7032091320267069, \"p4\": 0.5469147740904808, \"phi\": 0.38198450559098546}, {\"truth_threshold\": 9.300000138580799, \"match_probability\": 0.9984160824655384, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 651.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1380.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.3205317577548006, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6794682422451994, \"precision\": 1.0, \"recall\": 0.3205317577548006, \"specificity\": 1.0, \"npv\": 0.4534653465346535, \"accuracy\": 0.5654911838790933, \"f1\": 0.4854586129753915, \"f2\": 0.37094017094017095, \"f0_5\": 0.7022653721682848, \"p4\": 0.5460709222975572, \"phi\": 0.3812480093136779}, {\"truth_threshold\": 9.400000140070915, \"match_probability\": 0.9985219959137808, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 649.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1382.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.31954702117183653, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6804529788281635, \"precision\": 1.0, \"recall\": 0.31954702117183653, \"specificity\": 1.0, \"npv\": 0.45310645033636726, \"accuracy\": 0.5648614609571788, \"f1\": 0.4843283582089552, \"f2\": 0.36988487404536646, \"f0_5\": 0.701318348822131, \"p4\": 0.5452252937050418, \"phi\": 0.3805112567043593}, {\"truth_threshold\": 9.500000141561031, \"match_probability\": 0.9986208369212233, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 648.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1383.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.3190546528803545, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6809453471196455, \"precision\": 1.0, \"recall\": 0.3190546528803545, \"specificity\": 1.0, \"npv\": 0.45292721518987344, \"accuracy\": 0.5645465994962217, \"f1\": 0.48376259798432253, \"f2\": 0.3693570451436389, \"f0_5\": 0.7008436080467229, \"p4\": 0.5448018093975668, \"phi\": 0.38014278294145043}, {\"truth_threshold\": 9.600000143051147, \"match_probability\": 0.9987130764898899, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 640.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1391.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.3151157065484983, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6848842934515017, \"precision\": 1.0, \"recall\": 0.3151157065484983, \"specificity\": 1.0, \"npv\": 0.4514984227129338, \"accuracy\": 0.5620277078085643, \"f1\": 0.47922126544365407, \"f2\": 0.3651300775901415, \"f0_5\": 0.6970159006752341, \"p4\": 0.5413976563199846, \"phi\": 0.3771925827461599}, {\"truth_threshold\": 9.700000144541264, \"match_probability\": 0.9987991544181472, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 637.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1394.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.31363860167405216, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6863613983259478, \"precision\": 1.0, \"recall\": 0.31363860167405216, \"specificity\": 1.0, \"npv\": 0.4509649468294604, \"accuracy\": 0.5610831234256927, \"f1\": 0.4775112443778111, \"f2\": 0.36354297454628465, \"f0_5\": 0.6955667176239354, \"p4\": 0.5401135374024728, \"phi\": 0.3760851171312224}, {\"truth_threshold\": 9.80000014603138, \"match_probability\": 0.9988794813467569, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 631.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1400.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.31068439192516, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6893156080748399, \"precision\": 1.0, \"recall\": 0.31068439192516, \"specificity\": 1.0, \"npv\": 0.449901768172888, \"accuracy\": 0.5591939546599496, \"f1\": 0.4740796393688956, \"f2\": 0.36036550542547113, \"f0_5\": 0.6926454445664105, \"p4\": 0.5375326892816356, \"phi\": 0.37386823517229706}, {\"truth_threshold\": 9.900000147521496, \"match_probability\": 0.9989544406735176, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 619.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1412.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.3047759724273757, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6952240275726244, \"precision\": 1.0, \"recall\": 0.3047759724273757, \"specificity\": 1.0, \"npv\": 0.44779037935080174, \"accuracy\": 0.5554156171284634, \"f1\": 0.4671698113207547, \"f2\": 0.35399748370124673, \"f0_5\": 0.6867095629021522, \"p4\": 0.532319348798713, \"phi\": 0.36942624204333946}, {\"truth_threshold\": 10.000000149011612, \"match_probability\": 0.9990243903445719, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 613.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1418.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.3018217626784835, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6981782373215165, \"precision\": 1.0, \"recall\": 0.3018217626784835, \"specificity\": 1.0, \"npv\": 0.44674209910261414, \"accuracy\": 0.5535264483627204, \"f1\": 0.4636913767019667, \"f2\": 0.350806913128076, \"f0_5\": 0.6836939549408878, \"p4\": 0.5296861510411321, \"phi\": 0.3672008820983914}, {\"truth_threshold\": 10.100000150501728, \"match_probability\": 0.9990896645300149, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 611.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1420.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.30083702609551943, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.6991629739044806, \"precision\": 1.0, \"recall\": 0.30083702609551943, \"specificity\": 1.0, \"npv\": 0.44639376218323584, \"accuracy\": 0.552896725440806, \"f1\": 0.46252838758516274, \"f2\": 0.34974241556954777, \"f0_5\": 0.6826815642458101, \"p4\": 0.5288043991760993, \"phi\": 0.3664584176721763}, {\"truth_threshold\": 10.200000151991844, \"match_probability\": 0.9991505751910027, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 593.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1438.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.2919743968488429, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.708025603151157, \"precision\": 1.0, \"recall\": 0.2919743968488429, \"specificity\": 1.0, \"npv\": 0.443283004258614, \"accuracy\": 0.5472292191435768, \"f1\": 0.45198170731707316, \"f2\": 0.34013995640702077, \"f0_5\": 0.6734044969339087, \"p4\": 0.5207754894170561, \"phi\": 0.35976004197485845}, {\"truth_threshold\": 10.30000015348196, \"match_probability\": 0.9992074135451509, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 592.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1439.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.2914820285573609, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.7085179714426391, \"precision\": 1.0, \"recall\": 0.2914820285573609, \"specificity\": 1.0, \"npv\": 0.44311145510835914, \"accuracy\": 0.5469143576826196, \"f1\": 0.45139153640869234, \"f2\": 0.33960532354290957, \"f0_5\": 0.6728802000454649, \"p4\": 0.5203244044266416, \"phi\": 0.3593870139723867}, {\"truth_threshold\": 10.400000154972076, \"match_probability\": 0.9992604514366183, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 579.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1452.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.28508124076809455, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.7149187592319055, \"precision\": 1.0, \"recall\": 0.28508124076809455, \"specificity\": 1.0, \"npv\": 0.4408933384674625, \"accuracy\": 0.5428211586901763, \"f1\": 0.4436781609195402, \"f2\": 0.33264391589107206, \"f0_5\": 0.665976535541753, \"p4\": 0.5144102851795553, \"phi\": 0.3545284473447676}, {\"truth_threshold\": 10.600000157952309, \"match_probability\": 0.9993561239419685, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 570.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1461.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.28064992614475626, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.7193500738552437, \"precision\": 1.0, \"recall\": 0.28064992614475626, \"specificity\": 1.0, \"npv\": 0.43937068303914045, \"accuracy\": 0.5399874055415617, \"f1\": 0.43829296424452135, \"f2\": 0.3278122843340235, \"f0_5\": 0.6610995128740431, \"p4\": 0.5102600262107828, \"phi\": 0.35115431044642736}, {\"truth_threshold\": 10.700000159442425, \"match_probability\": 0.9993992164911604, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 564.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1467.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.2776957163958641, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.7223042836041359, \"precision\": 1.0, \"recall\": 0.2776957163958641, \"specificity\": 1.0, \"npv\": 0.4383614088820827, \"accuracy\": 0.5380982367758187, \"f1\": 0.4346820809248555, \"f2\": 0.324585635359116, \"f0_5\": 0.6578026592022393, \"p4\": 0.5074669616635656, \"phi\": 0.34889982155313615}, {\"truth_threshold\": 10.800000160932541, \"match_probability\": 0.9994394266126935, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 559.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1472.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.27523387493845397, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.724766125061546, \"precision\": 1.0, \"recall\": 0.27523387493845397, \"specificity\": 1.0, \"npv\": 0.4375238823079862, \"accuracy\": 0.5365239294710328, \"f1\": 0.43166023166023165, \"f2\": 0.3218933548312795, \"f0_5\": 0.6550269510194516, \"p4\": 0.5051230066125968, \"phi\": 0.34701785761793746}, {\"truth_threshold\": 10.900000162422657, \"match_probability\": 0.9994769469006325, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 546.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1485.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.2688330871491876, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.7311669128508124, \"precision\": 1.0, \"recall\": 0.2688330871491876, \"specificity\": 1.0, \"npv\": 0.435361216730038, \"accuracy\": 0.5324307304785895, \"f1\": 0.42374854481955765, \"f2\": 0.314878892733564, \"f0_5\": 0.6476868327402135, \"p4\": 0.4989569646924318, \"phi\": 0.34211036219115415}, {\"truth_threshold\": 11.000000163912773, \"match_probability\": 0.9995119571076428, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 532.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1499.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.2619399310684392, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.7380600689315608, \"precision\": 1.0, \"recall\": 0.2619399310684392, \"specificity\": 1.0, \"npv\": 0.43305597579425115, \"accuracy\": 0.5280226700251889, \"f1\": 0.4151385095591104, \"f2\": 0.30730129390018485, \"f0_5\": 0.6395768213512864, \"p4\": 0.49219626037015723, \"phi\": 0.33680061230395913}, {\"truth_threshold\": 11.10000016540289, \"match_probability\": 0.9995446249976983, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 527.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1504.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.25947808961102903, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.740521910388971, \"precision\": 1.0, \"recall\": 0.25947808961102903, \"specificity\": 1.0, \"npv\": 0.4322385805964515, \"accuracy\": 0.5264483627204031, \"f1\": 0.41204065676309615, \"f2\": 0.3045890648479945, \"f0_5\": 0.6366272046388016, \"p4\": 0.489750317956443, \"phi\": 0.3348976577244906}, {\"truth_threshold\": 11.200000166893005, \"match_probability\": 0.9995751071426191, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 519.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1512.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.2555391432791728, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.7444608567208272, \"precision\": 1.0, \"recall\": 0.2555391432791728, \"specificity\": 1.0, \"npv\": 0.4309371471584494, \"accuracy\": 0.5239294710327456, \"f1\": 0.40705882352941175, \"f2\": 0.3002429711905588, \"f0_5\": 0.6318480642804967, \"p4\": 0.48580131404368865, \"phi\": 0.3318453094320319}, {\"truth_threshold\": 11.300000168383121, \"match_probability\": 0.9996035496660847, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 499.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1532.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.24569177744953224, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.7543082225504677, \"precision\": 1.0, \"recall\": 0.24569177744953224, \"specificity\": 1.0, \"npv\": 0.42771759432200224, \"accuracy\": 0.517632241813602, \"f1\": 0.39446640316205533, \"f2\": 0.2893424562217326, \"f0_5\": 0.6195679165631984, \"p4\": 0.47572994995033335, \"phi\": 0.32417078214331824}, {\"truth_threshold\": 11.400000169873238, \"match_probability\": 0.99963008893853, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 473.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1558.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.2328902018709995, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.7671097981290005, \"precision\": 1.0, \"recall\": 0.2328902018709995, \"specificity\": 1.0, \"npv\": 0.42360340362560117, \"accuracy\": 0.5094458438287154, \"f1\": 0.3777955271565495, \"f2\": 0.27509596370827033, \"f0_5\": 0.6028549579403518, \"p4\": 0.46218369067653414, \"phi\": 0.3140908820446858}, {\"truth_threshold\": 11.500000171363354, \"match_probability\": 0.999654852226126, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 470.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1561.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.23141309699655344, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.7685869030034466, \"precision\": 1.0, \"recall\": 0.23141309699655344, \"specificity\": 1.0, \"npv\": 0.4231337767923134, \"accuracy\": 0.5085012594458438, \"f1\": 0.3758496601359456, \"f2\": 0.2734465906446358, \"f0_5\": 0.600869342879059, \"p4\": 0.46058564943838975, \"phi\": 0.3129196346210279}, {\"truth_threshold\": 11.60000017285347, \"match_probability\": 0.9996779582968373, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 451.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1580.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.2220580994583949, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.7779419005416052, \"precision\": 1.0, \"recall\": 0.2220580994583949, \"specificity\": 1.0, \"npv\": 0.42018348623853213, \"accuracy\": 0.5025188916876574, \"f1\": 0.36341659951651895, \"f2\": 0.2629737609329446, \"f0_5\": 0.5880052151238592, \"p4\": 0.4502862275083601, \"phi\": 0.30545891111233126}, {\"truth_threshold\": 11.700000174343586, \"match_probability\": 0.9996995179863626, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 443.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1588.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.21811915312653865, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.7818808468734614, \"precision\": 1.0, \"recall\": 0.21811915312653865, \"specificity\": 1.0, \"npv\": 0.4189535309184047, \"accuracy\": 0.5, \"f1\": 0.3581244947453517, \"f2\": 0.25855025096299755, \"f0_5\": 0.5824349198001578, \"p4\": 0.4458538540579834, \"phi\": 0.30229420994007733}, {\"truth_threshold\": 11.900000177323818, \"match_probability\": 0.9997384050389891, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 440.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1591.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.21664204825209257, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.7833579517479075, \"precision\": 1.0, \"recall\": 0.21664204825209257, \"specificity\": 1.0, \"npv\": 0.41849415204678364, \"accuracy\": 0.49905541561712846, \"f1\": 0.35613112100364225, \"f2\": 0.25688930406352173, \"f0_5\": 0.5803218148245846, \"p4\": 0.4441765200515326, \"phi\": 0.3011036869268423}, {\"truth_threshold\": 12.000000178813934, \"match_probability\": 0.9997559189953416, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 436.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1595.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.21467257508616444, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.7853274249138356, \"precision\": 1.0, \"recall\": 0.21467257508616444, \"specificity\": 1.0, \"npv\": 0.41788321167883213, \"accuracy\": 0.49779596977329976, \"f1\": 0.3534657478719092, \"f2\": 0.2546728971962617, \"f0_5\": 0.5774834437086093, \"p4\": 0.4419269349297068, \"phi\": 0.2995130466880727}, {\"truth_threshold\": 12.200000181794167, \"match_probability\": 0.9997875084304283, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 434.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1597.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.2136878385032004, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.7863121614967996, \"precision\": 1.0, \"recall\": 0.2136878385032004, \"specificity\": 1.0, \"npv\": 0.4175784099197666, \"accuracy\": 0.4971662468513854, \"f1\": 0.35212981744421906, \"f2\": 0.25356391680299134, \"f0_5\": 0.5760552163525352, \"p4\": 0.4407964459882871, \"phi\": 0.29871629989231974}, {\"truth_threshold\": 12.300000183284283, \"match_probability\": 0.9998017355340825, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 429.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1602.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.21122599704579026, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.7887740029542097, \"precision\": 1.0, \"recall\": 0.21122599704579026, \"specificity\": 1.0, \"npv\": 0.4168183472879505, \"accuracy\": 0.49559193954659947, \"f1\": 0.348780487804878, \"f2\": 0.2507891967730621, \"f0_5\": 0.5724579663730984, \"p4\": 0.43795337341921126, \"phi\": 0.2967201897291045}, {\"truth_threshold\": 12.400000184774399, \"match_probability\": 0.9998150102562988, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 425.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1606.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.20925652387986213, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.7907434761201378, \"precision\": 1.0, \"recall\": 0.20925652387986213, \"specificity\": 1.0, \"npv\": 0.4162122864412941, \"accuracy\": 0.49433249370277077, \"f1\": 0.34609120521172637, \"f2\": 0.24856708386945842, \"f0_5\": 0.5695523988206915, \"p4\": 0.43566133681892244, \"phi\": 0.2951188510698608}, {\"truth_threshold\": 12.500000186264515, \"match_probability\": 0.9998273963279586, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 412.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1619.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.20285573609059576, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.7971442639094042, \"precision\": 1.0, \"recall\": 0.20285573609059576, \"specificity\": 1.0, \"npv\": 0.4142547033285094, \"accuracy\": 0.49023929471032746, \"f1\": 0.3372902169463774, \"f2\": 0.24133083411433928, \"f0_5\": 0.5599347648817613, \"p4\": 0.42810118466525837, \"phi\": 0.28988608585562736}, {\"truth_threshold\": 12.600000187754631, \"match_probability\": 0.9998389532181915, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 410.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1621.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.2018709995076317, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.7981290004923683, \"precision\": 1.0, \"recall\": 0.2018709995076317, \"specificity\": 1.0, \"npv\": 0.4139551699204628, \"accuracy\": 0.4896095717884131, \"f1\": 0.33592789840229414, \"f2\": 0.24021560815561285, \"f0_5\": 0.5584309452465268, \"p4\": 0.4269226353344527, \"phi\": 0.2890770553039368}, {\"truth_threshold\": 12.700000189244747, \"match_probability\": 0.9998497364189812, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 402.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1629.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.19793205317577547, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8020679468242246, \"precision\": 1.0, \"recall\": 0.19793205317577547, \"specificity\": 1.0, \"npv\": 0.41276135544340303, \"accuracy\": 0.4870906801007557, \"f1\": 0.33045622688039455, \"f2\": 0.23574947220267417, \"f0_5\": 0.5523495465787304, \"p4\": 0.4221659841746226, \"phi\": 0.2858298489565581}, {\"truth_threshold\": 12.800000190734863, \"match_probability\": 0.9998597977108138, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 381.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1650.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.18759231905465287, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8124076809453471, \"precision\": 1.0, \"recall\": 0.18759231905465287, \"specificity\": 1.0, \"npv\": 0.40966010733452596, \"accuracy\": 0.4804785894206549, \"f1\": 0.31592039800995025, \"f2\": 0.2239858906525573, \"f0_5\": 0.5358649789029536, \"p4\": 0.4093429793942124, \"phi\": 0.27721668340679234}, {\"truth_threshold\": 12.90000019222498, \"match_probability\": 0.9998691854106266, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 361.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1670.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.1777449532250123, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8222550467749877, \"precision\": 1.0, \"recall\": 0.1777449532250123, \"specificity\": 1.0, \"npv\": 0.4067495559502664, \"accuracy\": 0.47418136020151136, \"f1\": 0.30183946488294316, \"f2\": 0.21272834413671185, \"f0_5\": 0.5194244604316547, \"p4\": 0.396646195182804, \"phi\": 0.26888228055540336}, {\"truth_threshold\": 13.300000198185444, \"match_probability\": 0.9999008579398913, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 354.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1677.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.17429837518463812, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8257016248153619, \"precision\": 1.0, \"recall\": 0.17429837518463812, \"specificity\": 1.0, \"npv\": 0.4057406094968108, \"accuracy\": 0.47197732997481107, \"f1\": 0.2968553459119497, \"f2\": 0.20877565463552725, \"f0_5\": 0.5134899912967799, \"p4\": 0.3920831758418028, \"phi\": 0.2659321886904984}, {\"truth_threshold\": 13.40000019967556, \"match_probability\": 0.999907496573012, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 352.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1679.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.17331363860167406, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.826686361398326, \"precision\": 1.0, \"recall\": 0.17331363860167406, \"specificity\": 1.0, \"npv\": 0.4054532577903683, \"accuracy\": 0.47134760705289674, \"f1\": 0.2954259336970206, \"f2\": 0.20764511562057575, \"f0_5\": 0.5117766792672288, \"p4\": 0.3907676489102373, \"phi\": 0.26508598490027957}, {\"truth_threshold\": 13.600000202655792, \"match_probability\": 0.9999194701253888, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 351.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1680.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.172821270310192, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.827178729689808, \"precision\": 1.0, \"recall\": 0.172821270310192, \"specificity\": 1.0, \"npv\": 0.40530973451327434, \"accuracy\": 0.47103274559193953, \"f1\": 0.2947103274559194, \"f2\": 0.20707964601769913, \"f0_5\": 0.5109170305676856, \"p4\": 0.3901078901006101, \"phi\": 0.26466231916854116}, {\"truth_threshold\": 13.700000204145908, \"match_probability\": 0.9999248625650565, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 347.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1684.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.1708517971442639, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8291482028557361, \"precision\": 1.0, \"recall\": 0.1708517971442639, \"specificity\": 1.0, \"npv\": 0.4047366560622128, \"accuracy\": 0.46977329974811083, \"f1\": 0.29184188393608074, \"f2\": 0.20481643253452958, \"f0_5\": 0.5074583211465341, \"p4\": 0.38745542187943405, \"phi\": 0.2629638474360856}, {\"truth_threshold\": 13.800000205636024, \"match_probability\": 0.999929893941616, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 344.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1687.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.16937469226981783, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8306253077301822, \"precision\": 1.0, \"recall\": 0.16937469226981783, \"specificity\": 1.0, \"npv\": 0.4043079096045198, \"accuracy\": 0.4688287153652393, \"f1\": 0.28968421052631577, \"f2\": 0.2031176192725555, \"f0_5\": 0.5048429703551511, \"p4\": 0.38545180714785676, \"phi\": 0.26168593346131325}, {\"truth_threshold\": 13.90000020712614, \"match_probability\": 0.9999345884275949, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 339.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1692.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.16691285081240767, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8330871491875923, \"precision\": 1.0, \"recall\": 0.16691285081240767, \"specificity\": 1.0, \"npv\": 0.4035953471977441, \"accuracy\": 0.4672544080604534, \"f1\": 0.28607594936708863, \"f2\": 0.20028358738036156, \"f0_5\": 0.5004428697962799, \"p4\": 0.3820848184886705, \"phi\": 0.2595481650395528}, {\"truth_threshold\": 14.000000208616257, \"match_probability\": 0.9999389685776376, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 331.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1700.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.16297390448055146, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8370260955194485, \"precision\": 1.0, \"recall\": 0.16297390448055146, \"specificity\": 1.0, \"npv\": 0.4024604569420035, \"accuracy\": 0.464735516372796, \"f1\": 0.2802709568162574, \"f2\": 0.19574216439976344, \"f0_5\": 0.4932935916542474, \"p4\": 0.3766241509696461, \"phi\": 0.25610652484242796}, {\"truth_threshold\": 14.200000211596489, \"match_probability\": 0.9999468686412301, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 317.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1714.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.15608074839980304, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.843919251600197, \"precision\": 1.0, \"recall\": 0.15608074839980304, \"specificity\": 1.0, \"npv\": 0.40048968170689053, \"accuracy\": 0.46032745591939545, \"f1\": 0.27001703577512776, \"f2\": 0.18777396043122851, \"f0_5\": 0.48044862079418005, \"p4\": 0.36684178397302697, \"phi\": 0.2500174578848653}, {\"truth_threshold\": 14.400000214576721, \"match_probability\": 0.9999537461476637, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 316.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1715.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.15558838010832102, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.844411619891679, \"precision\": 1.0, \"recall\": 0.15558838010832102, \"specificity\": 1.0, \"npv\": 0.40034965034965037, \"accuracy\": 0.4600125944584383, \"f1\": 0.26927993182786536, \"f2\": 0.1872037914691943, \"f0_5\": 0.4795144157814871, \"p4\": 0.366131657936778, \"phi\": 0.2495791529251488}, {\"truth_threshold\": 14.70000021904707, \"match_probability\": 0.9999624298714548, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 315.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1716.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.155096011816839, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.844903988183161, \"precision\": 1.0, \"recall\": 0.155096011816839, \"specificity\": 1.0, \"npv\": 0.400209716882209, \"accuracy\": 0.4596977329974811, \"f1\": 0.26854219948849106, \"f2\": 0.18663348738002133, \"f0_5\": 0.4785779398359161, \"p4\": 0.36541997841978086, \"phi\": 0.2491403840784887}, {\"truth_threshold\": 15.000000223517418, \"match_probability\": 0.9999694833578969, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 314.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1717.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.15460364352535697, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8453963564746431, \"precision\": 1.0, \"recall\": 0.15460364352535697, \"specificity\": 1.0, \"npv\": 0.4000698812019567, \"accuracy\": 0.4593828715365239, \"f1\": 0.2678038379530917, \"f2\": 0.1860630481156672, \"f0_5\": 0.4776391846668695, \"p4\": 0.36470673862472397, \"phi\": 0.2487011485670688}, {\"truth_threshold\": 15.100000225007534, \"match_probability\": 0.9999715269079685, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 307.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1724.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.15115706548498276, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8488429345150172, \"precision\": 1.0, \"recall\": 0.15115706548498276, \"specificity\": 1.0, \"npv\": 0.399093760892297, \"accuracy\": 0.45717884130982367, \"f1\": 0.262617621899059, \"f2\": 0.18206618431977226, \"f0_5\": 0.4710033752684873, \"p4\": 0.35966979322171594, \"phi\": 0.24561319539032303}, {\"truth_threshold\": 15.20000022649765, \"match_probability\": 0.9999734336151354, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 305.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1726.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.1501723289020187, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8498276710979813, \"precision\": 1.0, \"recall\": 0.1501723289020187, \"specificity\": 1.0, \"npv\": 0.39881574364332983, \"accuracy\": 0.45654911838790935, \"f1\": 0.2611301369863014, \"f2\": 0.18092300391505517, \"f0_5\": 0.46908643494309443, \"p4\": 0.3582162273053647, \"phi\": 0.24472655970635737}, {\"truth_threshold\": 15.300000227987766, \"match_probability\": 0.9999752126423825, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 302.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1729.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.14869522402757263, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8513047759724274, \"precision\": 1.0, \"recall\": 0.14869522402757263, \"specificity\": 1.0, \"npv\": 0.39839944328462074, \"accuracy\": 0.4556045340050378, \"f1\": 0.25889412773253323, \"f2\": 0.17920721576074056, \"f0_5\": 0.46619326952763196, \"p4\": 0.3560236322925244, \"phi\": 0.243392880897669}, {\"truth_threshold\": 15.400000229477882, \"match_probability\": 0.9999768725392036, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 297.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1734.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.14623338257016247, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8537666174298375, \"precision\": 1.0, \"recall\": 0.14623338257016247, \"specificity\": 1.0, \"npv\": 0.39770753733935393, \"accuracy\": 0.4540302267002519, \"f1\": 0.2551546391752577, \"f2\": 0.17634485215532597, \"f0_5\": 0.4613233923578751, \"p4\": 0.3523361639168977, \"phi\": 0.24115994372777358}, {\"truth_threshold\": 15.70000023394823, \"match_probability\": 0.9999812145830361, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 287.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1744.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.1413096996553422, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8586903003446578, \"precision\": 1.0, \"recall\": 0.1413096996553422, \"specificity\": 1.0, \"npv\": 0.39633091034960194, \"accuracy\": 0.4508816120906801, \"f1\": 0.24762726488352027, \"f2\": 0.17060991558673166, \"f0_5\": 0.45139981126140294, \"p4\": 0.34483355667090254, \"phi\": 0.23665460465756968}, {\"truth_threshold\": 15.900000236928463, \"match_probability\": 0.9999836463049459, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 279.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1752.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.13737075332348597, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8626292466765141, \"precision\": 1.0, \"recall\": 0.13737075332348597, \"specificity\": 1.0, \"npv\": 0.39523645150155334, \"accuracy\": 0.44836272040302266, \"f1\": 0.24155844155844156, \"f2\": 0.16601213852195645, \"f0_5\": 0.4432793136320305, \"p4\": 0.3387045984433298, \"phi\": 0.23301057719268842}, {\"truth_threshold\": 16.00000023841858, \"match_probability\": 0.9999847414462861, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 276.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1755.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.1358936484490399, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8641063515509602, \"precision\": 1.0, \"recall\": 0.1358936484490399, \"specificity\": 1.0, \"npv\": 0.39482758620689656, \"accuracy\": 0.4474181360201511, \"f1\": 0.23927178153446033, \"f2\": 0.16428571428571428, \"f0_5\": 0.44019138755980863, \"p4\": 0.33637622790175986, \"phi\": 0.23163454232472105}, {\"truth_threshold\": 16.100000239908695, \"match_probability\": 0.9999857632514492, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 274.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1757.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.13490891186607581, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8650910881339242, \"precision\": 1.0, \"recall\": 0.13490891186607581, \"specificity\": 1.0, \"npv\": 0.3945554789800138, \"accuracy\": 0.4467884130982368, \"f1\": 0.23774403470715835, \"f2\": 0.16313407954274828, \"f0_5\": 0.4381196034537896, \"p4\": 0.33481470493206833, \"phi\": 0.23071421789736327}, {\"truth_threshold\": 16.20000024139881, \"match_probability\": 0.9999867166312594, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 271.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1760.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.13343180699162974, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8665681930083703, \"precision\": 1.0, \"recall\": 0.13343180699162974, \"specificity\": 1.0, \"npv\": 0.39414802065404475, \"accuracy\": 0.44584382871536526, \"f1\": 0.23544743701129453, \"f2\": 0.16140559857057774, \"f0_5\": 0.434991974317817, \"p4\": 0.33245833489583054, \"phi\": 0.22932920140715485}, {\"truth_threshold\": 16.300000242888927, \"match_probability\": 0.9999876061677141, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 268.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1763.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.13195470211718366, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8680452978828164, \"precision\": 1.0, \"recall\": 0.13195470211718366, \"specificity\": 1.0, \"npv\": 0.3937414030261348, \"accuracy\": 0.4448992443324937, \"f1\": 0.23314484558503698, \"f2\": 0.15967588179218303, \"f0_5\": 0.43184015468901066, \"p4\": 0.3300848527615133, \"phi\": 0.22793865303523134}, {\"truth_threshold\": 16.400000244379044, \"match_probability\": 0.9999884361359999, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 264.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1767.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.12998522895125553, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8700147710487445, \"precision\": 1.0, \"recall\": 0.12998522895125553, \"specificity\": 1.0, \"npv\": 0.39320054945054944, \"accuracy\": 0.44363979848866497, \"f1\": 0.23006535947712417, \"f2\": 0.15736766809728184, \"f0_5\": 0.42759961127308066, \"p4\": 0.32689317655913785, \"phi\": 0.22607579137114425}, {\"truth_threshold\": 16.50000024586916, \"match_probability\": 0.9999892105250341, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 262.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1769.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.12900049236829148, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8709995076317085, \"precision\": 1.0, \"recall\": 0.12900049236829148, \"specificity\": 1.0, \"npv\": 0.39293067947838023, \"accuracy\": 0.44301007556675065, \"f1\": 0.2285215874400349, \"f2\": 0.1562127355115669, \"f0_5\": 0.425462812601494, \"p4\": 0.32528558993728013, \"phi\": 0.22514051416686065}, {\"truth_threshold\": 16.600000247359276, \"match_probability\": 0.9999899330566321, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 259.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1772.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.1275233874938454, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8724766125061546, \"precision\": 1.0, \"recall\": 0.1275233874938454, \"specificity\": 1.0, \"npv\": 0.39252656839218375, \"accuracy\": 0.4420654911838791, \"f1\": 0.2262008733624454, \"f2\": 0.15447930335202195, \"f0_5\": 0.42223671340071733, \"p4\": 0.3228593171458215, \"phi\": 0.2237326924763251}, {\"truth_threshold\": 16.700000248849392, \"match_probability\": 0.9999906072033913, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 249.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1782.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.12259970457902511, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8774002954209749, \"precision\": 1.0, \"recall\": 0.12259970457902511, \"specificity\": 1.0, \"npv\": 0.3911855141783396, \"accuracy\": 0.4389168765743073, \"f1\": 0.21842105263157896, \"f2\": 0.14869222500895737, \"f0_5\": 0.41129831516352827, \"p4\": 0.3146395889340626, \"phi\": 0.21899595538241903}, {\"truth_threshold\": 16.800000250339508, \"match_probability\": 0.9999912362053778, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 241.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1790.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.11866075824716889, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8813392417528311, \"precision\": 1.0, \"recall\": 0.11866075824716889, \"specificity\": 1.0, \"npv\": 0.3901192504258944, \"accuracy\": 0.4363979848866499, \"f1\": 0.21214788732394366, \"f2\": 0.14405260011954574, \"f0_5\": 0.4023372287145242, \"p4\": 0.3079124729407039, \"phi\": 0.2151553997982709}, {\"truth_threshold\": 16.900000251829624, \"match_probability\": 0.999991823085696, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 240.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1791.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.11816838995568685, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8818316100443131, \"precision\": 1.0, \"recall\": 0.11816838995568685, \"specificity\": 1.0, \"npv\": 0.38998637602179836, \"accuracy\": 0.4360831234256927, \"f1\": 0.21136063408190225, \"f2\": 0.14347202295552366, \"f0_5\": 0.4012036108324975, \"p4\": 0.30706183558280153, \"phi\": 0.214671987364791}, {\"truth_threshold\": 17.00000025331974, \"match_probability\": 0.9999923706650156, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 236.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1795.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.11619891678975874, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8838010832102413, \"precision\": 1.0, \"recall\": 0.11619891678975874, \"specificity\": 1.0, \"npv\": 0.38945578231292516, \"accuracy\": 0.43482367758186397, \"f1\": 0.20820467578297308, \"f2\": 0.14114832535885166, \"f0_5\": 0.39663865546218485, \"p4\": 0.30363716807944324, \"phi\": 0.21273067489732173}, {\"truth_threshold\": 17.100000254809856, \"match_probability\": 0.9999928815751264, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 235.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1796.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.11570654849827672, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8842934515017233, \"precision\": 1.0, \"recall\": 0.11570654849827672, \"specificity\": 1.0, \"npv\": 0.3893233594015641, \"accuracy\": 0.4345088161209068, \"f1\": 0.20741394527802295, \"f2\": 0.14056705347529608, \"f0_5\": 0.39548973409626387, \"p4\": 0.3027754178880268, \"phi\": 0.21224340311564244}, {\"truth_threshold\": 17.200000256299973, \"match_probability\": 0.999993358271586, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 234.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1797.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.11521418020679468, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8847858197932054, \"precision\": 1.0, \"recall\": 0.11521418020679468, \"specificity\": 1.0, \"npv\": 0.3891910265125765, \"accuracy\": 0.43419395465994964, \"f1\": 0.20662251655629138, \"f2\": 0.1399856424982053, \"f0_5\": 0.3943377148634985, \"p4\": 0.30191141261310905, \"phi\": 0.21175534246740363}, {\"truth_threshold\": 17.30000025779009, \"match_probability\": 0.999993803045519, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 230.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1801.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.11324470704086657, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8867552929591335, \"precision\": 1.0, \"recall\": 0.11324470704086657, \"specificity\": 1.0, \"npv\": 0.38866259334691106, \"accuracy\": 0.4329345088161209, \"f1\": 0.2034498009730208, \"f2\": 0.13765860665549437, \"f0_5\": 0.3896984073195527, \"p4\": 0.2984326196428951, \"phi\": 0.2097950941307122}, {\"truth_threshold\": 17.400000259280205, \"match_probability\": 0.9999942180346287, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 228.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1803.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.11225997045790251, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8877400295420975, \"precision\": 1.0, \"recall\": 0.11225997045790251, \"specificity\": 1.0, \"npv\": 0.3883989145183175, \"accuracy\": 0.43230478589420657, \"f1\": 0.20185922974767595, \"f2\": 0.13649425287356323, \"f0_5\": 0.3873598369011213, \"p4\": 0.2966794033341487, \"phi\": 0.2088100827778863}, {\"truth_threshold\": 17.50000026077032, \"match_probability\": 0.9999946052334694, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 224.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1807.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.11029049729197439, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8897095027080256, \"precision\": 1.0, \"recall\": 0.11029049729197439, \"specificity\": 1.0, \"npv\": 0.38787262872628725, \"accuracy\": 0.4310453400503778, \"f1\": 0.19866962305986696, \"f2\": 0.13416387158600862, \"f0_5\": 0.38264434574649814, \"p4\": 0.29314487543657275, \"phi\": 0.20683003918233825}, {\"truth_threshold\": 17.600000262260437, \"match_probability\": 0.999994966503032, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 218.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1813.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.10733628754308222, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8926637124569178, \"precision\": 1.0, \"recall\": 0.10733628754308222, \"specificity\": 1.0, \"npv\": 0.3870858688302907, \"accuracy\": 0.42915617128463474, \"f1\": 0.19386393952867942, \"f2\": 0.13066410932630065, \"f0_5\": 0.3754736479503961, \"p4\": 0.28777144299895174, \"phi\": 0.2038341485635611}, {\"truth_threshold\": 17.700000263750553, \"match_probability\": 0.9999953035796879, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 214.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1817.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.10536681437715412, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8946331856228459, \"precision\": 1.0, \"recall\": 0.10536681437715412, \"specificity\": 1.0, \"npv\": 0.3865631330182309, \"accuracy\": 0.42789672544080604, \"f1\": 0.19064587973273942, \"f2\": 0.12832813624370354, \"f0_5\": 0.37062694838933147, \"p4\": 0.2841402750546976, \"phi\": 0.20181904241617807}, {\"truth_threshold\": 17.80000026524067, \"match_probability\": 0.9999956180835331, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 212.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1819.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.10438207779419005, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8956179222058099, \"precision\": 1.0, \"recall\": 0.10438207779419005, \"specificity\": 1.0, \"npv\": 0.38630229419703105, \"accuracy\": 0.4272670025188917, \"f1\": 0.18903254569772626, \"f2\": 0.12715930902111325, \"f0_5\": 0.3681833970128517, \"p4\": 0.2823097318295965, \"phi\": 0.20080596635794612}, {\"truth_threshold\": 17.900000266730785, \"match_probability\": 0.9999959115261747, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 207.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1824.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.1019202363367799, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8980797636632201, \"precision\": 1.0, \"recall\": 0.1019202363367799, \"specificity\": 1.0, \"npv\": 0.3856517345907713, \"accuracy\": 0.4256926952141058, \"f1\": 0.18498659517426275, \"f2\": 0.12423478574000721, \"f0_5\": 0.3620146904512067, \"p4\": 0.2776889307543874, \"phi\": 0.19825669202622276}, {\"truth_threshold\": 18.0000002682209, \"match_probability\": 0.9999961853179954, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 205.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1826.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.10093549975381585, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.8990645002461841, \"precision\": 1.0, \"recall\": 0.10093549975381585, \"specificity\": 1.0, \"npv\": 0.38539212386401883, \"accuracy\": 0.4250629722921914, \"f1\": 0.18336314847942756, \"f2\": 0.12306399327650379, \"f0_5\": 0.3595229743949491, \"p4\": 0.27582256169212693, \"phi\": 0.1972301868969333}, {\"truth_threshold\": 18.200000271201134, \"match_probability\": 0.9999966791247992, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 198.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1833.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.09748892171344166, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9025110782865583, \"precision\": 1.0, \"recall\": 0.09748892171344166, \"specificity\": 1.0, \"npv\": 0.3844862323707186, \"accuracy\": 0.4228589420654912, \"f1\": 0.17765814266487215, \"f2\": 0.11896178803172314, \"f0_5\": 0.35069075451647186, \"p4\": 0.26920710012797755, \"phi\": 0.1936056512798248}, {\"truth_threshold\": 18.30000027269125, \"match_probability\": 0.999996901513191, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 196.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1835.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.0965041851304776, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9034958148695224, \"precision\": 1.0, \"recall\": 0.0965041851304776, \"specificity\": 1.0, \"npv\": 0.38422818791946306, \"accuracy\": 0.4222292191435768, \"f1\": 0.1760215536596318, \"f2\": 0.11778846153846154, \"f0_5\": 0.3481349911190053, \"p4\": 0.2672927556600361, \"phi\": 0.19256071296951466}, {\"truth_threshold\": 18.400000274181366, \"match_probability\": 0.9999971090089864, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 193.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1838.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.09502708025603152, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9049729197439685, \"precision\": 1.0, \"recall\": 0.09502708025603152, \"specificity\": 1.0, \"npv\": 0.38384177003017095, \"accuracy\": 0.42128463476070527, \"f1\": 0.1735611510791367, \"f2\": 0.11602741373091259, \"f0_5\": 0.3442739921512665, \"p4\": 0.26440064321915874, \"phi\": 0.19098524206407744}, {\"truth_threshold\": 18.500000275671482, \"match_probability\": 0.9999973026094866, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 190.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1841.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.09354997538158542, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9064500246184146, \"precision\": 1.0, \"recall\": 0.09354997538158542, \"specificity\": 1.0, \"npv\": 0.38345612860013395, \"accuracy\": 0.42034005037783373, \"f1\": 0.1710941017559658, \"f2\": 0.11426509502044743, \"f0_5\": 0.3403797921891795, \"p4\": 0.2614834724836873, \"phi\": 0.18939987167487887}, {\"truth_threshold\": 18.600000277161598, \"match_probability\": 0.999997483245208, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 187.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1844.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.09207287050713935, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9079271294928607, \"precision\": 1.0, \"recall\": 0.09207287050713935, \"specificity\": 1.0, \"npv\": 0.3830712612914018, \"accuracy\": 0.4193954659949622, \"f1\": 0.16862037871956717, \"f2\": 0.11250150403080256, \"f0_5\": 0.3364519611370997, \"p4\": 0.2585408498527471, \"phi\": 0.18780434136592739}, {\"truth_threshold\": 18.700000278651714, \"match_probability\": 0.9999976517843541, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 186.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1845.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.0915805022156573, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9084194977843427, \"precision\": 1.0, \"recall\": 0.0915805022156573, \"specificity\": 1.0, \"npv\": 0.38294314381270905, \"accuracy\": 0.41908060453400503, \"f1\": 0.16779431664411368, \"f2\": 0.11191335740072202, \"f0_5\": 0.33513513513513515, \"p4\": 0.2575542504705109, \"phi\": 0.1872701936518745}, {\"truth_threshold\": 18.80000028014183, \"match_probability\": 0.9999978090369889, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 177.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1854.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.08714918759231906, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.912850812407681, \"precision\": 1.0, \"recall\": 0.08714918759231906, \"specificity\": 1.0, \"npv\": 0.3817939313104368, \"accuracy\": 0.4162468513853904, \"f1\": 0.16032608695652173, \"f2\": 0.10661366100469823, \"f0_5\": 0.3231106243154436, \"p4\": 0.24854306779885504, \"phi\": 0.18240896617595923}, {\"truth_threshold\": 18.900000281631947, \"match_probability\": 0.9999979557589296, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 173.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1858.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.08517971442639094, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9148202855736091, \"precision\": 1.0, \"recall\": 0.08517971442639094, \"specificity\": 1.0, \"npv\": 0.3812853812853813, \"accuracy\": 0.4149874055415617, \"f1\": 0.1569872958257713, \"f2\": 0.10425454983729059, \"f0_5\": 0.3176643408005876, \"p4\": 0.24446004916721792, \"phi\": 0.18021592574699485}, {\"truth_threshold\": 19.000000283122063, \"match_probability\": 0.9999980926553794, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 167.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1864.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.08222550467749877, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9177744953225012, \"precision\": 1.0, \"recall\": 0.08222550467749877, \"specificity\": 1.0, \"npv\": 0.3805250913924892, \"accuracy\": 0.41309823677581864, \"f1\": 0.15195632393084624, \"f2\": 0.10071161500422145, \"f0_5\": 0.30937384216376435, \"p4\": 0.2382423776518271, \"phi\": 0.17688659554132066}, {\"truth_threshold\": 19.10000028461218, \"match_probability\": 0.9999982203843173, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 164.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1867.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.08074839980305268, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9192516001969473, \"precision\": 1.0, \"recall\": 0.08074839980305268, \"specificity\": 1.0, \"npv\": 0.3801460823373174, \"accuracy\": 0.4121536523929471, \"f1\": 0.14943052391799544, \"f2\": 0.09893822393822393, \"f0_5\": 0.30517305545217716, \"p4\": 0.23509063941010752, \"phi\": 0.17520327576885625}, {\"truth_threshold\": 19.200000286102295, \"match_probability\": 0.9999983395596597, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 155.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1876.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.07631708517971443, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9236829148202855, \"precision\": 1.0, \"recall\": 0.07631708517971443, \"specificity\": 1.0, \"npv\": 0.3790135716650116, \"accuracy\": 0.4093198992443325, \"f1\": 0.14181152790484905, \"f2\": 0.09361033941297259, \"f0_5\": 0.29234251225952473, \"p4\": 0.22545812557563438, \"phi\": 0.1700741339329014}, {\"truth_threshold\": 19.30000028759241, \"match_probability\": 0.9999984507542113, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 139.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1892.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.06843919251600197, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.931560807483998, \"precision\": 1.0, \"recall\": 0.06843919251600197, \"specificity\": 1.0, \"npv\": 0.37701679288771817, \"accuracy\": 0.40428211586901763, \"f1\": 0.12811059907834102, \"f2\": 0.08410988745007866, \"f0_5\": 0.2686509470429068, \"p4\": 0.20764213874468843, \"phi\": 0.16063226596860353}, {\"truth_threshold\": 19.400000289082527, \"match_probability\": 0.9999985545024187, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 126.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1905.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.0620384047267356, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9379615952732644, \"precision\": 1.0, \"recall\": 0.0620384047267356, \"specificity\": 1.0, \"npv\": 0.37540983606557377, \"accuracy\": 0.4001889168765743, \"f1\": 0.1168289290681502, \"f2\": 0.07636363636363637, \"f0_5\": 0.2485207100591716, \"p4\": 0.19246681908459196, \"phi\": 0.1526100499581647}, {\"truth_threshold\": 19.500000290572643, \"match_probability\": 0.9999986513029383, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 125.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1906.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.06154603643525357, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9384539635647464, \"precision\": 1.0, \"recall\": 0.06154603643525357, \"specificity\": 1.0, \"npv\": 0.37528679121599473, \"accuracy\": 0.3998740554156171, \"f1\": 0.11595547309833024, \"f2\": 0.07576675960722512, \"f0_5\": 0.24693796918214145, \"p4\": 0.19127198374939863, \"phi\": 0.1519783357121962}, {\"truth_threshold\": 19.60000029206276, \"match_probability\": 0.9999987416210334, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 124.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1907.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.06105366814377154, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9389463318562284, \"precision\": 1.0, \"recall\": 0.06105366814377154, \"specificity\": 1.0, \"npv\": 0.3751638269986894, \"accuracy\": 0.39955919395465994, \"f1\": 0.11508120649651972, \"f2\": 0.07516973811833172, \"f0_5\": 0.24535021764938664, \"p4\": 0.19007310478285272, \"phi\": 0.15134440126124685}, {\"truth_threshold\": 19.700000293552876, \"match_probability\": 0.9999988258908107, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 123.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1908.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.060561299852289516, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9394387001477105, \"precision\": 1.0, \"recall\": 0.060561299852289516, \"specificity\": 1.0, \"npv\": 0.37504094333442517, \"accuracy\": 0.3992443324937028, \"f1\": 0.11420612813370473, \"f2\": 0.07457257184430702, \"f0_5\": 0.2437574316290131, \"p4\": 0.1888701585551805, \"phi\": 0.1507082181772502}, {\"truth_threshold\": 19.80000029504299, \"match_probability\": 0.9999989045173057, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 121.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1910.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.05957656326932546, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9404234367306745, \"precision\": 1.0, \"recall\": 0.05957656326932546, \"specificity\": 1.0, \"npv\": 0.37479541734860883, \"accuracy\": 0.3986146095717884, \"f1\": 0.11245353159851301, \"f2\": 0.07337780473013948, \"f0_5\": 0.24055666003976142, \"p4\": 0.18645196888541976, \"phi\": 0.1494289894723331}, {\"truth_threshold\": 19.900000296533108, \"match_probability\": 0.9999989778784306, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 113.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1918.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.05563761693746923, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9443623830625307, \"precision\": 1.0, \"recall\": 0.05563761693746923, \"specificity\": 1.0, \"npv\": 0.37381651975187724, \"accuracy\": 0.396095717884131, \"f1\": 0.10541044776119403, \"f2\": 0.06859293432074784, \"f0_5\": 0.22754732178815948, \"p4\": 0.1766116659113618, \"phi\": 0.14421601967483658}, {\"truth_threshold\": 20.10000029951334, \"match_probability\": 0.9999991101913761, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 111.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1920.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.05465288035450517, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9453471196454948, \"precision\": 1.0, \"recall\": 0.05465288035450517, \"specificity\": 1.0, \"npv\": 0.3735725938009788, \"accuracy\": 0.3954659949622166, \"f1\": 0.10364145658263306, \"f2\": 0.06739526411657559, \"f0_5\": 0.22424242424242424, \"p4\": 0.1741087023528203, \"phi\": 0.142887432172067}, {\"truth_threshold\": 20.200000301003456, \"match_probability\": 0.9999991697791492, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 109.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1922.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.05366814377154111, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9463318562284588, \"precision\": 1.0, \"recall\": 0.05366814377154111, \"specificity\": 1.0, \"npv\": 0.3733289859797848, \"accuracy\": 0.39483627204030225, \"f1\": 0.10186915887850467, \"f2\": 0.06619701202477833, \"f0_5\": 0.22091609241994326, \"p4\": 0.17158817188926406, \"phi\": 0.14154813207402897}, {\"truth_threshold\": 20.300000302493572, \"match_probability\": 0.9999992253765136, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 91.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1940.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.0448055145248646, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9551944854751354, \"precision\": 1.0, \"recall\": 0.0448055145248646, \"specificity\": 1.0, \"npv\": 0.3711507293354943, \"accuracy\": 0.3891687657430731, \"f1\": 0.08576814326107446, \"f2\": 0.05538648813146683, \"f0_5\": 0.18997912317327767, \"p4\": 0.1480768274225295, \"phi\": 0.1289558040343884}, {\"truth_threshold\": 20.40000030398369, \"match_probability\": 0.9999992772506945, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 86.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1945.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.04234367306745446, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9576563269325455, \"precision\": 1.0, \"recall\": 0.04234367306745446, \"specificity\": 1.0, \"npv\": 0.3705501618122977, \"accuracy\": 0.38759445843828716, \"f1\": 0.0812470477090222, \"f2\": 0.05237515225334957, \"f0_5\": 0.18105263157894738, \"p4\": 0.14126803374967498, \"phi\": 0.12526154600224398}, {\"truth_threshold\": 20.60000030696392, \"match_probability\": 0.9999993708101274, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 72.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1959.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.03545051698670606, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9645494830132939, \"precision\": 1.0, \"recall\": 0.03545051698670606, \"specificity\": 1.0, \"npv\": 0.36887886597938147, \"accuracy\": 0.38318639798488663, \"f1\": 0.06847360912981455, \"f2\": 0.043923865300146414, \"f0_5\": 0.15523932729624837, \"p4\": 0.12150944981378942, \"phi\": 0.11435447741316879}, {\"truth_threshold\": 21.000000312924385, \"match_probability\": 0.9999995231631726, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 71.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1960.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.034958148695224026, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9650418513047759, \"precision\": 1.0, \"recall\": 0.034958148695224026, \"specificity\": 1.0, \"npv\": 0.3687600644122383, \"accuracy\": 0.38287153652392947, \"f1\": 0.0675547098001903, \"f2\": 0.043319097010372176, \"f0_5\": 0.15334773218142547, \"p4\": 0.1200573002429353, \"phi\": 0.11353928467532028}, {\"truth_threshold\": 21.1000003144145, \"match_probability\": 0.9999995550954947, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 70.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1961.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.034465780403742, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.965534219596258, \"precision\": 1.0, \"recall\": 0.034465780403742, \"specificity\": 1.0, \"npv\": 0.3686413393432067, \"accuracy\": 0.3825566750629723, \"f1\": 0.06663493574488338, \"f2\": 0.0427141811081279, \"f0_5\": 0.15144958892254434, \"p4\": 0.11859951946108616, \"phi\": 0.11271872714657621}, {\"truth_threshold\": 21.200000315904617, \"match_probability\": 0.9999995848894065, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 69.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1962.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.033973412112259974, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.96602658788774, \"precision\": 1.0, \"recall\": 0.033973412112259974, \"specificity\": 1.0, \"npv\": 0.3685226906984229, \"accuracy\": 0.3822418136020151, \"f1\": 0.06571428571428571, \"f2\": 0.04210911753936287, \"f0_5\": 0.14954486345903772, \"p4\": 0.11713607088211223, \"phi\": 0.11189268628385163}, {\"truth_threshold\": 21.500000320374966, \"match_probability\": 0.9999996628254004, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 67.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1964.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.032988675529295915, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.967011324470704, \"precision\": 1.0, \"recall\": 0.032988675529295915, \"specificity\": 1.0, \"npv\": 0.3682856223866195, \"accuracy\": 0.38161209068010077, \"f1\": 0.06387035271687322, \"f2\": 0.04089854718593578, \"f0_5\": 0.14571552849064812, \"p4\": 0.11419202241119845, \"phi\": 0.11022365852672915}, {\"truth_threshold\": 21.600000321865082, \"match_probability\": 0.999999685404968, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 66.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1965.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.03249630723781388, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9675036927621861, \"precision\": 1.0, \"recall\": 0.03249630723781388, \"specificity\": 1.0, \"npv\": 0.36816720257234725, \"accuracy\": 0.38129722921914355, \"f1\": 0.06294706723891273, \"f2\": 0.040293040293040296, \"f0_5\": 0.1437908496732026, \"p4\": 0.11271134775969976, \"phi\": 0.10938041200177231}, {\"truth_threshold\": 21.700000323355198, \"match_probability\": 0.9999997064724503, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 62.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1969.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.03052683407188577, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9694731659281143, \"precision\": 1.0, \"recall\": 0.03052683407188577, \"specificity\": 1.0, \"npv\": 0.36769428387925496, \"accuracy\": 0.38003778337531485, \"f1\": 0.05924510272336359, \"f2\": 0.03786953334962131, \"f0_5\": 0.13602457218078104, \"p4\": 0.10673009231574156, \"phi\": 0.1059459408998895}, {\"truth_threshold\": 21.800000324845314, \"match_probability\": 0.999999726129107, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 55.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1976.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.02708025603151157, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9729197439684885, \"precision\": 1.0, \"recall\": 0.02708025603151157, \"specificity\": 1.0, \"npv\": 0.3668695930791413, \"accuracy\": 0.3778337531486146, \"f1\": 0.052732502396931925, \"f2\": 0.03362269226066757, \"f0_5\": 0.12216792536650378, \"p4\": 0.0960314132133735, \"phi\": 0.09967408143925688}, {\"truth_threshold\": 22.40000033378601, \"match_probability\": 0.9999998193125794, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 48.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1983.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.023633677991137372, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9763663220088626, \"precision\": 1.0, \"recall\": 0.023633677991137372, \"specificity\": 1.0, \"npv\": 0.36604859335038364, \"accuracy\": 0.3756297229219144, \"f1\": 0.046176046176046176, \"f2\": 0.02936857562408223, \"f0_5\": 0.10796221322537113, \"p4\": 0.08502610073875708, \"phi\": 0.09301115301054898}, {\"truth_threshold\": 22.800000339746475, \"match_probability\": 0.9999998630645361, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 47.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1984.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.023141309699655343, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9768586903003447, \"precision\": 1.0, \"recall\": 0.023141309699655343, \"specificity\": 1.0, \"npv\": 0.36593160754234577, \"accuracy\": 0.37531486146095716, \"f1\": 0.04523580365736285, \"f2\": 0.028760249663443888, \"f0_5\": 0.10590356016223525, \"p4\": 0.08342802927538613, \"phi\": 0.09202247909630645}, {\"truth_threshold\": 23.000000342726707, \"match_probability\": 0.999999880790753, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 46.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1985.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.022648941408173313, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9773510585918267, \"precision\": 1.0, \"recall\": 0.022648941408173313, \"specificity\": 1.0, \"npv\": 0.365814696485623, \"accuracy\": 0.375, \"f1\": 0.04429465575349061, \"f2\": 0.028151774785801713, \"f0_5\": 0.1038374717832957, \"p4\": 0.08182335429924091, \"phi\": 0.0910237091474061}, {\"truth_threshold\": 23.20000034570694, \"match_probability\": 0.9999998962223214, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 45.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1986.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.022156573116691284, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9778434268833087, \"precision\": 1.0, \"recall\": 0.022156573116691284, \"specificity\": 1.0, \"npv\": 0.3656978601085915, \"accuracy\": 0.37468513853904284, \"f1\": 0.04335260115606936, \"f2\": 0.027543150936467132, \"f0_5\": 0.10176390773405698, \"p4\": 0.08021203063702596, \"phi\": 0.09001450647597614}, {\"truth_threshold\": 23.400000348687172, \"match_probability\": 0.9999999096562825, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 44.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1987.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.021664204825209258, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9783357951747907, \"precision\": 1.0, \"recall\": 0.021664204825209258, \"specificity\": 1.0, \"npv\": 0.365581098339719, \"accuracy\": 0.3743702770780856, \"f1\": 0.042409638554216866, \"f2\": 0.02693437806072478, \"f0_5\": 0.09968282736746716, \"p4\": 0.07859401270561604, \"phi\": 0.08899451553133284}, {\"truth_threshold\": 23.600000351667404, \"match_probability\": 0.9999999213512251, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 43.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1988.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.02117183653372723, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9788281634662728, \"precision\": 1.0, \"recall\": 0.02117183653372723, \"specificity\": 1.0, \"npv\": 0.3654644111075646, \"accuracy\": 0.37405541561712846, \"f1\": 0.041465766634522665, \"f2\": 0.026325456103832495, \"f0_5\": 0.09759418974126191, \"p4\": 0.07696925450739521, \"phi\": 0.08796336038865413}, {\"truth_threshold\": 23.70000035315752, \"match_probability\": 0.9999999266180979, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 42.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1989.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.0206794682422452, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9793205317577548, \"precision\": 1.0, \"recall\": 0.0206794682422452, \"specificity\": 1.0, \"npv\": 0.3653477983407786, \"accuracy\": 0.3737405541561713, \"f1\": 0.04052098408104197, \"f2\": 0.025716385011021307, \"f0_5\": 0.09549795361527967, \"p4\": 0.0753377096255321, \"phi\": 0.08692064307839845}, {\"truth_threshold\": 23.900000356137753, \"match_probability\": 0.9999999361173434, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 41.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1990.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.02018709995076317, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9798129000492368, \"precision\": 1.0, \"recall\": 0.02018709995076317, \"specificity\": 1.0, \"npv\": 0.3652312599681021, \"accuracy\": 0.3734256926952141, \"f1\": 0.03957528957528957, \"f2\": 0.02510716472749541, \"f0_5\": 0.09339407744874716, \"p4\": 0.07369933121919056, \"phi\": 0.08586594173547067}, {\"truth_threshold\": 24.00000035762787, \"match_probability\": 0.9999999403953735, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 35.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1996.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.017232890201871, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.982767109798129, \"precision\": 1.0, \"recall\": 0.017232890201871, \"specificity\": 1.0, \"npv\": 0.36453358802929003, \"accuracy\": 0.371536523929471, \"f1\": 0.03388189738625363, \"f2\": 0.02144870694938105, \"f0_5\": 0.08060801473975127, \"p4\": 0.06372287901796007, \"phi\": 0.07925886257954269}, {\"truth_threshold\": 24.100000359117985, \"match_probability\": 0.9999999443869169, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 32.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 1999.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.015755785327424915, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9842442146725751, \"precision\": 1.0, \"recall\": 0.015755785327424915, \"specificity\": 1.0, \"npv\": 0.3641857506361323, \"accuracy\": 0.37059193954659947, \"f1\": 0.031022782355792537, \"f2\": 0.0196174595389897, \"f0_5\": 0.07410838351088467, \"p4\": 0.0586384719748834, \"phi\": 0.07574980202172149}, {\"truth_threshold\": 24.2000003606081, \"match_probability\": 0.9999999481111586, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 31.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 2000.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.015263417035942885, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9847365829640571, \"precision\": 1.0, \"recall\": 0.015263417035942885, \"specificity\": 1.0, \"npv\": 0.3640699523052464, \"accuracy\": 0.3702770780856423, \"f1\": 0.030067895247332686, \"f2\": 0.01900674432863274, \"f0_5\": 0.07192575406032482, \"p4\": 0.0569290852372513, \"phi\": 0.0745449630242769}, {\"truth_threshold\": 24.300000362098217, \"match_probability\": 0.999999951585999, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 28.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 2003.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.013786312161496799, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9862136878385032, \"precision\": 1.0, \"recall\": 0.013786312161496799, \"specificity\": 1.0, \"npv\": 0.363722998729352, \"accuracy\": 0.36933249370277077, \"f1\": 0.027197668771248178, \"f2\": 0.01717369970559372, \"f0_5\": 0.06532897806812879, \"p4\": 0.051756446046609055, \"phi\": 0.07081241982024446}, {\"truth_threshold\": 24.50000036507845, \"match_probability\": 0.999999957853164, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 26.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 2005.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.012801575578532743, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9871984244214672, \"precision\": 1.0, \"recall\": 0.012801575578532743, \"specificity\": 1.0, \"npv\": 0.3634920634920635, \"accuracy\": 0.36870277078085645, \"f1\": 0.025279533300923675, \"f2\": 0.015950920245398775, \"f0_5\": 0.06088992974238876, \"p4\": 0.048270424636238894, \"phi\": 0.0682148893057115}, {\"truth_threshold\": 24.600000366568565, \"match_probability\": 0.9999999606756114, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 24.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 2007.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.011816838995568686, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9881831610044313, \"precision\": 1.0, \"recall\": 0.011816838995568686, \"specificity\": 1.0, \"npv\": 0.36326142131979694, \"accuracy\": 0.36807304785894207, \"f1\": 0.02335766423357664, \"f2\": 0.014727540500736377, \"f0_5\": 0.056417489421720736, \"p4\": 0.04475382343492924, \"phi\": 0.06551794967058633}, {\"truth_threshold\": 24.800000369548798, \"match_probability\": 0.9999999657661313, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 22.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 2009.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.010832102412604629, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9891678975873953, \"precision\": 1.0, \"recall\": 0.010832102412604629, \"specificity\": 1.0, \"npv\": 0.3630310716550412, \"accuracy\": 0.3674433249370277, \"f1\": 0.02143205065757428, \"f2\": 0.013503560029462312, \"f0_5\": 0.05191127890514394, \"p4\": 0.04120620154151108, \"phi\": 0.06270876930003502}, {\"truth_threshold\": 24.900000371038914, \"match_probability\": 0.999999968058671, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 21.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 2010.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.0103397341211226, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9896602658788775, \"precision\": 1.0, \"recall\": 0.0103397341211226, \"specificity\": 1.0, \"npv\": 0.3629160063391442, \"accuracy\": 0.36712846347607053, \"f1\": 0.02046783625730994, \"f2\": 0.01289134438305709, \"f0_5\": 0.04964539007092199, \"p4\": 0.03942061774542593, \"phi\": 0.06125728539403615}, {\"truth_threshold\": 25.00000037252903, \"match_probability\": 0.9999999701976862, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 19.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 2012.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.009354997538158542, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9906450024618415, \"precision\": 1.0, \"recall\": 0.009354997538158542, \"specificity\": 1.0, \"npv\": 0.3626860943934115, \"accuracy\": 0.36649874055415615, \"f1\": 0.018536585365853658, \"f2\": 0.011666461991894878, \"f0_5\": 0.045087802562885616, \"p4\": 0.035825619558433386, \"phi\": 0.05824884136336706}, {\"truth_threshold\": 25.100000374019146, \"match_probability\": 0.9999999721934579, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 17.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 2014.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.008370260955194485, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9916297390448056, \"precision\": 1.0, \"recall\": 0.008370260955194485, \"specificity\": 1.0, \"npv\": 0.3624564735675847, \"accuracy\": 0.36586901763224183, \"f1\": 0.0166015625, \"f2\": 0.010440977766859108, \"f0_5\": 0.040495474035254886, \"p4\": 0.03219846095822884, \"phi\": 0.05508044361350257}, {\"truth_threshold\": 25.200000375509262, \"match_probability\": 0.9999999740555788, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 15.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 2016.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.007385524372230428, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9926144756277696, \"precision\": 1.0, \"recall\": 0.007385524372230428, \"specificity\": 1.0, \"npv\": 0.3622271433090794, \"accuracy\": 0.36523929471032746, \"f1\": 0.01466275659824047, \"f2\": 0.009214891264283081, \"f0_5\": 0.035868005738880916, \"p4\": 0.028538670521671944, \"phi\": 0.05172269709897784}, {\"truth_threshold\": 25.400000378489494, \"match_probability\": 0.9999999774140695, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 14.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 2017.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.006893156080748399, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9931068439192516, \"precision\": 1.0, \"recall\": 0.006893156080748399, \"specificity\": 1.0, \"npv\": 0.362112586970272, \"accuracy\": 0.3649244332493703, \"f1\": 0.013691931540342298, \"f2\": 0.00860162202015237, \"f0_5\": 0.033540967896502155, \"p4\": 0.026696388534875385, \"phi\": 0.04996097057493643}, {\"truth_threshold\": 25.50000037997961, \"match_probability\": 0.9999999789265818, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 12.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 2019.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.005908419497784343, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9940915805022157, \"precision\": 1.0, \"recall\": 0.005908419497784343, \"specificity\": 1.0, \"npv\": 0.36188369152970923, \"accuracy\": 0.3642947103274559, \"f1\": 0.011747430249632892, \"f2\": 0.007374631268436578, \"f0_5\": 0.02886002886002886, \"p4\": 0.022986746233599045, \"phi\": 0.046240249339339734}, {\"truth_threshold\": 25.600000381469727, \"match_probability\": 0.9999999803378055, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 9.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 2022.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.004431314623338257, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9955686853766618, \"precision\": 1.0, \"recall\": 0.004431314623338257, \"specificity\": 1.0, \"npv\": 0.361540890432586, \"accuracy\": 0.3633501259445844, \"f1\": 0.008823529411764706, \"f2\": 0.005533013648100332, \"f0_5\": 0.02177068214804064, \"p4\": 0.017358654565300884, \"phi\": 0.040026259314463214}, {\"truth_threshold\": 25.700000382959843, \"match_probability\": 0.9999999816545239, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 8.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 2023.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.003938946331856229, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9960610536681438, \"precision\": 1.0, \"recall\": 0.003938946331856229, \"specificity\": 1.0, \"npv\": 0.3614267676767677, \"accuracy\": 0.3630352644836272, \"f1\": 0.00784698381559588, \"f2\": 0.004918839153959666, \"f0_5\": 0.019389238972370333, \"p4\": 0.015465403546152875, \"phi\": 0.03773116272757914}, {\"truth_threshold\": 25.900000385940075, \"match_probability\": 0.9999999840293354, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 6.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 2025.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.0029542097488921715, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9970457902511078, \"precision\": 1.0, \"recall\": 0.0029542097488921715, \"specificity\": 1.0, \"npv\": 0.361198738170347, \"accuracy\": 0.36240554156171284, \"f1\": 0.005891016200294551, \"f2\": 0.0036900369003690036, \"f0_5\": 0.014598540145985401, \"p4\": 0.011652683870064942, \"phi\": 0.032665835877723835}, {\"truth_threshold\": 26.00000038743019, \"match_probability\": 0.999999985098843, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 5.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 2026.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.002461841457410143, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9975381585425899, \"precision\": 1.0, \"recall\": 0.002461841457410143, \"specificity\": 1.0, \"npv\": 0.36108483128350677, \"accuracy\": 0.3620906801007557, \"f1\": 0.004911591355599214, \"f2\": 0.0030754090294009104, \"f0_5\": 0.01218917601170161, \"p4\": 0.009733083985039102, \"phi\": 0.02981498964104606}, {\"truth_threshold\": 26.100000388920307, \"match_probability\": 0.9999999860967289, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 4.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 2027.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.0019694731659281144, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9980305268340719, \"precision\": 1.0, \"recall\": 0.0019694731659281144, \"specificity\": 1.0, \"npv\": 0.3609709962168979, \"accuracy\": 0.36177581863979846, \"f1\": 0.003931203931203931, \"f2\": 0.0024606299212598425, \"f0_5\": 0.009770395701025891, \"p4\": 0.007804568825263287, \"phi\": 0.026663133550419747}, {\"truth_threshold\": 26.200000390410423, \"match_probability\": 0.9999999870277894, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 3.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 2028.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.0014771048744460858, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.9985228951255539, \"precision\": 1.0, \"recall\": 0.0014771048744460858, \"specificity\": 1.0, \"npv\": 0.36085723290261584, \"accuracy\": 0.3614609571788413, \"f1\": 0.0029498525073746312, \"f2\": 0.0018456995201181247, \"f0_5\": 0.007342143906020558, \"p4\": 0.00586707112734875, \"phi\": 0.023087312050119223}, {\"truth_threshold\": 26.400000393390656, \"match_probability\": 0.9999999887070348, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 2.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 2029.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.0009847365829640572, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.999015263417036, \"precision\": 1.0, \"recall\": 0.0009847365829640572, \"specificity\": 1.0, \"npv\": 0.36074354127284186, \"accuracy\": 0.36114609571788414, \"f1\": 0.001967535661583866, \"f2\": 0.0012306177701206006, \"f0_5\": 0.004904364884747425, \"p4\": 0.003920522953249476, \"phi\": 0.018847741566547744}, {\"truth_threshold\": 27.10000040382147, \"match_probability\": 0.9999999930483645, \"total_clerical_labels\": 3176.0, \"p\": 2031.0, \"n\": 1145.0, \"tp\": 1.0, \"tn\": 1145.0, \"fp\": 0.0, \"fn\": 2030.0, \"P_rate\": 0.6394836272040302, \"N_rate\": 0.36051636934280396, \"tp_rate\": 0.0004923682914820286, \"tn_rate\": 1.0, \"fp_rate\": 0.0, \"fn_rate\": 0.999507631708518, \"precision\": 1.0, \"recall\": 0.0004923682914820286, \"specificity\": 1.0, \"npv\": 0.3606299212598425, \"accuracy\": 0.3608312342569269, \"f1\": 0.000984251968503937, \"f2\": 0.0006153846153846154, \"f0_5\": 0.002457002457002457, \"p4\": 0.001964855681779181, \"phi\": 0.013325266908696693}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linker.evaluation.accuracy_analysis_from_labels_table(\n",
    "    labels_table, output_type=\"precision_recall\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e6ba74",
   "metadata": {},
   "source": [
    "## Truth table\n",
    "\n",
    "Finally, Splink can also report the underlying table used to construct the ROC and precision recall curves.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7c283ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-07T09:03:06.901873Z",
     "iopub.status.busy": "2024-06-07T09:03:06.901651Z",
     "iopub.status.idle": "2024-06-07T09:03:07.053323Z",
     "shell.execute_reply": "2024-06-07T09:03:07.052782Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>truth_threshold</th>\n",
       "      <th>match_probability</th>\n",
       "      <th>total_clerical_labels</th>\n",
       "      <th>p</th>\n",
       "      <th>n</th>\n",
       "      <th>tp</th>\n",
       "      <th>tn</th>\n",
       "      <th>fp</th>\n",
       "      <th>fn</th>\n",
       "      <th>P_rate</th>\n",
       "      <th>N_rate</th>\n",
       "      <th>tp_rate</th>\n",
       "      <th>tn_rate</th>\n",
       "      <th>fp_rate</th>\n",
       "      <th>fn_rate</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>specificity</th>\n",
       "      <th>npv</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f0_5</th>\n",
       "      <th>p4</th>\n",
       "      <th>phi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-12.4</td>\n",
       "      <td>0.000185</td>\n",
       "      <td>3176.0</td>\n",
       "      <td>2031.0</td>\n",
       "      <td>1145.0</td>\n",
       "      <td>1027.0</td>\n",
       "      <td>1145.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1004.0</td>\n",
       "      <td>0.639484</td>\n",
       "      <td>0.360516</td>\n",
       "      <td>0.505662</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.494338</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.505662</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.532806</td>\n",
       "      <td>0.683879</td>\n",
       "      <td>0.671681</td>\n",
       "      <td>0.561141</td>\n",
       "      <td>0.836455</td>\n",
       "      <td>0.683240</td>\n",
       "      <td>0.519057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-6.1</td>\n",
       "      <td>0.014369</td>\n",
       "      <td>3176.0</td>\n",
       "      <td>2031.0</td>\n",
       "      <td>1145.0</td>\n",
       "      <td>1026.0</td>\n",
       "      <td>1145.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1005.0</td>\n",
       "      <td>0.639484</td>\n",
       "      <td>0.360516</td>\n",
       "      <td>0.505170</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.494830</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.505170</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.532558</td>\n",
       "      <td>0.683564</td>\n",
       "      <td>0.671246</td>\n",
       "      <td>0.560656</td>\n",
       "      <td>0.836186</td>\n",
       "      <td>0.682913</td>\n",
       "      <td>0.518683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-5.0</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>3176.0</td>\n",
       "      <td>2031.0</td>\n",
       "      <td>1145.0</td>\n",
       "      <td>1024.0</td>\n",
       "      <td>1145.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1007.0</td>\n",
       "      <td>0.639484</td>\n",
       "      <td>0.360516</td>\n",
       "      <td>0.504185</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.495815</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.504185</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.532063</td>\n",
       "      <td>0.682935</td>\n",
       "      <td>0.670376</td>\n",
       "      <td>0.559685</td>\n",
       "      <td>0.835646</td>\n",
       "      <td>0.682259</td>\n",
       "      <td>0.517937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-4.9</td>\n",
       "      <td>0.032407</td>\n",
       "      <td>3176.0</td>\n",
       "      <td>2031.0</td>\n",
       "      <td>1145.0</td>\n",
       "      <td>1019.0</td>\n",
       "      <td>1145.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1012.0</td>\n",
       "      <td>0.639484</td>\n",
       "      <td>0.360516</td>\n",
       "      <td>0.501723</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.498277</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.501723</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.530830</td>\n",
       "      <td>0.681360</td>\n",
       "      <td>0.668197</td>\n",
       "      <td>0.557257</td>\n",
       "      <td>0.834289</td>\n",
       "      <td>0.680622</td>\n",
       "      <td>0.516071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-4.6</td>\n",
       "      <td>0.039602</td>\n",
       "      <td>3176.0</td>\n",
       "      <td>2031.0</td>\n",
       "      <td>1145.0</td>\n",
       "      <td>1018.0</td>\n",
       "      <td>1145.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1013.0</td>\n",
       "      <td>0.639484</td>\n",
       "      <td>0.360516</td>\n",
       "      <td>0.501231</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.498769</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.501231</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.530584</td>\n",
       "      <td>0.681045</td>\n",
       "      <td>0.667760</td>\n",
       "      <td>0.556771</td>\n",
       "      <td>0.834016</td>\n",
       "      <td>0.680295</td>\n",
       "      <td>0.515699</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   truth_threshold  match_probability  total_clerical_labels       p       n  \\\n",
       "0            -12.4           0.000185                 3176.0  2031.0  1145.0   \n",
       "1             -6.1           0.014369                 3176.0  2031.0  1145.0   \n",
       "2             -5.0           0.030303                 3176.0  2031.0  1145.0   \n",
       "3             -4.9           0.032407                 3176.0  2031.0  1145.0   \n",
       "4             -4.6           0.039602                 3176.0  2031.0  1145.0   \n",
       "\n",
       "       tp      tn   fp      fn    P_rate    N_rate   tp_rate  tn_rate  \\\n",
       "0  1027.0  1145.0  0.0  1004.0  0.639484  0.360516  0.505662      1.0   \n",
       "1  1026.0  1145.0  0.0  1005.0  0.639484  0.360516  0.505170      1.0   \n",
       "2  1024.0  1145.0  0.0  1007.0  0.639484  0.360516  0.504185      1.0   \n",
       "3  1019.0  1145.0  0.0  1012.0  0.639484  0.360516  0.501723      1.0   \n",
       "4  1018.0  1145.0  0.0  1013.0  0.639484  0.360516  0.501231      1.0   \n",
       "\n",
       "   fp_rate   fn_rate  precision    recall  specificity       npv  accuracy  \\\n",
       "0      0.0  0.494338        1.0  0.505662          1.0  0.532806  0.683879   \n",
       "1      0.0  0.494830        1.0  0.505170          1.0  0.532558  0.683564   \n",
       "2      0.0  0.495815        1.0  0.504185          1.0  0.532063  0.682935   \n",
       "3      0.0  0.498277        1.0  0.501723          1.0  0.530830  0.681360   \n",
       "4      0.0  0.498769        1.0  0.501231          1.0  0.530584  0.681045   \n",
       "\n",
       "         f1        f2      f0_5        p4       phi  \n",
       "0  0.671681  0.561141  0.836455  0.683240  0.519057  \n",
       "1  0.671246  0.560656  0.836186  0.682913  0.518683  \n",
       "2  0.670376  0.559685  0.835646  0.682259  0.517937  \n",
       "3  0.668197  0.557257  0.834289  0.680622  0.516071  \n",
       "4  0.667760  0.556771  0.834016  0.680295  0.515699  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_table = linker.evaluation.accuracy_analysis_from_labels_table(labels_table, output_type=\"table\")\n",
    "roc_table.as_pandas_dataframe(limit=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff86458e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f2d6d5f4",
   "metadata": {},
   "source": [
    "!!! note \"Further Reading\"\n",
    ":material-tools: For more on the quality assurance tools in Splink, please refer to the [Evaluation API documentation](../../linkereval.md).\n",
    "\n",
    "    :bar_chart: For more on the charts used in this tutorial, please refer to the [Charts Gallery](../../charts/index.md#model-evaluation).\n",
    "\n",
    "    :material-thumbs-up-down: For more on the Evaluation Metrics used in this tutorial, please refer to the [Edge Metrics guide.](../../topic_guides/evaluation/edge_metrics.md)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ee8f2d",
   "metadata": {},
   "source": [
    "## :material-flag-checkered: That's it!\n",
    "\n",
    "That wraps up the Splink tutorial! Don't worry, there are still plenty of resources to help on the next steps of your Splink journey:\n",
    "\n",
    ":octicons-link-16: For some end-to-end notebooks of Splink pipelines, check out our [Examples](../examples/examples_index.md)\n",
    "\n",
    ":simple-readme: For more deepdives into the different aspects of Splink, and record linkage more generally, check out our [Topic Guides](../../topic_guides/topic_guides_index.md)\n",
    "\n",
    ":material-tools: For a reference on all the functionality avalable in Splink, see our [Documentation](../../documentation_index.md)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
