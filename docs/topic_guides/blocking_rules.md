---
tags:
  - Blocking
  - Performance
---

# The Challenges of Record Linkage

One of the main challenges to overcome in record linkage is the **scale** of the problem.

The number of pairs of records to compare grows using the formula $\frac{n\left(n-1\right)}2$, i.e. with (approximately) the square of the number of records, as shown in the following chart:



![](../img/blocking/pairwise_comparisons.png)

For example, a dataset of 1 million input records would generate around 500 billion pairwise record comparisons.

So, when datasets get bigger the amount of computational resource gets extremely large (and costly). In reality, we try and reduce the amount of computation required using **blocking**.

## Blocking

Blocking is a technique for reducing the number of record pairs that are considered by a model.

Considering a dataset of 1 million records, comparing each record against all of the other records in the dataset generates ~500 billion pairwise comparisons. However, we know the vast majority of these record comparisons won't be matches, so processing the full ~500 billion comparisons would be largely pointless (as well as costly and time-consuming).

Instead, we can define a subset of potential comparisons using **Blocking Rules**. These are rules that define "blocks" of comparisons that should be considered. For example, the blocking rule:

 `"l.first_name = r.first_name and l.surname = r.surname"` 
 
 will generate pairwise record comparisons amongst pairwise comparisons where first name and surname match.

 Within a Splink model, you can specify multiple "blocks" through multiple Blocking Rules to ensure all potential matches are considered.

???+ "Further Reading"

    For more information on blocking, please refer to XXXX

### Choosing Blocking Rules

 The blocking process is a compromise between the amount of **compuational resource** used when comparing records and **capturing all true matches**. 

 Even after blocking, the number of comparisons generated is usually much higher than the number of input records - often between 10 and 1,000 times higher. As a result, the performance of Splink is heavily influenced by the number of comparisons generated by the blocking rules, rather than the number of input records.

 Getting the balance right between compuational resource and capturing matches can be tricky, and is largely dependent on the specific datasets and use case of the linkage. In general, we recommend a strategy of starting with strict blocking rules, and gradually loosening them. Sticking to less than 10 million comparisons is a good place to start, before scaling jobs up to 100s of millions (:simple-duckdb: DuckDB on a laptop), or sometimes billions (:simple-apachespark: Spark or :simple-amazonaws: Athena). 
 
 Guidance for choosing Blocking Rules can be found in the two [Blocking in Splink](#blocking-in-splink) topic guides.


## Blocking in Splink

There are two areas in Splink where blocking is used:

- [Training a Splink model](./blocking_model_training.md)
- [Making Predictions from a Splink model](./blocking_predictions.md)

each of which is described in their own, dedicated topic guide.

